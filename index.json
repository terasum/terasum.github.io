[{"categories":null,"contents":"   前言  近期正好有机会接触到了一个全链路追踪的改造项目，正好给自己一个了解其他领域的机会，其实也是为我自己找到了一个能够挑战自己的机会，这是一个很重要的项目，也涉及非常复杂的系统和相关方。而长期以技术人员的思维考虑事情的我，觉得这是一个很好的挑战自我的机会，一方面能够了解一个相对陌生的领域的技术，另一方面，也能够拓宽自己的能力边界。\n   技术背景  整体的出发点其实是一个客户路程的追踪展示需求，需求说简单也挺简单的，即希望能够看到客户在使用APP的过程当中出现了哪些问题，导致这些问题的原因是什么。但其实这里面涉及到的内容是相对复杂的，这涉及到前端的数据埋点，以及后端的链路跟踪，这是一个端到端的全链路追踪系统。\n其实全链路追踪这件事是有现成的解决方案的，在现代的微服务架构体系下，很多框架乃至解决方案都提供了非常完备的技术方案帮助追踪系统调用链路。但是不禁要问，究竟是什么时候起，需要全链路追踪系统了呢？\n我们知道，在设计系统架构的时候，合适是一个非常重要的原则，简单的系统如单体系统乃至仅有一两个微服务的系统，我们在分析系统解决问题的时候，其实是相对快速方便的，不存在调用链路冗长复杂，系统依赖关系模糊的情况，只有在软件系统发展到一定规模，系统业务达到一定量级了，才需要大量的服务去支撑完整系统，这个过程恰恰为大型信息系统演变的过程。\n Distributed Programming is the art of solving the same problem that you can solve on a single computer using multiple computers — Mikito Takada\n 所谓 “天下大事，合久必分”，软件系统往往也类似，达到一定规模，复杂度上升到一定程度，拆分是解决复杂性问题的优选方案，分布式系统横空出世，用来解决单机系统无法解决的问题。但是微服务架构带来的问题也很突出，系统很多，链路很长，服务器集群规模很大，出了问题往往没有办法快速定位原因。 为了解决这个业务规模增长带来的软件系统规模增长引发的开发维护难度也快速增长的尖锐问题，链路追踪自然而然成为了解决问题的良方。\n   现有系统分析  分布式系统的链路追踪和故障定位已经成为系统规模增之后不得不解决的难题，困扰着开发者的还包括这些问题：\n 根因定位难：原来笔者在支付宝工作时，除了一个线上故障，往往要拉好几个团队的人一起看，因为很多服务都拆分给了很多团队负责，出了问题很难说清楚是哪个服务的问题，所以一堆人都要到线上看，没有有效的跟踪定位能力，效率必然低下。 容量评估难：就如木桶原则，整个系统的性能短板在哪里，所有流量的毕竟之地，以及大促的时候需要重点关注以及扩容的服务是哪个，需要做出这些判断没有一个有效的链路请求数据真的只能拍脑袋随便猜。 架构设计难：在进行系统分析和架构设计的时候，往往需要对上下游的依赖进行分析评估以确定影响，没有一个有效的链路追踪系统，只能如盲人摸象，有文档还好，老司机指点也行，如果没有，只能看代码自求多福。 性能优化难：同容量评估类似，链路数据能够帮助我们快速找到系统的性能瓶颈，一个用户请求响应慢，服务返回不及时，到底是哪个服务导致的一目了然，也不用推诿扯皮。  上面的都是问题，这也帮助我们澄清了全链路系统应该解决的需求，链路架构与服务浏览地图、耗时、异常数据的采集与展现都是需要解决的问题。\n   Google 的 Dapper  2010年Google发布了一篇名叫《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》的论文，被业界誉为分布式系统链路追踪的开山级文献。该文献详细叙述了Google 内部是如何实现一个低损耗、应用透明、大规模的监控追踪系统来为生产系统保驾护航的。在 Dapper 之前 Magpie系统已经针对性能调试、容量规划、系统调优和异常检测等方面提出了一些解决思路。而 X-Trace则提供了一个针对网络流量的追踪方案。Dapper 结合了两者的优势。本小节则重点分析 Dapper 中几个重要的设计。\n   调用树与跨度（Span）  在一次调用过程当中，服务依赖关系可以是很复杂的，在Dapper中，举了如下图的例子： 这个路径由用户的X请求发起，穿过一个简单的服务系统。用字母标识的节点代表分布式系统中的不同处理过程\n如上图所示，用户发起的请求，内部进行了4次RPC调用，而且并不仅仅由A系统发起，C系统也发起了两次调用，可以清晰地看出来，调用存在一定的树形关系。 这张图能够更加清晰地标识调用之间的时序关系，为了串联一个完整的请求，自然而然地，我们会想到使用唯一的请求id用来标识一次用户请求，这也就是我们常听说的 trace id 这个想法自然且正确，但随之而来的问题是，trace id 能够确定一组内部请求，但是无法表示树形的层级结构，因为信息量不够，构建一个树形结构需要引入新的信息，这就是跨度，也就是父调用和当前调用信息，我们称之为 parent span 和current span。\n在跨度(span)信息中加上开始调用和结束调用时间，我们就能够迅速确定调用链的层级关系和性能信息，所以，一次调用需要一直携带的上下文信息包括：\n1 2 3 4 5 6 7  public class TraceData { String TraceID; String ParentSpanID; String CurrentSpanID; TimeStamp startTS; TimeStamp endTS; }   也许 endTS可以用调用延时来替代，但这只是实现细节差异了。\n   植入点  上文提及，调用需要携带上下文信息，但是如何携带其实是一个非常复杂的问题，Dapper设计的过程当中需要达到应用透明的要求，也就意味着，不可以侵入系统，但是为了能够将追踪上下文信息贯穿全链路，上下文信息的透传是必须的。 为了不侵入业务，Dapper 针对一些少量的通用组件进行了改造，主要改造内容包括：\n 基于一个请求一个线程处理的前提，Dapper 把单次请求的上下文放在 ThreadLocal 中进行存储。追踪上下文包括 traceID和span ID，被设计为了轻量的格式便于快速序列化。 如果调用不是同步的，比如延迟调用的或是异步调用的，比如通过线程池或流程执行器调用，他们大多使用一个一个通用的控制流库来回调。Dapper 对通用控制流库进行了改造，确保所有异步回调可以存储这次跟踪的上下文，而当回调函数被触发时，该次跟踪的上下文会与适当的线程关联上，这样Dapper依旧可以使用 TraceID 和 SpanID 来辅助构建异步调用路径。 更为重要的是，进程间边界之间的透传，几乎所有的Google系统服务的进程间通信是建立在一个用C++和Java开发的RPC框架上。Dapper 修改该框架来让RPC调用能够携带追踪上下文信息，TraceID 和 SpanID 会从客户端发送到服务端。当然，这要求有统一的应用框架来完成这件事情。     Annotation (注解)   此注解非 Java 中的注解\n 有上面的基础，其实已经能够推导出追踪的链路细节了，但是也仅限于此，明显我们还需要一些额外的信息，比如缓存命中率啊，计算耗时，业务数据等。Dapper 提供了 Annotation 的能力，除了链路信息，也可以携带透传一些额外信息，如用户ID等,Annotation 可以是文本，也可以是键值对，这个设计也为后面的追踪系统提供了类似Tag或是Bagger的能力。\n   采样率  Dapper 设计的另一个目标就是低损耗，所以提供了一个采样采集的能力，如果请求量很大，而采集系统又很耗费性能，显然会把原有的服务拖垮。这样服务提供方也不愿意部署这个拉胯的追踪系统。因此Dapper设计了一个采样少部分流量的能力，而采样多少流量的比率就被称为采样率，这是一个非常好的设计，一方面降低了服务方的顾虑，另一方面也可以降低采集分析端的存储和分析压力，能够很好地控制损耗。\n   Dapper 的采集架构示意  Dapper 跟踪记录和收集的过程分为三个阶段（参见上图)。\n 首先，应用系统将 span数据写入（1）本地日志文件中。 然后，Dapper的守护进程和收集组件把这些数据从生产环境的主机中拉出来（2） 最终写到（3）Dapper的Bigtable仓库中。  一次跟踪数据被设计成 Bigtable 中的一行，每一列相当于一个 span 。Bigtable 的支持稀疏表格布局正适合这种情况，因为每一次跟踪可以有任意多个 span。\n  这也就说明了类似的数据其实比较适合存在 HBase 这种数据库中 有关请求延时数据，建议参考原论文，本文觉得没有参考意义，就不放了。      Dapper 的一些实施细节   Google 几乎在所有的生产服务器上都部署了 Dapper 的跟踪收集守护进程； 一些非RPC框架的系统如原生TCP Socket和SOAP RPC的库，是无法支持Dapper跟踪的，因此需要单独编码接入到Dapper中； 考虑到生产环境的安全，Dapper 默认是关闭的，可以通过开关打开； Annotation携带的额外数据是很有必要的，70％的Dapper span和90％的所有 Dapper Trace 都至少携带一个额外 Annotation 数据； 系统损耗包括两部分，一部分是生成追送和收集追踪数据带来的性能下降；另一部分是存储和分析追踪数据，第一部分会影响应用服务本身，第二部分则通过分布式系统降低了影响；   Dapper运行库中最重要的跟踪生成消耗在于创建和销毁span和annotation，并记录到本地磁盘供后续的收集。根span的创建和销毁需要损耗平均204纳秒的时间，而同样的操作在其他span上需要消耗176纳秒。时间上的差别主要在于需要在跟span上给这次跟踪分配一个全局唯一的ID。\n    Dapper 的API 设计  数据存储之后需要进行查询分析，Dapper为此设计了几个API (Dapper API, DAPI):\n 通过 TraceID 来访问：DAPI 可以通过他的全局唯一的 TraceID 读取任何一次请求跟踪信息; 批量访问：DAPI 可以利用 MapReduce 提供对上亿条 Dapper 跟踪数据的并行读取。用户重写一个虚拟函数，它接受一个 Dapper 的跟踪信息作为其唯一的参数，该框架将在用户指定的时间窗口中调用每一次收集到的跟踪信息；（在线 MR 是一种非常昂贵的设计） 索引访问：Dapper 的存储仓库支持一个符合通用调用模板的唯一索引。该索引根据通用请求跟踪特性(commonly-requested trace features)进行绘制来识别 Dapper 的跟踪信息。因为跟踪ID是根据伪随机的规则创建的，这是去访问跟某个服务或主机相关的跟踪数据最好的办法。 （设计特定的索引是非常有必要的，在实际系统应用当中，通过服务名称搜索是非常常见的）     Dapper 的UI设计  Dapper 论文介绍了几个典型的UI流程，如下图所示：  用户可以查询服务和响应时间，通过输入Trace的搜索条件完成，比如span的名称；同时用户也可以选择他们关心的一些成本度量(cost metric)如服务响应时间； 性能概要的表格，能够对应相关联的各项服务，用户可以将执行信息进行自由排序，或者选择一种直方图去展现更多的细节； 一旦某个单一的分布式执行跨度被选中后，用户能看到关于执行跨度的的图形化描述。被选中的服务被高亮展示在该图的中心； 在生成与步骤1中选中的成本度量(cost metric)维度相关的统计信息之后，Dapper的用户界面会提供了一个简单的直方图。在这个例子中，我们可以看到所选中部分的一个大致的分布式响应时间分布图。 用户需要检查某个跟踪的具体情况，Dapper 使用由一个全局时间轴（下方绿框处），并能够展开和折叠树形结构。分布式跟踪树的连续层用内嵌的不同颜色的矩形表示。每一个RPC的span被从时间上分解为一个服务器进程中的耗时（绿色部分）和在网络上的耗时（蓝色部分）。用户 Annotation 可选展示。     Dapper 总结  Dapper 提供了一个很好的追踪系统的实践参考，尽管其没有提供具体的数据结构设计和框架代码实现，但是它提出的跨度信息、采样率、Annotation等概念为后面的pinpoint、CAT、SkyWalking的实现都提供了重要的借鉴意义。在他们的设计过程当中，我发现在Google的十几年前的论文中就能够发现他们系统设计的先进性和架构基础设施的前瞻性，确实给我带来的相当震撼和启发。\n   Woonduk Kang 的 Pinpoint  Pinpoint 是一个 APM（应用程序性能管理）工具，用于用 Java/PHP 编写的大型分布式系统。受 Dapper 的启发，Pinpoint 提供了一种解决方案，通过跟踪分布式应用程序中的事务来帮助分析系统的整体结构以及其中的组件如何互连。\n   Pinpoint 能够解决的问题  如今的服务通常由许多不同的组件组成，它们相互之间进行通信以及对外部服务进行 API 调用。每笔交易的执行方式通常都被保留为黑匣子。 Pinpoint 跟踪这些组件之间的事务流，并提供清晰的视图来识别问题区域和潜在的瓶颈。\n 应用程序拓扑图 应用程序实时监控 单笔交易的代码级可见性 无代码侵入性 性能损耗极小（3%) 上述的能力似乎看上去比Dapper强上不少，事实上，在开源的APM系统当中，Pinpoint也算是佼佼者了。     Pinpoint的架构设计  Pinpoint的架构设计非常有借鉴意义，几乎是一个采集系统最简单的架构体系了，如下图所示： 一个追踪系统包括如下几个部分：\n 采集部分：也就是上图的 Agent和 Collector 存储部分：即上图的 HBase Storage 分析部分：集成在了 Pinpoint Web UI 中 展现部分: 主要是 Pinpoint Web UI  首先讲讲 Agent 部分，采用了 java agent 技术进行字节码增强，这个技术用来做APM其实非常合适，在之前的工作当中也接触到用来做应用流量复制（影子流量）的，原理并不复杂，javaagent 技术其实就是一个特殊的 jar 文件，利用 JVM 提供的 Instrumentation API 来更改加载到 JVM 中的字节码。技术细节将在后文介绍。\n   Pinpoint 的数据结构设计  pinpoint是基于Dapper进行设计的，其数据结构也类似，主要包括:\n Span: RPC（远程过程调用）跟踪的基本单元；它表示当 RPC 到达并包含跟踪数据时处理的工作。为了确保代码级别的可见性，Span 将标记为 SpanEvent 的子项作为数据结构。每个 Span 包含一个 TraceId。 Trace: 一个 Span 的集合；它由相关的 RPC（跨度）组成。同一跟踪中的 Span 共享相同的 TransactionId。 Trace 通过 SpanIds 和 ParentSpanIds 排序为层次树结构。 TraceId：由 TransactionId、SpanId 和 ParentSpanId 组成的键的集合。 TransactionId 表示消息ID，SpanId 和ParentSpanId 都代表RPC 的父子关系。 TransactionId (TxId)：从单个事务跨分布式系统发送/接收的消息的 ID；它在整个服务器组中必须是全局唯一的。 SpanId：接收RPC消息时处理的作业ID；它是在 RPC 到达节点时生成的。 ParentSpanId (pSpanId)：生成 RPC 的父 span 的 SpanId。如果节点是事务的起点，则不会有父跨度 - 对于这些情况，我们使用值 -1 来表示跨度是事务的根跨度。  与 Dapper 的差异是，Pinpoint 中的 TransactionId 等于 Dapper 中的 TraceId, 但是笔者人为 Pinpoint 的术语设计多少有点混乱，Trace 是Span的集合， TraceId则不代表这个集合，而是 TxId， SpanId和ParentSpanId的集合，这个极容易混淆。\n其实在Pinpoint中真正在传递的只有 TxId, SpanId和ParentSpanId: Pinpoint 可以使用 TransactionId 找到关联的 n 个 Span，并可以使用 SpanId 和 ParentSpanId 将它们排序为分层树形结构。\nSpanId 和 ParentSpanId 是 64 位长整数。由于数字是任意生成的，因此可能会出现冲突，但考虑到从 -9223372036854775808 到 9223372036854775807 的值范围，这不太可能发生。\nTransactionId 由 AgentIds、JVM（Java 虚拟机）启动时间和 SequenceNumbers 组成。\n AgentId：JVM启动时用户创建的ID；它在安装了 Pinpoint 的整个服务器组中必须是全局唯一的。使其唯一的最简单方法是使用主机名 ($HOSTNAME)，因为主机名通常不重复； JVM 启动时间：需要保证从零开始的唯一序列号。此值用于防止用户错误创建重复的 AgentId 时发生 ID 冲突； SequenceNumber：Pinpoint Agent下发的ID，从零开始依次递增的数字；针对每条请求消息生成；  Dapper 和 Twitter 的分布式系统跟踪平台 Zipkin 生成随机 TraceId（Pinpoint 中的 TransactionId）并将冲突情况视为正常情况。但在 Pinpoint 中尽可能避免这种冲突。为此，Pinpoint 采用了ID生成空间大，携带数据长但冲突概率低的方式解决。\n 所有的系统在设计过程当中均不采用中心化分发ID的方式，一方面可以避免中心化分发系统宕机所带来的服务不可用风险，另一方面也分担了性能开销。\n    Pinpoint的字节码增强技术  Twitter 的 Zipkin 使用修改后的基础库及其容器 (Finagle) 提供分布式事务跟踪功能。但显然，它需要修改代码。Pinpoint采用了javaagent技术，实现了无需修改代码即可追踪的能力，并达成了代码级别的可见性并自动处理标签信息。\n   Item 优势 劣势     手工埋点 API简单，可以最小化bug风险 需要开发人员修改代码，追踪能力弱   自动增强 无需修改代码，可以采集更加丰富的数据 相对代码埋点开销高；需要更加专业的人员能够开发相对核心底层中间件追踪系统；代码bug风险更大，影响更广，因此需要专家进行开发    字节码增强难度更大，风险更高，当然带来的收益也就更多。开发agent需要大量的研发资源，需要非常专业的java专家进行开发，但是带来的好处就是，应用服务端则基本不需要改造了。如果服务数量足够多，开发足够复杂，应该将成本转移到agent的开发。\n基于Javaagent技术的代码追踪能力还有一些隐藏的优势：\n 无需担心API设计的缺陷，如果采用代码埋点方式，一个API的修改需要修改大量服务，这基本上是不能承受的，因此一开始就要把API设计好； 方便地开关，Javaagent技术能够帮助我们快速地启用或者禁用追踪，而代码埋点则需要应用系统进行配置，无法很好地控制；  Javaagent 启用的 JVM参数命令，如果出问题了，可以去掉这些命令\n1 2 3  -javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar \\ -Dpinpoint.agentId=\u0026lt;Agent\u0026#39;s UniqueId\u0026gt; \\ -Dpinpoint.applicationName={The name indicating a same service (AgentId collection)}      Pinpoint 的一些技术细节  使用二进制格式（thrift) Pinpoint可以使用二进制格式 (Thrift) 来提高编码和传输速度，提高网络使用效率（使用和调试比较困难）。\n使用可变长度编码 常见的编码方式是整型4或8字节，这种称为定长编码，空间利用效率多少会低一些，而Pinpoint 通过 Thrift 的 Compact Protocol 将数据编码为可变长度，优化存储传输空间。\n通过常量表来表示特定API 或 SQL 直接将API请求信息和SQL信息发送至后端会导致信息过载，通过建立一个常量到API或是SQL的映射可以大大减小网络传输和存储的压力，这个常量映射存储在HBase中。\n追踪采样 Pinpoint在针对大流量系统中可以只采样 1~5%的流量，降低分析压力。\n使用异步数据传输降低应用线程影响\n Pinpoint 采用异步的方式传输数据，即使传输失败也不会影响业务线程； Pinpoint 使用UDP的方式传输数据，降低网络不稳定对系统带来的影响，同时提供了API，用户可以自己调整传输方式；     Pinpoint 的UI 界面    ServerMap - 可以展示当前服务的拓扑图，点击一个节点，可以展示这个节点的细节，比如当前的状态和处理的交易数量\n  Realtime Active Thread Chart - 实施跟踪当前应用内部的线程数量。\n  Request/Response Scatter Chart -当前的请求和响应的散点图，能够发现潜在问题；当然点击交易信息通过“挖掘”能力查看。\n  CallStack - 取得代码级别的可视化能力，在分布式环境下可以快速找到当前请求的瓶颈在哪，就和本地调用一样\n  Inspector - 更多的额外信息，比如CPU,内存，GC时间以及TPS，JVM参数等\n     Pinpoint 总结  Pinoint做Java的监控其实很不错了，功能都有，并且能够完成所有的关键能力，可视化做的也可以。但是也仅限于Java了。\n   Twitter 的 zipkin  相比于 Pinpoint Twitter的追踪系统 Zipkin 则简单一些。Zipkin 能够收集微服务架构中的延迟问题所需的时间数据。\nZipkin可以通过TraceId进行搜索，当然如果没有的话，可以通过 service, operation name tags等信息进行搜索，Zipkin能够提供一个带跨度层级的延迟信息。\nZipkin也提供了一个能够查询链路拓扑的界面，虽然有一些简陋： Zipkin 也通过agent技术获取数据，利用HTTP协议或是Kafka等队列技术发送给后端，最后存储到 Apache Cassandra或Elasticsearch中。\n   Zipkin的架构  Zipkin的架构非常简单，主要设计和Dapper也大同小异，所以也没有什么好介绍的。 Zipkin 中有四个组件：\n collector 采集器：最终数据到达 Zipkin 收集器守护程序后，将对其进行验证、存储和索引，以供查找。 storage 存储：原生支持 Cassandra，还支持 ElasticSearch 和 MySQL。 search 搜索（分析）：查询程序提供了一个简单的 JSON API 供 WebUI 来检索跟踪数据 webUI 展示：Web UI 提供了一种基于服务、时间和tag检索跟踪数据的方法。注意：UI 中无身份验证  Zipkin 的 agent 库采用Http协议，Kafka或 Scribe 协议将 span 数据发送至采集器。\n   Zipkin 的数据模型  Zipkin的数据模型由json定义，不清楚是如何处理大规模数据的，其模型定义如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  [ { \u0026#34;traceId\u0026#34;: \u0026#34;5982fe77008310cc80f1da5e10147517\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;get-traces\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;ebf33e1a81dc6f71\u0026#34;, \u0026#34;parentId\u0026#34;: \u0026#34;bd7a977555f6b982\u0026#34;, \u0026#34;timestamp\u0026#34;: 1458702548478000, \u0026#34;duration\u0026#34;: 354374, \u0026#34;localEndpoint\u0026#34;: { \u0026#34;serviceName\u0026#34;: \u0026#34;zipkin-query\u0026#34;, \u0026#34;ipv4\u0026#34;: \u0026#34;192.168.1.2\u0026#34;, \u0026#34;port\u0026#34;: 9411 }, \u0026#34;tags\u0026#34;: { \u0026#34;lc\u0026#34;: \u0026#34;JDBCSpanStore\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;QueryRequest{serviceName=zipkin-query, spanName=null, annotations=[], binaryAnnotations={}, minDuration=null, maxDuration=null, endTs=1458702548478, lookback=86400000, limit=1}\u0026#34; } ... ]   核心数据也是 traceId, spanId, parentSpanId,这些Id都是随机生成的：\n当传入的请求没有附加跟踪信息时，会生成一个随机的 trace ID 和 span ID。spanID 可以作为 trace ID 的低 64 位重复使用，但也可以完全不同；\n如果请求已经附加了跟踪信息，则服务应使用该信息，因为服务器接收和服务器发送事件与客户端发送和客户端接收事件属于同一跨度。\n如果服务调用下游服务，则会创建一个新跨度作为前一个跨度的子级。它由相同的trace id，一个新的span id标识，并且父id被设置为前一个span的span id。新的 span id 应该是 64 个随机位。\n注意如果服务使多个下游调用，则必须重复此过程。这是每个后续跨度都有相同的跟踪ID和父ID，而是一个新的和不同的跨度ID。\n   一些zipkin的技术细节   时间是用毫秒的 span.Timestamp 和 duration 只能由启动跨度的服务设置，（原因没看明白）     dianpin 的 CAT  大众点评开源的 CAT（Central Application Tracking），是基于 Java 开发的分布式实时监控系统。CAT在基础存储、高性能通信、大规模在线访问、服务治理、实时监控、容器化及集群智能调度等领域提供业界领先的、统一的解决方案。 CAT 支持四种消息类型的监控：\n Transaction 适合记录跨越系统边界的程序访问行为,比如远程调用，数据库调用，也适合执行时间较长的业务逻辑监控，Transaction用来记录一段代码的执行时间和次数 Event 用来记录一件事发生的次数，比如记录系统异常，它和transaction相比缺少了时间的统计，开销比transaction要小 Heartbeat 表示程序内定期产生的统计信息, 如CPU利用率, 内存利用率, 连接池状态, 系统负载等 Metric 用于记录业务指标、指标可能包含对一个指标记录次数、记录平均值、记录总和，业务指标最低统计粒度为1分钟     CAT的架构设计  CAT主要分为三个模块，cat-client，cat-consumer，cat-home\n cat-client 嵌入SDK, 提供给业务以及中间层埋点的底层sdk。 cat-consumer 采集器, 用于实时分析从客户端提供的数据。 cat-home 分析查询，作为用户提供给用户的展示的控制端。  CAT使用 HDFS 进行存储。\n上图是CAT目前多机房的整体结构图：\n 路由中心是根据应用所在机房信息来决定客户端上报的CAT服务端地址 每个机房内部都有的独立的原始信息存储集群HDFS cat-home可以部署在一个机房也可以部署在多个机房，在做报表展示的时候，cat-home会从cat-consumer中进行跨机房的调用，将所有的数据合并展示给用户 实际过程中，cat-consumer、cat-home以及路由中心都是部署在一起，每个服务端节点都可以充当任何一个角色  CAT 是侵入式的 用户需要集成 CAT client进行代码的采集，这个过程是需要嵌入SDK的，和上面讨论的几个系统都有些差异。\nCAT采用私有协议\n CAT序列化协议是自定义序列化协议，自定义序列化协议相比通用序列化协议要高效很多（文档原文）。 CAT是直接上报的 并不是通过日志输出异步上报的 上图是CAT的客户端上报逻辑，取得数据后，放到内存队列并通过特定的线程发送至服务端。  CAT 的服务端做分析再转储 如上图，CAT服务端在整个实时处理中，基本上实现了全异步化处理。\n 消息接收是基于Netty的NIO实现 消息接收到服务端就存放内存队列，然后程序开启一个线程会消费这个消息做消息分发 每个消息都会有一批线程并发消费各自队列的数据，以做到消息处理的隔离 消息存储是先存入本地磁盘，然后异步上传到hdfs文件，这也避免了强依赖hdfs  CAT的服务端直接进行了实时分析，主要采用内存分析，但是分析模型就是比较固定了：计数、计时和关系处理三种，基本上能够满足需求。\nCAT 能够在服务端做实时处理，主要包括如下几个特性：\n 去中心化，数据分区处理 基于日志只读特性，以一个小时为时间窗口，实时报表基于内存建模和分析，历史报表通过聚合完成 基于内存队列，全面异步化，单线程化，无锁设计 全局消息ID，数据本地化生产，集中式存储 组件化、服务化理念     CAT 的存储设计  CAT 的数据结构没有与上面的系统一样，几乎是自定义了一套，可以看出来由一些差异，文档中也没有非常明显的地方体现这部分，可以看下图： 可以看到CAT协议的消息头中包括消息ID，根消息ID， 目前尚不清楚其实如何进行span关联的，内部重新定义了一个Transaction结构，说了可以嵌套，嵌套方式也很难在图上看出来。\nCAT的MessageID格式由四段组成，以ShopWeb-0a010680-375030-2为例：\n 第一段是应用名shop-web 第二段是当前这台机器的ip的16进制格式，01010680表示10.1.6.108 第三段的375030，是系统当前时间除以小时得到的整点数 第四段的2，是表示当前这个客户端在当前小时的顺序递增号  CAT针对消息日志也进行了压缩存储，目前看实时计算之后还需要把算出来的数据压缩存储至磁盘： 整体存储结构如下图\nCAT数据文件分为两种，一类是index文件，一类是Data文件\n data文件是分段GZIP压缩，每个分段大小小于64K，这样可以用16bits可以表示一个最大分段地址 一个MessageId都用需要48bits的空间大小来存索引，索引根据MessageId的第四段来确定索引的位置，比如消息MessageId为ShopWeb-0a010680-375030-2，这条消息ID对应的索引位置为2*48bits的位置 48bits前面32bits存数据文件的块偏移地址，后面16bits存数据文件解压之后的块内地址偏移 CAT读取消息的时候，首先根据MessageId的前面三段确定唯一的索引文件，在根据MessageId第四段确定此MessageId索引位置，根据索引文件的48bits读取数据文件的内容，然后将数据文件进行GZIP解压，在根据块内偏移地址读取出真正的消息内容。   笔者认为,CAT的存储模型限制了其分析能力，特别是二次的在线离线分析都受限了。\n    CAT总结  CAT 的报表能力是比较强的，但是从通信协议、存储模型、采集方式上看，都与市面上的成熟系统有较大差异，采用该系统容易与其绑死，扩展性也受限，如果是需要小成本轻量级的分析能力的话可以选用。\n   Apache 的 SkyWalking  最后再看看国人主导的 SkyWalking，单从界面上看，SkyWalking 已经秒杀上述一众系统了。 SkyWalking 的scope就大很多，定位为大而全的APM系统，希望能够采集链路、日志以及指标信息，并进行统一的分析和展示。\n   Skywalking 架构  SkyWalking包括四个组件：探针, 平台后端，存储,UI 探针: 收集数据并重新重新格式化满足 Skywalking 要求（不同的探针支持不同的数据来源）； 平台后端: 支持数据聚合，分析和流传输过程涵盖tracing，指标和日志； 存储: 通过开放/可插拔接口储存Skywalking数据。现有 Elasticsearch，H2，MySQL，TIDB，或是自行实现相关存储接口； UI: 是一种高度可定制的基于Web的界面，允许Skywalking最终用户可视化和管理Skywalking数据；\nSkywalking 则架构设计上更为现代，充分利用了开源社区的优势，整合了多种能力，如自动探针、手动SDK、以及服务网格 istio等。\n   skywalking的数据模型   tracing, Skywalking 原生数据格式，支持 Zipkin V1 和 V2，包括 Jaeger。 metrics，Skywalking与Service Mesh平台（例如Istio，Envoy和LinkerD）集成，以将可观察性构建到数据平面或控制平面中。此外，Skywalking本机代理可以在metrics模式中运行，这大大提高了性能。 logging, 包括从磁盘或网络收集的日志，本机代理可以自动将跟踪上下文与日志绑定，或使用 Skywalking 绑定跟踪并记录文本内容。  其中 tracing 数据采用Cross Process Propagation Headers Protocol和Cross Process Correlation Headers Protocol进行传输，这些数据通常在 HTTP/MQ/HTTP2的头中进行透传。\n而 SkyWalking Trace Data Protocol V3 则是Agent/SDK传输给平台后端的协议，虽然协议名称这么多花里胡哨，但是最终发送给平台后端的数据使用grpc定义的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  message SegmentObject { // A string id represents the whole trace.  string traceId = 1; // A unique id represents this segment. Other segments could use this id to reference as a child segment.  string traceSegmentId = 2; // Span collections included in this segment.  repeated SpanObject spans = 3;}message SpanObject { // The number id of the span. Should be unique in the whole segment.  // Starting at 0.  int32 spanId = 1; // The number id of the parent span in the whole segment.  // -1 represents no parent span.  // Also, be known as the root/first span of the segment.  int32 parentSpanId = 2; // Start timestamp in milliseconds of this span,  // measured between the current time and midnight, January 1, 1970 UTC.  int64 startTime = 3; // End timestamp in milliseconds of this span,  // measured between the current time and midnight, January 1, 1970 UTC.  int64 endTime = 4; // The status of the span. False means the tracked execution ends in the unexpected status.  // This affects the successful rate statistic in the backend.  // Exception or error code happened in the tracked process doesn\u0026#39;t mean isError == true, the implementations of agent plugin and tracing SDK make the final decision.  bool isError = 11; }   在 SkyWalking 中，TraceSegment 是一个介于 Trace 与 Span 之间的概念，它是一条 Trace 的一段，可以包含多个 Span。在微服务架构中，一个请求基本都会涉及跨进程（以及跨线程）的操作，例如， RPC 调用、通过 MQ 异步执行、HTTP 请求远端资源等，处理一个请求就需要涉及到多个服务的多个线程。TraceSegment 记录了一个请求在一个线程中的执行流程（即 Trace 信息）。将该请求关联的 TraceSegment 串联起来，就能得到该请求对应的完整 Trace。\n 笔者人为， 这个Segment的设计定义不够清晰，没有办法明确划分边界，对上够不着trace对下又细不到span，这个设计的合理性还是存在疑问的。\n日志记录和传输格式则通过SkyWalking Log Data Protocol定义(协议格式太多往往系统过于复杂)。\n支持多种后端存储 目前分析支持多种存储后端，包括H2, ElasticSearch, MySQL, TiDB等\n   SkyWalking 总结  SW的设计是相对复杂的，生态也比较成熟，支持多种存储后端，也支持多种采集器方式，尤其是在 agent的支持方面，支持多语言、多方式，这点还是非常强大的，在开放标准兼容性方面，主动适配了Zipkin的协议，也为后续的扩展打好了基础。\n在 javaagent 方面，目前分析主要是支持系统层面的能力，框架和调用层面的支持并没有详细说，分析下来应该是有待提高的。\n监控的精细程度， Pinpoint 就略胜一筹了，能够做到代码级别的监控，而UI界面则SkyWalking就更加强大了,应该说是各有千秋，可以按照各自需求选用。\n   追踪系统标准  分布式系统追踪与监控出来有一段时间了，因此社区也已经自发地形成一些标准，这些标准包括OpenTracing、 OpenCensus 和 OpenTelemetry，这些标准都旨在构建更有效的方式来监控分布式系统。OpenCensus 是一个 Google 开源社区项目，其中 OpenTracing 是云原生计算基金会项目。OpenTelemetry 则是OpenCensus和OpenTracing的集大成者。\n   OpenTracing   “OpenTracing 用于跟踪的标准化 API，并提供了一种规范，开发人员可以使用该规范来检测服务或库以进行分布式跟踪。OpenTracing 还为开发人员提供了一种收集指标的方法，尽管它不是开箱即用的实现。”\n 所以，OpenTracing 是标准定义，不一定包括实现。\n   Open Census   “OpenCensus 是一组特定于语言的库，用于检测应用程序、收集统计数据（指标）并将数据导出到支持的后端。”\n 所以 OpenCensus 是一组实现，但是没有平台无关的标准定义。\n   OpenTelemetry  上述两个系统其实致力于解决同一个问题，而且各具优势，因此它们及时结合是很自然的。而且，OpenTelemetry 则接替了他们。\n “OpenTelemetry 使强大的便携式遥测成为云原生软件的内置功能。提供一组 API、库、代理和收集器服务，以从您的应用程序中捕获分布式跟踪和指标。”\n 简单来说：OpenCensus + OpenTracing = OpenTelemetry\n   一些技术细节     聊聊SOFA Tracer  SOFATracer 是蚂蚁金服开发的基于 OpenTracing 规范 的分布式链路跟踪系统，其核心理念就是通过一个全局的 TraceId 将分布在各个服务节点上的同一次请求串联起来。通过统一的 TraceId 将调用链路中的各种网络调用情况以日志的方式记录下来同时也提供远程汇报到 Zipkin 进行展示的能力，以此达到透视化网络调用的目的(来自官网)。\n之所以特意提及SOFA Tracer 是因为，这个系统其实设计的还算优雅（另一个原因是本人自己用过），经历过大规模的工程实践中，几个理念可以拿出来分析一下。\n异步落地磁盘的日志打印能力 首先SOFATracer采用了打印日志的方式，不进行网络上报，我认为这是有好处的，首先不会增加网络实时通信的压力，在大促的时候可以减少对整体系统的影响。其次是持久化效果好，及时采集服务宕机，重启之后依旧能够采集到遗失的数据。\nSLF4J的兼容性 事实上直接采用了SLF4J的能力，在大集团整体采用类似日志框架的前提下（这个不难统一）依赖冲突的情况将会大大缩减，集成成本大大降低。\n遵循 Open Tracing 规范 遵循标准规范的好处就是，可以直接使用 zipkin 的UI进行展示（当然内部采用了更加高级的UI)，在实际的工程当中，故障其实是通过两个系统完成的，trace定位服务，日志检索ELK定位错误。我们其实要有这样的认知，在服务量级足够大之后，存储和分析的压力都非常大，依赖于追踪系统完成根因分析的成本相对还是比较高的，这个时候人力成本是否应该纳入考虑？\nSOFA Tracer 的接入方式相对轻量 尽管也是采用SDK接入的方式，由于框架比较统一，只需要引入Maven插件即可完成接入，不需要太侵入业务，这个成本相对还是低廉的，这其实是在 agent 探针和完全SDK代码埋点之间的一个平衡，我觉得这个设计还是非常绝妙的。\nTraceId 和 SpanId 生成规则 SOFA Tracer 将TraceId和SpanID的生成规则讲得明明白白：\nTraceId 一般由接收请求经过的第一个服务器产生，产生规则是： 服务器 IP + 产生 ID 时候的时间 + 自增序列 + 当前进程号 ，比如：\n1  0ad1348f1403169275002100356696   而SpanId则通过如下方式生成：\n SpanId 目前的生成的规则参考了阿里的鹰眼组件。\n 应该说，上述的设计是优雅的，足够用，又不冗余，用29字节标识traceID，后续的spanID又足够简洁，符合标准，又不显冗余。\n   Javaagent 技术原理  java agent 的旁路增强原理，利用JVMTI对Java代码进行增强。\n JVMTI （JVM Tool Interface）是Java虚拟机对外提供的Native编程接口，通过JVMTI，外部进程可以获取到运行时JVM的诸多信息，比如线程、GC等。Agent是一个运行在目标JVM的特定程序，它的职责是负责从目标JVM中获取数据，然后将数据传递给外部进程。加载Agent的时机可以是目标JVM启动之时，也可以是在目标JVM运行时进行加载，而在目标JVM运行时进行Agent加载具备动态性，对于时机未知的Debug场景来说非常实用。\n 我们使用Pinpoint的配图进行说明，如下图所示：\n在用户类进行加载时，将会被 Pinpoint 的agent拦截到，并根据Pinpoint的agent内置的拦截器代码规则，针对特定的代码进行前后AOP增强，然后将追踪信息放入到追踪上下文中。 在 Pinpoint 中，API 拦截部分和数据记录部分是分开的。拦截器被注入到我们想要跟踪的方法中，并调用 before() 和 after() 方法来处理数据记录。通过字节码检测，Pinpoint Agent 可以仅从必要的方法记录数据，从而使分析数据的大小变得紧凑。\n但是问题来了，用户写的代码已经被编译成字节码了，那么Pinpoint又如何完成拦截呢？这就涉及字节码动态修改了，其实原理和cglib的代理原理类似，但是是通过javaagent这种方式实现的，通过在启动的时候添加-ajavaagent参数完成增强，而字节码修改能力比较常见的库就有 byte-buddy等。\n字节码检测技术必须处理 Java 字节码，往往会增加开发风险，同时降低效率。此外，对开发人员的要求也非常高，需要对拦截的代码点位非常熟悉，对不同的框架也非常了解，否则开发agent是非常有风险的一件事情。\n   分析总结  其实这篇文章讲的非常仔细，也基本覆盖了目前主流的系统，目前开源的系统都各有优缺点，需要根据实际情况进行分析取舍，如果希望能够有较为详细的信息供分析，可以考虑pinpoint，如果希望有比较好的UI, 对系统比较小的影响，可以考虑SkyWalking；如果希望使用一个相对轻量级的追踪系统，Zipkin也是可以考虑的。\n本文对现有的追踪系统进行了分析，也初步介绍了一些分布式链路追踪系统的标准，希望能够给大家在分析设计过程当中提供参考。\n   参考文献   Dapper, a Large-Scale Distributed Systems Tracing Infrastructure Magpie: online modelling and performance-aware systems X-Trace: A Pervasive Network Tracing Framework Pinpoint: Problem determination in large, dynamic, Internet service Dapper — Google’s Secret Weapon Pinpoint documentation pinpoint-techdetail zipkin documentation CAT wiki CAT model SkyWalking 的核心概念 全链路监控（一）：方案概述与比较 SOFA Tracer TraceId 和 SpanId 生成规则 Guide to Java Instrumentation Java 动态调试技术原理及实践  ","date":"2022-02-01","permalink":"https://chenquan.me/posts/tracing-system-analysis/","tags":["tracing","metrics","system"],"title":"全链路追踪系统技术分析"},{"categories":null,"contents":"   引言  在一个分布式系统当中，如果需要提供可靠服务，数据操作（读写）的冗余是必要的。假设现在我们有一个分布式数据库集群，其中的服务器节点会相互同步数据。一种简单的思维，如果网络正常，服务器稳定，一切顺利，我们只需要把数据写入其中一台服务器，集群中的所有服务器就都会同步到写入的数据。但事情往往没有这么简单，实际上这台写入的服务器可能会宕机、断网，甚至代码也会有bug，这些问题都有可能导致数据同步失败，之前的写入操作实际上有可能是失败的，所有服务器无法同步到数据。同样地，如果写入成功，我们随便挑一台服务器读取数据就没有问题了吗？如果这台服务器同步速度慢一些，那么我们读取的值也是不对的。那么，如果我们在写入的时候多写如几台服务器不就好了，一种极端思维是我写入所有服务器，这样当然所有服务器上的数据都是最新的；在读取的时候我们也可以读所有服务器，这样一定能够读到最新的数据，但是这样显然很耗费资源，我们能不能读写一部分也能够得到确定的最新结果呢，如果可以的话，读写多少服务器比较合适呢？\n这个服务器数量，就是所谓的 quorum，一般来讲，在写入数据的时候，只需要达到 quorum 台服务器，我们就可以认为写入成功，同样地，在读取数据的时候，只需要读取 quorum 台服务器，就可以认为读取的结果是正确的，并且是最新的。\n显然这个quorum值是取决于分布式集群所有服务器的数量的，全网服务器数量越多，quorum 越大，当然，如果 quorum 越大，那么需要操作的服务器数量就越多，操作开销也越大。\n那么，quorum 多大比较合适呢？\n在集群的数据同步过程当中，存在几种情况：\n一是并发操作，就是多人同时操作数据集群，如果有两个人进行写入操作，比如 Alice写入 A = 2， 同时 Bob 写入 A =3 ，如果一半服务器接受 A=3，而另一半接受 A=4 是不行的，这就出现了集群脑裂的情况，为了避免这个情况，要求我们至少写入集群中的多数节点。\n另一种是非串行化读写，如果我们在写入数据的完成之后，用户立即来读取数据了，但是恰好用户读取的服务器还没有同步到最新数据，用户读的数据是过时的，这样也是不对的，我们应该保证，写入成功之后，读取的值应该是上次写入之后的结果，所以我们至少要读到一个写入操作过的节点。\n那么写入操作和读取操作至少都需要多少个节点才能保证上述情况不会发生呢？问题将会在下文中揭晓。\n   Prerequisite     共识问题 (Consensus Problem)  分布式系统的共识问题（Consensus Problem）是指寻找一种协议，使得该协 议满足以下三大属性：\n 一致性（Agreement）：所有的非缺陷进程都必须同意一个值； 正确性（Validity）：所有非缺陷进程所同意的值必须来之非故障进程所提案的值； 可结束性（Termination）：每个非缺陷的进程必须最终确定一个值。  通常也把一致性和正确性合称安全性（Safety），把可结束性称为活性 （Liveness），而在分布式系统的算法和设计中， 安全性和活性是两个非常重要的属性，更通俗地讲，这两个属性的另一种解释是：\n 安全性 (Safety) ：错误的值永远不会被采用（something “bad” will never happen）； 活性 (Liveness) ：最终正确的值将会被确定并同意，但是无法确定时间 （ something “good” will must happen, but we don’t know when）  也可通俗解释为：\n  活性 (Liveness) 就是集群能够确定一个值，通常来讲就是少数服从多数；\n  安全性 (Safety) 就是集群能够确定的值是正确值，而不是错误值。\n     错误类型与错误容忍(Fault Tolerance)  错误容忍指的是一个系统在其的某些部件出现错误之后依旧能够继续正常工作，这种能力叫做错误容忍。\n在分布式系统当中可能出现的错误主要有两种：\n  CF (Crash Fault)：宕机故障，系统中的某些节点可能出现宕机故障，不会响应请求，但是不会恶意响应；\n  BF (Byzantine Fault): 拜占庭故障，系统中的某些节点可能出现拜占庭故障，可能会不响应请求，也可能错误响应请求；\n   出现拜占庭故障的节点我们称为拜占庭节点。\n  能够容忍宕机故障的系统我们称为CFT（Crash Fault Tolerance）系统 能够容忍拜占庭故障的系统我们称为BFT（Crash Fault Tolerance）系统     FLP不可能结论  分布式系统理论中最重要的结果之一是由Fischer，Lynch和Patterson于1985年4月发表的短篇论文\u0026quot; Impossibility of Distributed Consensus with One Faulty Process’，该论文最终确定了异步环境中分布式过程可以实现的目标的上限，并获得了分布式计算领域最有影响力论文的Dijkstra奖。\n这个论文的结论称为“ FLP不可能结论”，解决了过去五到十年在分布式理论研究中一直存在的争议。众所周知，共识问题在同步系统中可以解决。非正式描述为，同步模型通过等待一个完整的步长等待来自处理器的答复，并假设如果未收到答复而崩溃，则可以检测到故障。\n在异步系统中，这种故障检测是不可能的，在异步设置中，处理器完成工作然后响应消息所花费的时间没有限制。因此，无法确定处理器是否崩溃还是花费了很长时间进行响应。\nFLP结论表明，在异步系统中，如果有一个进程可能出现崩溃，则没有解决共识问题的分布式算法。\n 科学告诉我们什么是不可能的；工程则告诉我们，可以付出一些代价，将不可能变成可行。\n    Quorum机制 (Quorum intersection)  Quorum 机制，是一种分布式系统中常用的，用来保证数据冗余和最终一致性的投票算法，其主要数学思想来源于鸽巢原理。Quorum 系统可以定义为一组集合（称为 Quorum 集合）这组集合满足一定的相交属性。在有冗余数据的分布式存储系统当中，冗余数据对象会在不同的机器之间存放多份拷贝。但是同一时刻一个数据对象的多份拷贝只能用于读或者用于写。\n在分布式系统中，冗余数据是保证可靠性的手段，因此冗余数据的一致性维护就非常重要。一般而言，一个写操作必须要对所有的冗余数据都更新完成了，才能称为成功结束。比如一份数据在5台设备上有冗余，因为不知道读数据会落在哪一台设备上，那么一次写操作，必须5台设备都更新完成，写操作才能返回。对于写操作比较频繁的系统，这个操作的瓶颈非常大。\nQuorum算法可以让写操作只要写完3台就返回。剩下的由系统内部缓慢同步完成。而读操作，则需要也至少读3台，才能保证至少可以读到一个最新的数据。\n分布式系统中的每一份数据拷贝对象都被赋予一票。每一个读操作获得的票数必须大于最小读票数（read quorum）（$V_r$），每个写操作获得的票数必须大于最小写票数（write quorum）($V_w$）才能读或者写。如果系统有$V$票（意味着一个数据对象有$V$份冗余拷贝），那么最小读写票数(quorum)应满足如下限制：\n $V_r + V_w \u0026gt; V$ $V_w \u0026gt; \\frac{V}{2}$  第一条规则保证了一个数据不会被同时读写。当一个写操作请求过来的时候，它必须要获得$V_w$个冗余拷贝的许可。而剩下的数量是$V-V_w$ 不够$V_r$，因此不能再有读请求过来了。同理，当读请求已经获得了$V_r$个冗余拷贝的许可时，写请求就无法获得许可了。\n第二条规则保证了数据的串行化修改。一份数据的冗余拷贝不可能同时被两个写请求修改。\nQuorum的读写最小票数可以用来做为系统在读、写性能方面的一个可调节参数。写票数$V_w$越大，则读票数$V_r$越小，这时候系统读的开销就小。反之则写的开销就小。\n再以一个分布式系统为例，系统中有 N 个服务器，客户端需要从这个分布式系统中进行写入并读取数据，我们将写入操作定义为$OP_w$ ，被写入的服务器集合为$Q_w$ ，将读取操作定义为$OP_r$ ，被读取的服务器集合为$Q_r$ ，假设故障的节点集合 $F$。\nQuorum 要求系统达到如下要求： $$ Q_w \\cap Q_r \\neq \\emptyset $$\n$$ Q_w - F \\neq \\emptyset $$\n$$ Q_r - F = \\emptyset $$\n$$ \\vert{Q_w}\\vert + \\vert Q_r \\vert \u0026gt; \\vert N \\vert $$\n$$ \\vert Q_w \\vert \u0026gt; \\frac{\\vert N \\vert}{2} $$\n  公式 (1) 说明了系统当中的多数原则，在写入和读取的成员应当是有交集 的，这样才能够保证读取到写入的正确值\n  公式 (2)、(3)说明了在读取和写入的过程当中至少成功操作一个正常服务器\n  公式 (4) 说明了多数原则的数量关系\n  公式 (5) 说明了串行化原则，一个系统不能同时进行两个不同的写入操作，交集属性确保任何读取操作都可以访问已写入的最新值，能够保证读写操作的正确性。\n     共识算法类型  以下图片来自区块链共识算法的发展现状与展望\n   PBFT 共识算法  PBFT 是 Practical Byzantine Fault Tolerance 的缩写，意为实用拜占庭容错算法。\n该算法首次将拜占庭容错算法复杂度从指数级降低到了多项式级，其可以在恶意节点不高于总数 1/3 的情况下同时保证安全性（Safety）和活性（Liveness）。\n   术语与变量     约定变量   集群数量定义为 $N$，其数量定义为 $|N| = n$ 拜占庭或者是宕机节点集合为定义为 $F$，其数量定义为 $|F| = f$ $ quorum$ 法定成员集合$Q$，即每次访问的节点数量$|Q|$     术语   Primary: 主节点 Replica: 副本节点 Client: 客户端，用于向共识集群发送消息，请求共识 View: 视图，Primary和replica共同达成的一个状态视图，所有节点都基于某个视图进行共识 Sequence Number：序列号，由主节点生成的序列号，用于标识共识轮次   Check Point: 检查点，如果某个序列号被确认，则成为检查点 Stable checkpoint: 稳定检查点，该检查点通常会被持久化     推定变量    在撰写本小节的时候，阅读了很多博客和文章，很多解释都模棱两可，没有真正讲清楚，笔者也尝试来解释一下 3f + 1的问题。\n 引言中我们抛出了一个问题，在分布式系统中需要读写多少台节点才可以满足正确性要求，即quorum 的值为多少比较合适？\nquorum 的值是根据不同的情形需要单独处理的，简单而言，在CFT系统当中，只需要达到2f + 1就可以了，因为每次操作都已经达到多数，不管读写我们都能够达成，满足 Quorum intersection 的要求。\n但是在BFT系统中，则不一样，下文将进行分析。\n 在BFT中，是允许同时存在故障节点和拜占庭节点的，所以才要求 3f+1，如果只允许拜占庭节点存在的话，其实 2f+1 也是可以满足要求的。\n 在一个由 $N$ 个节点组成的共识网络中，RBFT 最多能容忍$f$个节点的拜占庭错误，其中： $$ f=\\lfloor \\frac{N−1}{3} \\rfloor $$ 而能够保证达成共识的节点个数为： $$ quorum=\\lceil \\frac{N+f+1}{2}\\rceil $$ 那么这些值都是如何确定的呢？\n已知节点集合为 $N$, 拜占庭错失效节点集合为 $F$， 宕机或者是未访问的节点集合为 $X$ ，访问的最小节点集合($quorum$)为$Q$。\n 以上图为例，quorum = 3, 即一个黑色框选的范围，黑色框未框选的节点可能是有效节点，也可能是无效节点（黑色节点）。未框选的部分为$X$。\n （1） Liveness, 活性要求。整个系统需要能够确定一个值。\n可以这么理解，为了能够获取一个值，最极端的方式就是访问所有节点$N$， 这是 $Q$ 的上届，当然我们的目标是尽量减少访问的数量，降低开销，减少多少访问的数量比较合适呢？已经知道了有$|F|$个节点是拜占庭节点，活性要求能够拿到一个值，而这$|F|$个节点可能都不响应，所以我们至多可以访问 $|N| - |F|$个节点，至少需要访问$|F| + 1$个节点，才能够拿到最终结果。\n所以我们有：\n$$ |N| - |F| \\geq |Q| \\geq |F| + 1 $$\n 如果我们访问的|Q|个节点都是宕机节点，其下界可以保证我们一定能够得到一个结果；而其上界则缩小了其能够访问的节点数量上限，虽然我们依旧可以访问$|N|$个节点，但是可以确定的是其中$|F|$次访问是没有意义的。\n但是这个时候如果访问的这 $|N| -|F|$个节点恰好都是拜占庭节点呢？我们也不知道哪些节点是拜占庭节点，这个问题是下面的“安全性”要求需要解决的。\n （2）Safety,安全性要求。已知拜占庭节点和正常节点一样发送消息，但是会发送错误的消息误导。\n一般来讲，在访问集群的时候是有两个过程的，一个是写(w)过程，一个是读(r)过程，经过了两次交互，每次交互的节点集群子集是不一定相同的，我们把第一次写过程访问的集群称为$Q_w$，把第二次读过程访问的集群称为$Q_r$。\n实际上我们无法确定写操作的响应节点集群$Q_w$和读操作的响应节点集群$Q_r$是否是同一组。所以在上述基础上，要求读写集群需要有交集（quorum intersection），即 $|Q_w \\cap Q_r| \\neq \\emptyset$。只有这样，我们才能够读到正确的写入的结果。 但是如果交集正好都是拜占庭节点的话那就被完美骗过了（读过程和写过程都信任了拜占庭节点），所以，又要求 $|Q_w \\cap Q_r| \u0026gt; f$ ，即读写集合数量都至少要比$f$大，那么我们可以得到 $|Q_w \\cap Q_r| = (|N|-|F|) + (|N|-|F|) - |N| \u0026gt; |F|$ ，因为节点是整数，所以有: $$ (|N|-|F|) + (|N|-|F|) - |N| \\geq |F| + 1 $$\n$$ \\Rightarrow |N| \\geq 3|F| + 1 $$\n所以我们能够得到 $f = ⌊\\frac{n-1}{3}⌋$，此时能够确定的 $quorum = |Q| = n-f \\geq 2f + 1 $\n可以简单推出:\n$$ quorum \\geq \\lceil \\frac{N+f+1}{2} \\rceil $$\n 3f+1 的一种解释：\n节点总数是n，其中作恶节点有f，那么剩下的正确节点为n- f，意味着只要收到n - f个消息就能做出决定，但是这n - f个消息有可能有f个是由作恶节点冒充的，那么正确的消息就是n-f-f个，为了多数一致，正确消息必须占多数，也就是 n - f - f \u0026gt; f 但是节点必须是整数个，所以 n 最少是 3f+1 个。\n 笔者认为上述解释是有问题的，首先作恶节点有f个，剩下的正确节点有n-f个，但是并不意味着收到 n-f个消息就能做出决定，我可以收到 f+1 个消息就可以做出一种决定，诚然 f+1 个消息有可能有f个都是错误的，实际可能无法确定结果，所以可以调整为收到 2f + 1 个消息确定结果,因为 f + f + 1 个消息肯定有 f+1个消息是诚实节点发出的，并且为多数，但也没有说明这是最小值；2f + 1 即前面讨论的quorum，但是 quorum与 N - f的关系也其实是没有讲清楚的，因为 2f + 1 可以等于 N ，也可以等于N-1等等，综上，其实这种解释不够严谨，但是比较好理解。\n 3f+1的另一种解释：\n对于 pbft 算法，因为 pbft 算法的除了需要支持容错故障节点之外，还需要支持容错作恶节点。假设集群节点数为 N，有问题的节点为 f。有问题的节点中，可以既是故障节点，也可以是作恶节点，或者只是故障节点或者只是作恶节点。那么会产生以下两种极端情况：\n 第一种情况，f 个有问题节点既是故障节点，又是作恶节点，那么根据小数服从多数的原则，集群里正常节点只需要比 f 个节点再多一个节点，即 f+1 个节点，确节点的数量就会比故障节点数量多，那么集群就能达成共识。也就是说这种情况支持的最大容错节点数量是 （n-1）/2。 第二种情况，故障节点和作恶节点都是不同的节点。那么就会有 f 个问题节点和 f 个故障节点，当发现节点是问题节点后，会被集群排除在外，剩下 f 个故障节点，那么根据小数服从多数的原则，集群里正常节点只需要比 f 个节点再多一个节点，即 f+1 个节点，正确节点的数量就会比故障节点数量多，那么集群就能达成共识。所以，所有类型的节点数量加起来就是 f+1 个正确节点，f 个故障节点和 f 个问题节点，即 3f+1=n。   这种解释，第一种情况是没有问题的，很好理解，但是在第二种情况中，故障节点和作恶节点是否加在一起是“有问题节点”这个点其实有点混乱了，按照第二种解释的说法，故障节点和作恶节点都有f个，那当然很容易就推出来需要f+1个节点正常，N= 3f+1，但其实应该是，故障节点和作恶节点总共加起来一共 f 个，这种解释其实举个例子就可以说明了，假设我们现在一共有4个节点，其中一个是拜占庭节点，一个是宕机节点，集群中只有两个节点可以正常工作，其实整个算法运行不起来的。\n   PBFT 主要思想  PBFT 的主要思想其实是：\n 将节点数量固定（3f +1） 通过三阶段来处理恶意主节点 通过更大的quorum集合来解决共识失效 通过授权通信（消息签名）解决消息认证问题     常规流程 normal-case     初始定义   i : 节点ID (replica id) ，应该是在 $[0, N-1]$ 范围内的值 v# : 视图编号(view number), 初始为零，用 v# 表示 Primary : 主节点，通常采用模运算取得: Primary = v# mod N​ log : 操作日志， 通常记录了当前收到的消息,形式为 \u0026lt;v#, seq#, status, d\u0026gt;  seq# 为 sequence number status 为 pre-prepared 、prepared或者是committed m 即本轮共识的具体操作，可以是数据库操作，也可以是区块写入操作   d(m) ，是m的密码学摘要信息 (digest)     标准算法流程  算法示意图\nBFT算法主要流程\n 原论文(osdi99)中是要求客户端直接把请求发送给主节点的，我觉得可以辩证理解，有可能此时这个主节点有问题，需要切换主节点，这个过程客户端感知是比较滞后的，略有缺陷，所以我选了一张更加合适的图作为示意。原论文当中的示意图：\nBFT算法主要流程 (osdi99)\n 算法核心过程（正常流程）\n  STEP 1: 客户端发送请求给主节点（或者发给所有节点），图示是发给所有节点的。之后主节点将会触发三阶段协议，但是这里有一个优化，节点可以先把请求缓存起来，等到攒够一堆请求之后再一起发送，这样可以降低网络开销和系统负载，这个优化是可选的。\n  STEP 2: 主节点发送 pre-prepare消息给所有节点\n 主节点广播消息形式为 \u0026lt;\u0026lt;PRE-PREPARE, v#, seq#, d, sig\u0026gt;, m\u0026gt;(p)，其中(p) 表示由主节点发出, d是当前消息摘要，m为原始消息, sig 为d的数字签名 主节点将上述的广播消息存储在本地日志中，并标记本节点为 pre-prepared状态 注意，主节点也有可能是恶意节点，可能出现如下异常  针对同一个 d 发送不同的 seq# 给不同的从节点 发送重复的 seq#      STEP 3: 从节点检查主节点发送的 pre-prepare消息，并进行验证\n 验证过程包括：  消息中的数字签名sig是否合法 是否在同一个v# 是否有接收到过一个拥有相同v#和seq#但d是不同的历史消息 seq#是否在水位线H和h之间 水位线是为了防止一个异常主节点快速消耗seq#空间而设置的，当然还有别的用途   记录上述操作到日志中，标记本节点为 pre-prepared状态 发送 prepare消息给所以偶节点 prepare消息的形式为\u0026lt;PREPARE, v#, seq#, d, i,\u0026gt;(i) 其中(i) 标识从i节点发出 主节点将上述的广播消息存储在本地日志中 该过程需要所有节点相互广播    STEP 4: 所有节点接收并匹配所有的 prepare消息，并进行处理：\n  每个节点达到 prepared(m, v#, seq#, i)需要满足如下条件：\n 拥有一个 操作请求 m 拥有一个 pre-prepare ，其view为 v# 下并且其sequence是 seq# 以及 2f 个从其他节点收到的prepare消息，对应前面的pre-prepare消息（通过检查其 view 是否相同，sequence是否箱体以及digest 是否相同)，加上自己的消息就达到2f+1了    达成上述条件之后，标记本节点为 prepared(m, v#, seq#, i)状态\n  达成prepared(m, v#, seq#, i)状态之后，各个节点发送 commit 消息给所有节点\n  commit消息形式为 \u0026lt;COMMIT, v#, seq#, D(m), i\u0026gt;(i)\n  此时节点已经能够确认所有的诚实节点已经准备好提交相同的值了\n    prepared 状态证明\n  上图解释了为什么能够确定诚实节点已经准备好了，其中的 v = v#， n = seq#\n    STEP 5: 所有节点接收所有的 commit消息，并进行处理\n  首先将收到的commit消息写入日志，其次要求这些消息是处于水位线 h和H之间的\n  原论文中还定义了 committed(m,v#,seq#) 以及 committed-local(m,v#,seq#,i)这两种状态 ，前面是全网状态，后面是本地节点状态，全网状态在工程实践中用处不大， 因为我们没有办法站在上帝视角去观察，所以只需要研究committed-local即可，原文这么写可能是为了学术上的要求。\n  要达成committed-local状态需要满足以下条件：\n 当前节点已经达成prepared(m, v#,seq#,i)状态 并且本节点已经收到了 2f + 1个commit消息 (可以包括自己的) 上述收到的 commit消息需要与前面的prepared状态保持对应，即相同的 view, seq 以及 digest 等    达成 commit-local(m, v#, seq#, i)之后，将会按照m中的请求顺序进行执行，当然执行和共识可能是异步的，所以需要从低的seq# 开始执行\n  执行完成之后，所有节点将发送结果给到客户端\n    STEP 6: 客户端收到超过2f+1的一致消息则确认当前结果成功\n  整个过程不需要要求所有的消息是有序到达的，因为seq#会保证顺序，只需要对应的pre-prepare， prepare和commit消息都是完整的即可。\n   数据结构     State状态数据  节点的状态主要包含三部分：\n 世界状态，包括已经处理过的交易 消息日志 当前 view     Three Phase Protocol  这里列出了三阶段协议相关的消息结构，其中 PRE-PREPARE 消息包含新生成的区块，其他消息则主要包含一些 id、sequence number、区块内容摘要和签名等信息。\n   一种优化流程   以下是基于公开资料收集的 Hyperchain 优化之后的RBFT算法\n RBFT 做的优化之一是，客户端发送请求到任意节点，如果这个节点不是主节点的话，将会进行一次广播。\n优化之二是，主节点收到交易之后会进行验证，并把验证结果放在pre-prepare中进行再全网广播，pre-prepare是以区块为单位处理的，这样的 pre-prepare 消息中既包含了排好序的交易信息也包含了区块验证结果。\n从节点在收到主节点的 pre-prepare 消息后先检查消息的合法性，检查通过后广播 prepare 消息表明本节点同意主节点的排序结果；在收到 quorum-1 (即2f )个 prepare 消息后从节点才会开始验证区块，并将验证结果与主节点的验证结果进行比对，比对结果一致则广播 commit 表明本节点同意主节点的验证结果，否则直接发起 view-change 表明本节点认为主节点有异常行为。\nRBFT 常规流程具体分为如下几个步骤：\n **交易转发阶段：**客户端将交易发送到区块链中的任意节点（包括共识节点与记账节点），其中记账节点在收到交易后会主动转发给与其相连的共识节点；而共识节点在收到客户端的交易后将其广播给其他共识节点，这样所有共识节点的交易池中都会维护一份完整的交易列表；   共识节点就是参与RBFT共识过程的节点，记账节点是不参与共识，只接受结果并写入区块的节点\n   **PrePrepare 阶段：**主节点按照如下策略进行打包：用户可以根据需求自定义打包的超时时间（batch timeout）与打包的最大区块大小（batch size），主节点在超时时间内收集到了足够多（超过最大区块大小个数）的交易或者超时时间到达后仍未收集到足够多的交易都会触发主节点的打包事件。主节点将交易按照接收的时间顺序打包成块，并进行验证，计算执行结果，最后将定序好的交易信息连同验证结果等写入 pre-prepare 消息中广播给所有共识节点，开始三阶段处理流程；\n  Prepare 阶段： 从节点在收到主节点的 pre-prepare 消息后，首先进行消息合法性检查，检查当前的视图与序列号号等信息，检查通过后向共识节点广播 prepare 消息；\n  **Commit 阶段：**从节点在收到quorum-1 即(2f) 个prepare 消息以及相应的 pre-prepare 消息后进行验证，并将验证结果与主节点写入pre-prepare 消息中的验证结果进行比对，比对结果一致则广播 commit 表明本节点同意主节点的验证结果，否则直接发起 view-change 表明本节点认为主节点存在异常行为，需要切换主节点；\n  **写入账本：**所有共识节点在收到 quorum 个 commit 消息后将执行结果写入本地账本。\n     检查点机制与垃圾回收  在上述过程当中，整体流程每个阶段都写入了日志，基本上每一轮都会产生很多日志缓存；如此下去明显系统存储是不够用的，为了解决上述问题，PBFT引入了垃圾回收（GC）机制，通过检查点 (checkpoint)来进行垃圾回收。\n共识过程中的日志log需要一直保存在节点中，一直到这个日志状态已经被至少 f+1 个非拜占庭副本节点处理过了，并且在view-change过程当中需要能够将这个已经处理过的日志证明给其他节点。\n进一步地，还有一种情况是，如果一些副本节点因为某些原因落后了，但是其他非拜占庭节点已经把一些消息都清除了，落后的节点无法通过日志跟上，只能通过直接同步状态跟上，其他节点也需要证明上述状态是合法的。\n当然生成这样的证明的成本是高昂的，所以我们不需要每个区块都生成一次，通常来说周期性生成一次比较好，一般来说这个周期间隔是固定的（比如每100个 seq# ） ，当共识轮次能够被100整除时，将会生成一个检查点(checkpoint)，经过共识确认的检查点被称为稳定检查点stable checkpoint，当然稳定检查点需要带有至少2f+1个节点签名，即所谓的proof。生成稳定检查点之后，就可以清除该检查点之前的消息缓存，实现GC。\n检查点生成过程\n当一个 replica i生成了一个检查点，将会封装并广播消息 \u0026lt;CHECKPOINT, seq#, d(state), i\u0026gt;，所有的备份节点收到了2f+1个拥有相同的 d(state)的检查点消息（包括自己的消息），将会生成稳定检查点，而 2f+1个检查点消息即稳定检查点证明proof。 其中d(state)是当前状态摘要信息。\n当稳定检查点生成之后，将会删除稳定检查点之前的 pre-prepare、prepare、commit消息，当然也会删掉更早的检查点消息。\n状态摘要机制\n状态摘要可以通过增量哈希实现，减少计算量。\n水位线机制\n水位线机制用于限制哪些范围的消息可以被接受。水位线包括低水位线(low-water mark) h和高水位线(high water mark) H = h + k。\n通常来讲，低水位线是最近的一个稳定检查点的seq#，而高水位线需要在低水位线上加上一个常量k, 常量 k 需要足够大，避免共识一致需要等检查点稳定，但是又不能让共识过程跑太远，避免稳定检查点生成失败后大量共识过程需要重做。\n水位线机制一方面是为了限制序号空间，另一方面是为了让检查点生成的时候不阻塞共识过程，但又不至于共识过程跑得太快，检查点生成太慢，如果一直领先很多，检查点就没有意义了。\n   视图变更流程 view-change     标准VC流程  视图变更时为了保证系统能够有一定的活性liveness，避免在主节点出现问题的时候无法恢复。\nview-change 通常是由超时机制触发的，这个超时定时器一般是系统处理交易伊始就设置的，当系统不再接受交易了。就不需要设置这个定时器。\n 当然在工程上，如果一直设置和取消定时器是很麻烦的，所以，一般来说会一直启用定时器，并通过主节点心跳机制刷新定时器时间。\n论文原话是定时器是为了避免从节点长时间的等待请求，这个需要结合pre-prepare进行说明，在通常情况下，主节点将会发送pre-prepare 来正常进行共识，从节点也可以通过该消息确认主节点是否存活，因此如果超过一段时间没有收到 pre-prepare的话就会认为该主节点有问题了。但是在工程上或者在实际运行当中，长时间没有pre-prepare是常见的，因此一般会通过心跳来进行探测保活，所以也不一定非得要用定时器。\n 当这个定时器是设置在v#的，如果它超时了，将会触发 view change，然后把 view 设置为 v#+1，此时将停止接受消息（只接受三种消息: CHECKPOINT, VIEW-CHANGE,NEW-VIEW ）\n当定时器超时，系统将会广播 \u0026lt;VIEW-CHANGE, v#+1, seq#(stable_checkpoint), C-set, P-set,i\u0026gt;(i), 其中：\n (i)标识由节点i发出 seq#(stable_checkpoint)是上一次稳定检查点的 sequence（对于节点i而言，其他节点的未必一致） C是 2f+1个能够证明seq#(stable_checkpoint)是正确检查点的的消息集合 P是 $P_m$ 的集合，$P_m$是针对每一个消息m收到的 pre-prepare 消息（不包括客户端请求），这些消息的sequence 比前面的seq#(stable_checkpoint)大，以及对应这些pre-prepare消息的2f个有效的，并经过各个节点签名的prepare消息，也就是不稳定的prepare消息，这些消息需要重新确认，通过P-set可以把原来的在稳定检查点之后确定的交易进行重新共识。  此时，系统的view已经变成 v# + 1了，当 v#+1这一轮的主节点从其他节点收到了 2f 个有效的 view-change消息，它将会广播\u0026lt;NEW-VIEW, v#+1, V-set, O-set\u0026gt;(p) ，其中：\n  Vset 是有效的 view-change 消息集合，包括 这一轮新的主节点发送出去的view-change消息（或者是将要发送的消息），\n  Oset 是 pre-prepare 消息集合，通过以下方式获得：\n  主节点需要决定 min-s 和 max-s, min-s 是在Vset中的最近一次稳定检查点的 sequence, max-s 是Vset中所有prepare消息里面最大的 sequence\n  主节点需要创建一个新的 pre-prepare消息, 在新的view v#+1 上针对每一个介于 min-s 和 max-s 之间的所有seq# 重新构建 pre-prepare消息，这里又有两种情况：\n P-set集合当中至少有一个元素在V-set中，判断条件是它们拥有相同的 Sequence number 或者是 P-set和V-set没有交集，一般来讲就是P-set为空   简单来说，就是在 view-change 消息当中的P-set，需要转换为现在的O-set， 转换的前提是 view-change 消息中已经经过共识的Vset中包括了P-set中的成员，这中间的P-set是无法校验的，只能校验C-set的一致性，每个节点的P-set可能都不一样，当然，P-set的元素都是经过2f+1个节点确认过的\n 在第一种情况下，主节点需要创建 \u0026lt;PRE-PREPARE, v# + 1, seq#, d\u0026gt;(p) 其中d 是在 V-set中拥有最大的 view 值的，并且是特定 sequence number (因为在V-set中可能有很多个相同 seq# 但是不同 v# 的消息)的pre-prepare消息所携带原始请求的消息摘要.\n在第二种情况下，主节点需要创建一个 \u0026lt;PRE-PREPARE, v+1, seq#, d_null\u0026gt;(p)，d_null是一个针对 null请求的特殊摘要，为了就是让当前的sequence能够跟上，实际上该共识完成之后不会做任何事情。\n之后将把 O-set中的消息存放到自己的日志中，如果min-s比自己本地的稳定检查点还大的话，将插入稳定检查点，并清除之前的日志。\n最后进入 view v#+1 状态。\n    从节点收到了 new-view消息之后,校验其签名，并且如果Oset是正确的，将计算处理Oset的数据，把OSet中的消息存储到自己的日志当中，视情况移动稳定检查点，主要流程和主节点类似，然后进入 view v#+1 状态。\nmin-s 和 max -s 是为了避免在VC过程当中重新处理用户的请求，显然 min-s是一个stable checkpoint, 而max-s是介于stable checkpoint和下一个未生成的stable checkpoint 之间的。C-set 确定了 view-change 之前的所有稳定检查点状态，并且得到了2f+1个节点同意，所以稳定检查点之前的状态不会丢失。P-set 则包括了不稳定的prepare消息，在视图完成变更之后，需要重新封装为prepare进行处理。\n O-set 在hyperledger fabric 0.6中被成为Q-set\n  REF: 另外这段来自清源的博客的总结的挺好的，供参考：\n总结一下，在view-change中最为重要的就是C，P，Q三个消息的集合，C确保了视图变更的时候，stable checkpoint之前的状态安全。P确保了视图变更前，已经PREPARE的消息的安全。Q确保了视图变更后P集合中的消息安全。回想一下pre-prepare和prepare阶段最重要的任务是保证，同一个主节点发出的请求在同一个视图（view）中的顺序是一致的，而在视图切换过程中的C，P，Q三个集合就是解决这个问题的。\n  REF: 美图技术团队的说法\n如果 max-s - min-s \u0026gt;0，则产生消息 \u0026lt;pre-prepare,v+1,n,d\u0026gt; ；如果 max-s - min-s =0，则产生消息 \u0026lt;pre-prepare,v+1,n,d(null)\u0026gt;。\n    数据结构     view-change 数据结构  VIEW-CHANGE 消息包含的内容比较多：\n首先需要基于一个稳定的 checkpoint，因此需要包含 2f+1 个 CHECKPOINT 消息以证明该 checkpoint 是有效的。\n然后，在该 checkpoint 之上的所有 sequence number，都需要打包对应的 PRE-PREPARE 消息以及 2f 个 PREPARE 消息。\n   New-view 数据结构  NEW-VIEW 消息首先需要包含 2f+1 个 VIEW-CHANGE 消息，以证明确实有超过 2/3 的节点同意在更高的 view 上进行新一轮共识。\n然后，根据收到的所有 VIEW-CHANGE 消息中的 checkpoint 信息，找出最小值 min_s 和最大值 max_s，打包该区间内的每一个 sequence number 对应的 PRE-PREPARE 消息。\n特别的，为了减少重复验证，如果在某个 sequence number 上从未进行过 view change（即第一轮就达成了共识），则 PRE-PREPARE 中包含一个特殊的 null 请求的摘要信息。\n具体逻辑参见下图：\n   一种优化流程  view-change 在主节点异常的时候需要需要能够发现，而主节点异常无外乎两种情况：\n  主节点宕机\n  主节点是恶意节点，发送错误消息\n  针对 1. 可以用心跳机制进行检测；\n针对 2. 首先我们要明确其检测恶意节点的机制，一种方式是校验prepare消息中的请求签名,这种方式可以发现节点篡改，而针对消息顺序的篡改，可以通过一种简单约定，比如必须要按照绝对时间顺序排序等；\n当然如果一直由某一个特定节点打包，带来的问题是网络不公，这个问题有一种优化是每轮共识都重选主节点，还有一些优化是可以通过设置轮换间隔进行控制。\nRBFT ViewChange流程\n上图中，Primary(v) 为拜占庭节点，需要进行 ViewChange，与原有发起VC的主要流程是一致的，但多了一个 finishVCReset 的过程，这个过程主要是确认所有节点接受了新节点，并且将自身的状态进行了一个变更，流程细节如下：\n（1）从节点在检测到主节点有异常情况（没有按时收到nullRequest消息）或者接收到来自其他f+1个节点的ViewChange消息之后会向全网广播ViewChange消息，自身view从v更改为v+1；\n（2）新视图中主节点收到N-f 个ViewChange消息后，根据收到的ViewChange消息计算出新视图中主节点开始执行的checkpoint和接下来要处理的交易包，封装进NewView消息并广播，发起VcReset；\n（3）从节点接收到NewView消息之后进行消息的验证和对比，如果通过验证，进行VcReset，如果不通过，发送ViewChange消息，进行又一轮ViewChange；\n（4）所有节点完成VcReset之后向全网广播FinishVcReset；\n（5）每个节点在收到N-f个FinishVcReset消息之后，开始处理确定的checkpoint后的交易，完成整个ViewChange流程；\n由于共识模块与执行模块之间是异步通信的，而 ViewChange 之后执行模块可能存在一些无用的 validate 缓存，因此共识模块需要在ViewChange完成之前通知执行模块清除无用的缓存，Hyperchain共识通过VcReset事件主动通知执行模块清除缓存，并在清理完成之后才能完成ViewChange；\n 这其实是架构设计上的一个问题，因为共识和执行模块从内部实现的角度看应该是异步的，因此共识尽管认为已经VC完成，但是执行模块可能还在异步处理当中，这个过程需要新增一个finishVCReset加以确定。\n    主动恢复流程  区块链网络在运行过程中由于网络抖动、突然断电、磁盘故障等原因，可能会导致部分节点的执行速度落后于大多数节点。在这种场景下，节点需要能够做到自动恢复才能继续参与后续的共识流程。为了解决这类数据恢复的问题，RBFT 算法提供了一种动态数据自动恢复的机制 (recovery)，recovery 通过主动索取现有共识网络中所有节点的视图、最新区块等信息来更新自身的存储状态，最终同步至整个系统的最新状态。在节点启动、节点重启或者节点落后的时候，节点将会自动进入 recovery，同步至整个系统的最新状态。\n   自主恢复流程  上图中，Replica 4 为落后节点，需要进行 recovery。此节点在 RBFT 中的自动恢复流程如下：\n Replica 4 首先广播 NegotiateView 消息，获取当前其余节点的视图信息； 其余三个节点向 Replica 4 发送 NegotiateViewResponse，返回当前视图信息，此时可以确定网络中的view情况。 Replica 4 收到 quorum 个 NegotiateViewResponse 消息后，更新本节点的视图，即当前的view信息，应该认识到 N信息应该是预先配置好的； Replica 4 广播 RecoveryInit 消息到其余节点，通知其他节点本节点需要进行自动恢复，请求其余节点的检查点信息和最新区块信息； 正常运行节点在收到 RecoveryInit 消息之后，发送 RecoveryResponse，将自身的检查点信息以及最新区块信息返回给 Replica 4 节点； Replica 4 节点在收到 quorum 个 RecoveryResponse 消息后，开始尝试从这些 response 中寻找一个全网共识的最高的检查点，随后将自身的状态更新到该检查点， 此时仅仅有checkpoint的高度，其实内部的PQC-set均没有同步，也就是说，这个时候如果需要检查checkpoint的合法性的话，只能依靠数量检查； Replica 4 节点向正常运行节点索要检查点之后的 PQC （即前文所述的POC-set数据）数据，最终同步至全网最新的状态。     增删节点流程  在联盟链场景下，由于联盟的扩展或者某些成员的退出，需要联盟链支持成员的动态进出服务，而传统的 PBFT 算法不支持节点的动态增删。RBFT 为了能够更加方便地控制联盟成员的准入和准出，添加了保持集群非停机的情况下动态增删节点的功能。\n   增加节点  Replica 5新节点加入的流程如下图所示；\n 新节点启动之后，读取本地配置信息，需要是被自身为新增节点，并进行view的协商流程 view协商完成以后，向网络中其他节点建立连接并且发送AddNode消息； 当集群中的节点收到AddNode消息后，会广播AgreeAdd的消息，此时，新节点已经能够从原来网络集群中同步数据了，甚至于说，只要其中某个同意了就可以从同意的节点同步数据，其实可能全网还没有完全同意新增；   原有设计与实现略有偏差：\n当现有节点收到N条（N为现有区块链共识网络中节点总数）AddNode消息后，更新自身的路由表，随后开始回应新增节点的共识消息请求（在此之前，新增节点的所有共识消息是不予处理的）；\n   完成view协商之后，5号节点同时会进行recovery动作，完成recovery之后，向全网现有节点广播ReadyForN请求；\n  现有节点在收到ReadyForN请求后，重新计算新增节点加入之后的N,view等信息，随后将其与PQC消息封装到AgreeUpdateN消息中，进行全网广播；\n   Replica 5加入后的共识网络会产生一个新的主节点，该主节点在收到N-f个AgreeUpdateN消息后，以新的主节点的身份发送UpdateN消息；    全网所有节点在收到UpdateN消息之后确认消息的正确性，进行VCReset；\n  每个节点完成VCReset后，全网广播 FinishUpdateN 消息；\n  节点在收到 N-f 个 FinishUpdateN 消息后，处理后续请求，完成新增节点流程。\n     删除节点  删除节点比增加节点稍微简单一些， 因为没有数据同步过程，但是存在变更触发的问题，新增节点可以通过新节点主动触发， 但是删除节点需要客户端进行触发，删除节点不存在部分节点成功的问题，因此也有一个限制，就是必须要所有节点同时变更。\n 上图简单分析了删除节点需要额外阶段确认的原因，我认为确认可以提升整个系统的健壮性。在删除节点这个过程当中，实际操作会要求所有节点达到确定退出并更新N的状态，也就是说，不仅仅要求quorum个节点，而是要求所有节点，来满足整体网络的可运行与健壮性（当然这个是工程上的考虑）。\n    附加概念     交易池  在区块链系统当中，交易池 (TxPool) 是常见的，对于PBFT算法而言不是必须的，但是为了能够便于下文理解，先在主流程内简单介绍一下。\n交易池用于缓存交易数据，交易池其实扮演了蓄洪的作用，一方面缓冲了外部请求的压力，另一方面，也让交易处理更加稳定，减少了请求毛刺。在区块链节点接收到交易之后，将会把交易缓存到自己的交易池当中，随后向全网广播交易，通过该机制，可以让所有的节点都能够拥有完整的交易列表（如果从节点在验证之前发现缺少了某些交易，也只需要向主节点索取缺少的那些交易而不用索取整个区块里面所有的交易），从而满足PBFT只需要广播交易Hash进行共识的条件，上述方式较传统方式可以减小传输带宽，降低主节点压力，提升共识稳定性。\n尽管下图的以太坊交易池的作用有所不同，但是思想其实是一致的，都是把交易进行一个缓冲，降低共识的压力。\n以太坊交易池\n    延伸阅读  PBFT是第一个得到广泛应用的 BFT 算法。随后业界还提出了若干改进版的BFT共识算法。\n 以下内容参考自 高性能联盟区块链技术研究\n 文献[14]: Q/U 提出了一种可伸缩的故障容忍方法，系统可根据需要配置可容忍的故障数量，而不会显着降低性能。Q / U是一种quorum-based协议，可用于构建故障可扩展的拜占庭式容错服务。相较使用agreement-based的副本状态机协议，Q / U协议可以提供更好的吞吐量和故障可伸缩性。使用Q / U协议构建的原型服务在实验中优于使用副本状态机实现的相同服务，使用Q / U协议时，性能减少了36％，而拜占庭式容错的数量从1增加到5，使用副本状态机协议时，性能下降了83％。\n文献[15]: HQ提出了一种混合拜占庭式容错状态机副本协议，在没有争用的情况下，HQ采用轻量级的基于仲裁的协议，节省了副本间二次通信的成本。一旦出现争议，HQ则使用BFT解决争用。此外，总部仅使用3f + 1个副本来容忍f个故障，为节点故障提供了更佳优秀的恢复能力。\n文献[16]: Zyzzyva 提出了一种使用推测来降低成本并简化拜占庭容错状态机副本的协议。在Zyzzyva中，副本可直接响应客户端的请求，而不需要首先运行PBFT的三阶段共识协议来完成请求的定序处理。副本节点可采用主节点提出的请求定序，并立即回应客户端。副本节点可能会出现不一致，一旦客户端检测到不一致，将帮助副本节点收敛在单一请求定序上。同时，Zyzzyva将副本节点开销降低到了理论最小值附近。\n文献[17]: High throughput BFT提出了一种高吞吐量的拜占庭式容错架构，它使用特定应用程序的信息来识别和同时执行独立的请求。该体系结构提供一种通用的方法来利用应用程序间的并行性，在提高吞吐量的同时，还不损害系统工作的正确性。\n文献[18]: Aardvark算法提出了一种实用的BFT方法，通常被称为RBFT（Robust BFT）算法，RBFT算法使得系统在面对最好和最坏的情况下，性能都能大致保持不变，极大的提高了系统的可用性。\n   参考文献  [1]. Quorum (分布式系统)\n[2]. osdi99\n[3]. Making Byzantine Fault Tolerant SystemsTolerate Byzantine Faults (Aardvark)\n[4]. 共识算法系列之一：raft和pbft算法\n[5]. PBFT基础流程\n[6]. hyperchain RBFT说明文档\n[7]. PBFT实用拜占庭容错算法深入详解\n[8]. 共识 | 拜占庭容错的代表 PBFT\n[9]. 详解实用拜占庭容错协议\n[10]. Hyperledger Fabric中PBFT算法详解\n[11]. https://www.the-paper-trail.org/post/2008-08-13-a-brief-tour-of-flp-impossibility/\n[12]. 区块链共识算法的发展现状与展望\n[13]. 朱立, 俞欢, 詹士潇, 邱炜伟, \u0026amp; 李启雷. (2019). 高性能联盟区块链技术研究. Journal of Software, 30(6).\n[14]. Abd-El-MalekM, Ganger GR, Goodson GR, Reiter MK, Wylie JJ. Fault-scalable Byzantine fault-tolerant services. ACM SIGOPS Operating Systems Review, 2005, 39(5): 59-74. [doi:10.1145/1095809]\n[15]. Cowling J, Myers D, Liskov B, Rodrigues R, Shrira L. HQ replication: A hybrid quorum protocol for Byzantine fault tolerance. In: Proc. of the 7th Symp. on Operating Systems Design and Implementation. USENIX Association, 2006.177-190.\n[16]. Kotla R, Dahlin M. High throughput Byzantine fault tolerance. In: Proc. of the 2004 Int\u0026rsquo;l Conf. on Dependable Systems and Networks. IEEE Computer Society, 2004.575.\n[17]. Kotla R, Alvisi L, Dahlin M, Clement A, Wong E. Zyzzyva:Speculative byzantine fault tolerance. ACM SIGOPS Operating Systems Review, 2007, 41(6): 45-58. [doi:10.1145/1323293]\n[18]. Clement A, Wong EL, Alvisi L, Dahlin M, Marchetti M. Making Byzantine fault tolerant systems tolerate Byzantine faults. In: Proc. of the NSDI, Vol.9.2009.153-168.\n","date":"2021-01-18","permalink":"https://chenquan.me/posts/pbft-key-points/","tags":["共识","区块链"],"title":"PBFT算法关键要点详叙"},{"categories":null,"contents":"","date":"2020-11-26","permalink":"https://chenquan.me/articles/","tags":null,"title":"文章归档"},{"categories":null,"contents":"首先需要克隆 wrk2 仓库的代码:\n1  git clone https://github.com/giltene/wrk2.git   编译安装\nmake \u0026amp;\u0026amp; sudo make install  注意:\n在macos下编译需要修改一行代码，我的系统版本为10.14 Mojave则在deps/luajit/src/Makefile文件293行\n修改为相应的系统版本号：\n1 2  # export MACOSX_DEPLOYMENT_TARGET=10.4 export MACOSX_DEPLOYMENT_TARGET=10.14   安装完毕之后，就可以进行压力测试了。\n   编写压测lua脚本  以post测试为例,编写如下lua脚本：\npost.lua\n1 2 3  wrk.method = \u0026#34;POST\u0026#34; wrk.body = \u0026#34;{\\\u0026#34;jsonrpc\\\u0026#34;:\\\u0026#34;2.0\\\u0026#34;, \\\u0026#34;method\\\u0026#34;: \\\u0026#34;node_getNodeStates\\\u0026#34;, \\\u0026#34;id\\\u0026#34;: 1}\u0026#34; wrk.headers[\u0026#34;Content-type\u0026#34;] = \u0026#34;application/json\u0026#34;      压力测试  wrk拥有如下选项：\nUsage: wrk \u0026lt;options\u0026gt; \u0026lt;url\u0026gt; Options: -c, --connections \u0026lt;N\u0026gt; 需要保持的连接数量 -d, --duration \u0026lt;T\u0026gt; 测试持续的时间 -t, --threads \u0026lt;N\u0026gt; 需要使用的线程数量 -s, --script \u0026lt;S\u0026gt; 需要加载的lua脚本 -H, --header \u0026lt;H\u0026gt; 添加请求头 -L --latency 打印延迟统计 -U --u_latency 打印不正确的延迟统计 --timeout \u0026lt;T\u0026gt; 连接/请求超时时间 -B, --batch_latency 统计整个批量时间 (正式对应的每一个op) -v, --version 打印版本细节 -R, --rate \u0026lt;T\u0026gt; TPS [必填参数]  简单测试命令:\n1  ./wrk -t10 -c10 -d30s -R50 -L --script=post.lua http://localhost:8081   ","date":"2019-01-11","permalink":"https://chenquan.me/posts/wrk2-note/","tags":null,"title":"WRK2 http benchmark 快速笔记"},{"categories":null,"contents":"","date":"2018-11-08","permalink":"https://chenquan.me/archive/","tags":null,"title":"归档"},{"categories":null,"contents":" Author: 陈权@hyperchain\n    0x01 问题明确  当前我们需要模拟如下场景：\n 节点在高网络延迟下的表现 节点在高网络丢包率下的表现  有些bug我们是在比较苛刻的网络条件下测试得到的，而按照传统的经验，我们会想办法将环境部署到互联网上进行测试，但是这种方式非常原始，而且得到的结果也具备大量的偶然性，我们想要复现相同的结果也是十分困难的，因此我们使用容器技术对不同的网络情况进行模拟，以期得到可控的网络抖动测试环境。\n   0x02 pumba 工具介绍  先对容器网络Chaos-testing 工具pubma进行简单介绍，以及相关使用的说明。\n   What is Pubma(a)?  相信出生于90年代的小伙伴们都知道一个电影叫 The Lion King（狮子王），里面有一个角色的名字就叫Pumbaa，在Swahili语中，Pumbaa的意识是”保持愚蠢，无知，以及懒惰”。当然这个工具起这个名字的内涵大概就是想模拟一个“愚蠢的，不可预知的环境”的意思。\n   Pumba 能做什么？  简单地说，Pubma 能够完成包括对Docker容器的 kill，stop， remove，pause。\n当然， Pubma 也能够完成网络模拟，模拟包括一系列的网络问题（延迟，丢包，使用不同的丢包模型，带宽限制等等）。针对网络模拟，Pumba使用的是Linux内核tc netem实现的。 如果目标container不支持tc的话，Pumba将会使用sidekick 附着到目标容器进行控制。\n   怎么使用 Pumba  通常可以传一个容器列表到Pumba中，可以简单地写一个正则表达式来选择匹配的容器。如果你没有指定容器，那么Pumba将会对所有运行的容器进行干预。\n如果你使用了--random选项，那么Pumba将会在提供的容器列表中选择一些随机容器进行干扰。\n你也可以通过传入一些重复参数，以及持续时间参数来更加精细地控制你需要产生的chaos 混沌。\n   如何安装 pumba     源码安装  1 2 3 4 5 6 7 8  # Download binary from https://github.com/gaia-adm/pumba/releases curl -L https://github.com/alexei-led/pumba/releases/download/0.5.2/pumba_darwin_amd64 -- # Linux curl -L https://github.com/alexei-led/pumba/releases/download/0.5.2/pumba_linux_amd64 -- output /usr/local/bin/pumba chmod +x /usr/local/bin/pumba \u0026amp;\u0026amp; pumba --help Install with Homebrew (MacOS only) brew install pumba \u0026amp;\u0026amp; pumba --help          使用 Docker 镜像  1  docker run gaiaadm/pumba pumba --help      Pumba 使用例子   你可以通过--help来查看帮助：  1 2 3 4 5 6  # pumba help pumba --help pumba kill --help pumba netem delay --help   通过^test正则随机kill掉一些 Docker 容器  1 2 3 4 5 6 7 8 9 10 11 12 13  # 在第一个terminal中运行7个测试容器，并什么都不做 for i in {0..7}; do docker run -d --rm --name test$i alpine tail -f /dev/null; done # 然后运行一个 名叫 `skipme` 的容器 docker run -d --rm --name skipme alpine tail -f /dev/null # 在另一个 terminal 中查看当前运行的docker 容器 watch docker ps -a # 回到第一个terminal中，然后每隔10s kill一个\u0026#39;test\u0026#39;开头的容器，并且忽略`skipme`容器 pumba --random --interval 10s kill re2:^test 你可以随时按下 Ctrl-C 来停止 Pumba   为ping 命令增加 3000ms(+-50ms)延迟，持续20s，并使用normal 分配 模型  1 2 3 4 5 6 7  # 运行 \u0026#34;ping\u0026#34; 容器在terminal 1中 docker run -it --rm --name ping alpine ping 8.8.8.8 # 在termainal2中, 运行 pumba netem delay 命令, 分配到 \u0026#34;ping\u0026#34; 容器; 使用一个 \u0026#34;tc\u0026#34; 辅助容器 pumba netem --duration 20s --tc-image gaiadocker/iproute2 delay --time 3000 jitter 50 --distribution normal ping pumba 将会在 20s 后退出, 或者用 Ctrl-C 退出   模拟丢包，为了模拟丢包我们需要使用三个terminal，同时使用iperf工具来监控当前的网络带宽。 在第一个terminal中，我们运行一个 server Docker 容器，然后用ipref来监控这个dokcer，这个server容器会启动一个UDP服务器。  在第二个terminal中，启动一个有iperf监控报文发送容器，该容器会发UDP数据包到 server 容器。然后我们在第三个Terminal中运行 pumba netem loss命令，来为容器增加丢包场景。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # 创建一个docker网络 docker network create -d bridge testnet # Terminal 1 # 运行 server 容器 docker run -it --name server --network testnet --rm alpine sh -c \u0026#34;apk add --no-cache iperf; sh\u0026#34; # shell inside server container: run a UDP Server listening on UDP port 5001 # 在进入交互命令行的Server容器中运行UDP服务，在5001端口监听 sh$ iperf -s -u -i 1 # Terminal 2 # 运行 client 容器 docker run -it --name client --network testnet --rm alpine sh -c \u0026#34;apk add --no-cache iperf; sh\u0026#34; # 在进入交互命令行的 client容器中，发送UDP数据报到服务端，可以看到没有数据丢包 sh$ iperf -c server -u # Terminal 1 # 我们可以看到服务端没有数据丢包 # Terminal 3 # inject 20% packet loss into client container, for 1m # 往client容器注入 20% 的数据丢包，持续一分钟 pumba netem --duration 1m --tc-image gaiadocker/iproute2 loss --percent 20 client # Terminal 2 # 重新在客户端container 中发送数据报，可以看到20%的丢包 sh$ iperf -c server -u      0x03 Weave 网络  Weave网络是一个广泛使用的，易用的，简单的容器网络解决方案，能够支持夸主机之间Docker容器的互联。通过 weave-scope可以对当前docker的网络连接情况进行监控。\n Weave通过创建虚拟网络使docker容器能够跨主机通信并能够自动相互发现。\n通过weave网络，由多个容器构成的基于微服务架构的应用可以运行在任何地方：主机，多主机，云上或者数据中心。\n应用程序使用网络就好像容器是插在同一个网络交换机上一样，不需要配置端口映射，连接等。\n在weave网络中，使用应用容器提供的服务可以暴露给外部，而不用管它们运行在何处。类似地，现存的内部系统也可以接受来自于应用容器的请求，而不管容器运行于何处。\n 我们通过weave网络来对容器之间的网络连接进行兼容，同时我们也能够观察各个容器的资源使用情况。weave-network和weave-scope相互配合可以达到网络监控的目的。\n   安装weave-network和weave-scope  weave-network 是docker 的network插件，通常配合docker-compose安装。\n1 2 3 4 5  curl -sSL https://get.docker.com/ | sh apt-get install -yq python-pip build-essential python-dev pip install docker-compose curl -L git.io/weave -o /usr/local/bin/weave chmod a+x /usr/local/bin/weave   weave-scope是一个网络监控平台，通常会在本地启动：\n1 2  sudo curl -L git.io/scope -o /usr/local/bin/scope sudo chmod a+x /usr/local/bin/scope      启动weave网络，和scope监控   启动 weave scope:  1  scope launch   启动之后，可以在 http://localhost:4040 看到当前机器的docker情况。\n启动weave network  1  weave launch      0x04 docker-compose 和 weave 网络  通常来说hyperchain集群以4个节点为一组，我们可以通过以下docker-compose进行配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  # docker-compose.ymlversion:\u0026#34;3\u0026#34;services:node1:image:hyperchain/hpc:latestports:- \u0026#34;5001:5003\u0026#34;command:[\u0026#39;-n\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;-i\u0026#39;,\u0026#39;1\u0026#39;,\u0026#39;-p\u0026#39;,\u0026#39;5000\u0026#39;]dns:172.17.0.1networks:- internalnode2:image:hyperchain/hpc:latestports:- \u0026#34;5002:5003\u0026#34;command:[\u0026#39;-n\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;-i\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;-p\u0026#39;,\u0026#39;5000\u0026#39;]dns:172.17.0.1networks:- internalnode3:image:hyperchain/hpc:latestports:- \u0026#34;5003:5003\u0026#34;command:[\u0026#39;-n\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;-i\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;-p\u0026#39;,\u0026#39;5000\u0026#39;]dns:172.17.0.1networks:- internalnode4:image:hyperchain/hpc:latestports:- \u0026#34;5004:5003\u0026#34;command:[\u0026#39;-n\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;-i\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;-p\u0026#39;,\u0026#39;5000\u0026#39;]dns:172.17.0.1networks:- internalnetworks:internal:driver:bridge# 请注意，这里需要配置为网桥模式，否则无法进行网络干预driver:weavemesh# 如果使用多机集群，建议使用weavemesh   简单解释一下，四个节点分别指定相应的ID和容许连接的节点数量，容器内部会自动生成相应的配置文件，同时节点的连接服务监听端口都是5000，所有的容器要求指定为 node${i}, 因为容器内部使用hostname进行连接。\n我们需要注意的是，单机集群的internal的网络驱动是bridge，下文采用单机集群进行演示\n 启动容器集群：\n1  docker-compose up   我们可以看到服务正常运行。\n此时我们可以看看weave-scope里面的状态：\n我们可以看到四个容器均是相互连接的，这个和我们的预期相符。一个简单的P2P网络集群已经正常启动，接下来我们试试使用pumba加点网络抖动。\n   0x05 网络抖动 chaos测试  我们的网络集群已经正常启动，接下来我们为节点加上一些网络Chaos。\n   网络延迟  利用新的terminal来运行如下命令：\n1  pumba --random --interval 6s --log-level info netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 5s delay --time 3000 --jitter 30 --correlation 20 re2:^docker   通过上述命令随机指定docker开头的容器接受数据包的实验增加3000ms+-50ms。\n通过测试可以得到，系统吞吐能力直线下降。\n 100tps压测，开启chaos之前\n  开启chaos之后\n  恢复正常之后\n    网络丢包   在Terminal中运行如下命令进行丢包模拟：  1  pumba --random --interval 6s --log-level info netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 5s loss --percent 80 --correlation 20 re2:^docker   上面的命令模拟了随机节点80%丢包率的情况，我们需要注意，不论是延迟还是丢包干扰，都会对客户端发起的请求产生影响。\n 开启丢包干扰之前\n  开启丢包干扰之后 (80%)\n注意：在我的个人电脑(macbook pro 2015 early)上进行的测试存在很多额外干扰，上述数据仅仅作为参考，无法作为结论数据。\n  运行如下命令，固定干扰节点：  1  pumba --random --log-level info netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 10m loss --    在去除--interval参数之后，我们可以看到，pumba针对docker-compose_node4_1 也就是我们的node4应用了丢包控制(60%)。\n 我们可以看到，针对node2的干扰是对该容器的所有网络请求进行干扰的：\n 针对客户端的请求阻断\n 我们也可以看到，对于较高duration的情况下，如果丢包率较高，是会完全影响服务的正常工作的：\n 可以看到针对node4发送的包无法到达，而其他节点则正常\n  运行如下命令阻断node2和node3的网络(100% 10min)：  1 2 3  pumba --log-level info netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 10m loss --percent 100 docker-compose_node3_1 pumba --log-level info netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 10m loss --percent 100 docker-compose_node2_1   我们通过该命令观察容器之间的状态:\n 容器之间已经无法连接到node2/3, 对于node2/3而言则无法访问所有节点\n  运行如下命令进行重复包产生:  1  pumba --random --interval 11s --log-level=\u0026#34;info\u0026#34; netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 10s duplicate percent 80 --correlation 20 re2:^docker   可以观察到重复包在同一时间由4节点收到了：\n 我们也可以将数据包进行扰乱（可能多发，可能少发），通过如下命令将80%的数据包进行指定node1节点扰乱，持续10m：  1  pumba --log-level=\u0026#34;info\u0026#34; netem --tc-image=\u0026#34;gaiadocker/iproute2\u0026#34; --duration 10m corrupt percent 100 docker-compose_node1_1   我们可以得到如下结果：\n 扰乱之前，按照每个节点收到一个包正常运行\n  进行扰乱后，我们可以看到数据包的个数没有变化，但是到达时间发生了变化。\n    0x06 总结  我们可以看到，在docker+docker-compose组合中，pumba能够做的 Chaos-testing 非常丰富，配置也十分灵活。能够进行包括延迟，丢包，扰乱，重复包等噪音。而本身pumba这个工具也可以帮助我们对docker 容器本身进行 stop，rm， pause 等操作，可以模拟服务宕机等异常情况。pumba能够帮助我们发现在复杂的网络和物理场景下软件的一些潜在问题。\n但是在使用pumba的过程当中，也遇到了一些问题，比方说mac本身是没有tc命令的，这就要求制定--tc-image，而如果使用了tc镜像，如果再指定--interval的话，那么会创建出很多个container，这是pumba的一个bug，笔者也给该项目提了相关的issue。\n更多chaos测试和pumba的相关内容可以参考项目：pumba\n   0x07  参考文献：\nhttps://codefresh.io/docker-tutorial/chaos_testing_docker/\nhttps://github.com/alexei-led/pumba/blob/master/README.md\nweave网络介绍\nhttps://microservices-demo.github.io/deployment/docker-compose-weave.html\nhttps://github.com/microservices-demo/microservices-demo/blob/master/deploy/docker-compose-weave/docker-compose.yml\nhttps://www.weave.works/docs/net/latest/install/using-weave/#peer-connections\n","date":"2018-09-12","permalink":"https://chenquan.me/posts/chaos-testing-by-pubma/","tags":["区块链","混沌测试"],"title":"使用 pubma 对区块链网络进行混沌测试"},{"categories":null,"contents":".vimrc熟悉又痛苦的一个文件，从自己编译YCM，安装Ctags到索性一键安装SpaceVim最终还是归于沉寂。\n玩来玩去，我觉得一个vim的配置还是要简单实用才是最实在的。因此我找到vim-bootstrap然后稍微改了一下，现在放在我自己的dotfile中，供大家参考。\n在这里我放上我的 vim-cheatsheet。\n   VIM cheat sheet   如果你在使用本.vimrc，你可以参考本使用文档\n直接将vimrc文件放到$HOME并命名为.vimrc即可\n    阅读前说明    如果你看到一条命令是: (半角冒号)开头的话，请不要犹豫，直接先输入:(半角冒号)\n  在本配置中，\u0026lt;Leader\u0026gt; 键是,(半角逗号)\n  半角符号即英文符号\n  尖括号的中命令表示需要按下，例如\u0026lt;F3\u0026gt; 表示需要按下键盘上的F3\n  \u0026lt;C-R\u0026gt; 表示要按下 Ctrl + R 请注意是R而不是小写的r，因此需要同时按下\nCtrl + shift + r\n  S-t 表示需要按下 shift + t\n  \u0026lt;leader\u0026gt;f 表示需要按下 , + f\n     通用部分     插件安装  一般使用本vimrc的话，初次启动vim 会自动帮你安装插件，你不需要特别输入相关命令。\n如果vim没有帮你自动安装的话，你可以输入：\n:PlugInstall  进行安装。\n   自动纠正  一般来说，你肯定会遇到保存文件的时候不小心输入了类似Wq\n这样的命令，本配置文件会自动帮助你纠正这种简单的输入错误：\n纠错映射表如下：\nW! w! Q! q! Qall! qall! Wq wq Wa wa wQ wq WQ wq W w Q q Qall qall     NERDTree  NERDTree 即文件树，存在如下命令：\nF2 文件树搜索 F3 展开或者隐藏文件树     grep文件搜索  搜索的时候会跳过.git 和node_modules以及*.log和*.db\n\u0026lt;leader\u0026gt;f     终端  在vim中可以使用vimshell,来执行命令，相关命令是:\n\u0026lt;leader\u0026gt;sh  可以进入vimshell,进入之后可以随意输入相关命令并执行，执行完毕想退出，请退出insert模式并按q\n   Split (屏幕划分)  \u0026lt;leader\u0026gt;h 垂直划分 \u0026lt;leader\u0026gt;v 水平划分 在不同的划分window中进行光标移动: C-w j 向下移动 C-w k 向上移动 C-w h 向左移动 C-w l 向右移动     Git相关命令  当前配置中安装了git插件，并配置了相关的alias,可以快速使用git相关命令\n\u0026lt;Leader\u0026gt;ga git add \u0026lt;Leader\u0026gt;gc git commit \u0026lt;Leader\u0026gt;gsh git push \u0026lt;Leader\u0026gt;gll git pull \u0026lt;Leader\u0026gt;gs git status \u0026lt;Leader\u0026gt;gb git blame \u0026lt;Leader\u0026gt;gd git diff \u0026lt;Leader\u0026gt;gr git remove     TAB相关命令  我个人觉得TAB相关命令是最实用的：\n\u0026lt;Tab\u0026gt; 下一个TAB \u0026lt;S-Tab\u0026gt; 上一个TAB \u0026lt;S-t\u0026gt; 新建TAB  注意: 这里的\u0026lt;Tab\u0026gt; 就是键盘上的tab\n   SESSION相关命令  SESSION就是会话，你可以把当前的编辑状态保存为会话，然后需要的时候可以恢复\n\u0026quot; session management \u0026lt;leader\u0026gt;so 打开一个会话 \u0026lt;leader\u0026gt;ss 保存一个会话 \u0026lt;leader\u0026gt;sd 删除一个会话 \u0026lt;leader\u0026gt;sc 关闭一个会话     设置当前工作目录  \u0026lt;leader\u0026gt;.     从当前的工作目录打开一个文件进行编辑：  打开的文件将会覆盖当前编辑的文件窗口\n\u0026lt;leader\u0026gt;e     从当前目录打开一个文件进行编辑(tab)  打开的文件将会新建一个TAB打开\n\u0026lt;leader\u0026gt;te     打开符号定义窗口(Tagbar)  \u0026lt;F4\u0026gt;     复制粘贴  当前配置能够自动读取剪贴板中的内容进行粘贴，如果粘贴的时候会自动缩进，\n请在粘贴之前输入：\n:paste     buffer 导航  buffer类似于tab，但有区别:\nA buffer is the in-memory text of a file.\nA window is a viewport on a buffer.\nA tab page is a collection of windows.\nbuffer导航 \u0026lt;leader\u0026gt;z 下一个buffer \u0026lt;leader\u0026gt;q 下一个buffer \u0026lt;leader\u0026gt;x 上一个buffer \u0026lt;leader\u0026gt;w 上一个buffer 关闭buffer \u0026lt;leader\u0026gt;c :bd\u0026lt;CR\u0026gt;     搜索高亮清除  当我们搜索之后，会有高亮字符，可以用下面的方式清除所有高亮\n\u0026lt;leader\u0026gt;\u0026lt;space\u0026gt;     window切换  前面也说过如何切换当前焦点的window, 这个也有快捷键\n\u0026quot;\u0026quot; Switching windows \u0026lt;C-j\u0026gt; 向下切换 \u0026lt;C-k\u0026gt; 向上切换 \u0026lt;C-l\u0026gt; 向左切换 \u0026lt;C-h\u0026gt; 向右切换     visual模式缩进  在visual模式下，可能希望将一段代码整体缩进，这个时候可以通过visual模式选中一段代码之后，利用\u0026lt;\n和\u0026gt;进行缩进调整。\n   visual模式代码整行移动  在visual模式下，可以通过大写的J 和K将当前选中的代码进行上移或者下移\n   GO语言相关快捷键     定义跳转   \u0026lt;Leader\u0026gt;dd go-def \u0026lt;Leader\u0026gt;dv go-doc \u0026lt;Leader\u0026gt;db go-doc-browser     Go相关命令   \u0026lt;leader\u0026gt;r go-run \u0026lt;leader\u0026gt;t go-test \u0026lt;Leader\u0026gt;gt go-coverage-toggle \u0026lt;Leader\u0026gt;i go-info \u0026lt;Leader\u0026gt;l go-metalinter     Go相关声明   \u0026lt;C-g\u0026gt; GoDecls \u0026lt;leader\u0026gt;dr GoDeclsDir \u0026lt;leader\u0026gt;rb build_go_files() ","date":"2018-08-30","permalink":"https://chenquan.me/posts/vimrc-configuration/","tags":["vim","vimrc"],"title":"寻觅了很久的 .vimrc"},{"categories":null,"contents":"在v8引擎的6.5版本以上，google采用了GN+Ninja的编译组合，因此本文主要是基于GN+Ninjia的编译方式进行说明。\n   获取源码  在官方文档中，还特别提示了避免HFS环境下的unicode问题，需要额外配置一下：\n1  $ git config --global core.precomposeUnicode true   现在v8在github上面有源码镜像，你只需要git clone下来即可。\n1  $ git clone git@github.com:v8/v8.git      获取依赖     depot_tools  首先git clone如下的仓库：\n1  $ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git    Note: 国内用户可以clonegithub上面的镜像代码：\ngit clone git@github.com:cybertk/depot_tools.git\n 然后将depot_tools加入你的PATH环境变量中(通常会加到.bashrc或者.zshrc中)：\n1  export PATH=\u0026#34;$PATH:/path/to/depot_tools\u0026#34;      gclient  取得depot_tools之后，需要取得大量编译依赖，google提供了一个比较方便的工具gclient来获取依赖。\n1  gclient sync   尴尬的一点在于，由于众所周知的原因，依赖下载十分缓慢。现在我给出一些解决方案：\n方案A: 设置代码镜像\n在获取到的v8代码中，第一级目录下有一个DEPS文件,通过修改其中的代码源实现加速：\n1  $ vim DEPS   内容如下\n1 2 3 4 5 6 7 8  vars = { \u0026#39;build_for_node\u0026#39;: False, \u0026#39;checkout_instrumented_libraries\u0026#39;: False, \u0026#39;chromium_url\u0026#39;: \u0026#39;https://chromium.googlesource.com\u0026#39;, \u0026#39;download_gcmole\u0026#39;: False, \u0026#39;download_jsfunfuzz\u0026#39;: False, \u0026#39;download_mips_toolchain\u0026#39;: False, }   将chromium_url的值修改为:https://source.codeaurora.org/quic/lc\n Note: 请注意，这个chromium源码镜像并不全，没有python相关的库，但是大部分库都有，可以先把大部分代码下载下来，再使用代理把剩下的下载下来。\n 方案B: 设置代理\n这是根治的方法，至于代理是如何设置本文不涉及。\n1 2  export http_proxy=127.0.0.1:1080 export https_proxy=127.0.0.1:1080    Note：请注意http_proxy一定要小写。\n    编译初始化  v8使用Ninja作为编译工具，同时使用GN来生成.ninja文件。\n使用GN可以生成不同平台的编译配置文件：\n1  gn gen out.gn/x64.release   当然你也可以在生成编译配置的时候传入一些参数:\n1  gn gen out.gn/foo --args=\u0026#39;is_debug=false target_cpu=\u0026#34;x64\u0026#34; v8_target_cpu=\u0026#34;arm64\u0026#34; use_goma=true\u0026#39;   使用如下参数加速编译：\n is_debug = false 编译成release版本 is_component_build = true 编译成动态链接库而不是很大的可执行文件 symbol_level = 0 将所有的debug符号放在一起，可以加速二次编译，并加速链接过程 可以使用ccache加速编译过程，安装ccache, 可以传入参数cc_wrapper=\u0026quot;ccache\u0026quot;,参考这里 使用jumbo  编译选项参考chromium编译方式\n例如我的编译生成命令是：\n1  gn gen out.gn/x64.release --args=\u0026#39;is_debug=false is_component_build=true symbol_level=0 cc_wrapper=\u0026#34;ccache\u0026#34; target_cpu=\u0026#34;x64\u0026#34; \u0026#39;   或者你可以使用 gn args out/x64.yourdir 这样的方式配置编译选项文件，如果你想编译一个简单的静态库，你可以用如下参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  is_component_build = false is_debug = false libcpp_is_static = false symbol_level = 1 treat_warnings_as_errors = false use_custom_libcxx = false use_sysroot = false v8_deprecation_warnings = false v8_embedder_string = \u0026#34;-v8_Your_Define_String\u0026#34; v8_enable_gdbjit = false v8_enable_i18n_support = false v8_enable_test_features = false v8_experimental_extra_library_files = [] v8_extra_library_files = [] v8_imminent_deprecation_warnings = false v8_monolithic = true v8_static_library = false v8_target_cpu = \u0026#34;x64\u0026#34; v8_untrusted_code_mitigations = false v8_use_external_startup_data = false v8_use_snapshot = true   请注意，官方wiki文档中使用的tools/dev/v8gen.py x64.release一直由于缺少什么文件导致无法运行，如果你也遇到本问题，请参考本文。\n   编译  编译完整的V8代码（假设是x64.relrease）\n1  ninja -C out.gn/x64.release   如果想编译特定目标，比如d8：\n1  ninja -C out.gn/x64.release d8      生成开发项目  如果想使用xcode进行开发，你可以这样做：\n1  gn gen out/gn --ide=xcode   然后用xcode打开即可\nopen out/gn/ninja/all.xcworkspace    有关 xcode 调试代码的说明  有朋友反馈上述编译得到的 xcode 项目调试时无法看到源码, 该朋友的解决方案是如下：\n原因：\n在 xcode 的设置里面, 断点的标记形式, 是以绝对路径的形式的, 而 v8/chrome 默认的配置 (配置文件: compiler.gni) 会删掉绝对路径的符号, 这样与 xcode 就无法找到对应符号, 所以看不到源码。\n1 2 3 4 5 6 7 8 9  # If the platform uses stripped absolute paths by default, then we don\u0026#39;t expose # it as a configuration option. If this is causing problems, please file a bug. if (strip_absolute_paths_from_debug_symbols_default) { strip_absolute_paths_from_debug_symbols = true } else { declare_args() { strip_absolute_paths_from_debug_symbols = false } }   参考这个链接： https://groups.google.com/a/chromium.org/g/chromium-dev/c/kE1HeGL4mj0/m/TOKsRsyTEwAJ.\ngn 编译时添加参数：\n1  gn gen out/Default --args=\u0026#34;strip_absolute_paths_from_debug_symbols=false”   这样就可以让 xcode 调试的时候看到到项目的源代码了。\n","date":"2018-05-12","permalink":"https://chenquan.me/posts/how-to-compile-v8-engine-by-gn/","tags":["javascript","v8"],"title":"如何用GN编译V8引擎"},{"categories":null,"contents":"这是Lodash源码分析系列文章的第三篇，前面两篇文章(Lodash 源码分析（一）“Function” Methods、Lodash 源码分析（二）“Function” Methods)分别分析了Lodash “Function” 中的一些重要函数，也给出了简化的实现，为理解其内部机理和执行方式提供了便利。这篇文章将专注于Array，Array是Lodash中非常重要的内容，我们将分析其代码实现以及同类似库中的实现对比。\n   _.head  _.head函数其实很简单，返回一个数组的第一个元素，完全可以在两三行代码中实现。可以看到Lodash中是这么实现的：\n1 2 3  function head(array) { return (array \u0026amp;\u0026amp; array.length) ? array[0] : undefined; }   Lodash进行了简单的判断，然后返回了第一个元素。这么简单的函数其实没有什么好说的，但我拿出来说是想介绍另一个库Ramda.js的实现：\n1  module.exports = nth(0);   它是用nth函数实现该功能的，那么这个函数式怎么样的呢？\n1 2 3 4  module.exports = _curry2(function nth(offset, list) { var idx = offset \u0026amp;lt; 0 ? list.length + offset : offset; return _isString(list) ? list.charAt(idx) : list[idx]; });   这个函数就有点意思了，用了柯里化，是一个函数式的实现，当head函数返回一个nth(0)时，其实返回的是一个柯里化之后的函数，然后再接受一个数组，判断数组类型之后返回list[offset]的值。\n再看看Lodash的nth的实现：\n1 2 3 4 5 6 7 8 9 10 11 12  function nth(array, n) { return (array \u0026amp;\u0026amp; array.length) ? baseNth(array, toInteger(n)) : undefined; } function baseNth(array, n) { var length = array.length; if (!length) { return; } n += n \u0026amp;lt; 0 ? length : 0; return isIndex(n, length) ? array[n] : undefined; }   仔细对比两个库的实现，两个库都允许负下标的处理，但是对于Ramda而言，如果list是一个null或者undefined类型的数据的话，将会抛出TypeError，而Lodash则优雅一些。\n   _.join  _.join函数是另一个简单的函数：\n1 2 3 4 5 6  var arrayProto = Array.prototype; var nativeJoin = arrayProto.join; function join(array, separator) { return array == null ? \u0026#39;\u0026#39; : nativeJoin.call(array, separator); }   重写之后函数变为:\n1 2 3  function join(array,separator) { return array == null ? \u0026#39;\u0026#39; : Array.prototype.join.call(array, separator); }   我们再对比一下Ramda的实现：\n1 2  \u0026amp;lt;br /\u0026gt;var invoker = require(\u0026#39;./invoker\u0026#39;); module.exports = invoker(1, \u0026#39;join\u0026#39;);   再看看invoker函数：\n1 2 3 4 5 6 7 8 9  module.exports = _curry2(function invoker(arity, method) { return curryN(arity + 1, function() { var target = arguments[arity]; if (target != null \u0026amp;\u0026amp; _isFunction(target[method])) { return target[method].apply(target, Array.prototype.slice.call(arguments, 0, arity)); } throw new TypeError(toString(target) + \u0026#39; does not have a method named \u0026#34;\u0026#39; + method + \u0026#39;\u0026#34;\u0026#39;); }); });   invoker函数就是为了返回一个curry化的函数，那么我们其实可以这么理解如果用Lodash实现一个函数化的join可以这么实现：\n1 2 3 4  function _join(array,separator){ return Array.prototype.join.call(array,seprator); } var join = _.curry(_join);   那么我们可以和Ramda的使用方式一样使用：\n1 2  join(_,\u0026#34;,\u0026#34;)([1,2,3]); // 1,2,3      _.remove  这个方法很有意思，我们可以看到不同的实现方式（通常实现/函数式实现），两种实现差别很大，所以拿出来分析一下。\n先看看Lodash的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  /** * Removes all elements from `array` that `predicate` returns truthy for * and returns an array of the removed elements. The predicate is invoked * with three arguments: (value, index, array). * * **Note:** Unlike `_.filter`, this method mutates `array`. Use `_.pull` * to pull elements from an array by value. * * @static * @memberOf _ * @since 2.0.0 * @category Array * @param {Array} array The array to modify. * @param {Function} [predicate=_.identity] The function invoked per iteration. * @returns {Array} Returns the new array of removed elements. * @example * * var array = [1, 2, 3, 4]; * var evens = _.remove(array, function(n) { * return n % 2 == 0; * }); * * console.log(array); * // =\u0026amp;gt; [1, 3] * * console.log(evens); * // =\u0026amp;gt; [2, 4] */ function remove(array, predicate) { var result = []; if (!(array \u0026amp;\u0026amp; array.length)) { return result; } var index = -1, indexes = [], length = array.length; predicate = getIteratee(predicate, 3); while (++index \u0026amp;lt; length) { var value = array[index]; if (predicate(value, index, array)) { result.push(value); indexes.push(index); } } basePullAt(array, indexes); return result; }   **一定要注意的是，该方法会修改原数组。**官方也对其进行了说明。该方法同_.fliter的区别也就在是否会修改原对象上。\n我们分析一下Lodash是如何实现这个功能的，首先判断数组是否合法，如果不合法就直接返回。在Lodash中的实现其实很简单，首先得到一个predicate谓词函数，该谓词函数用于判断元素是否符合条件，如果符合条件就将其从原数组中移除。逻辑也比较简单，但是该函数会修改原array，该功能是通过basePullAt()实现的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  /** * The base implementation of `_.pullAt` without support for individual * indexes or capturing the removed elements. * * @private * @param {Array} array The array to modify. * @param {number[]} indexes The indexes of elements to remove. * @returns {Array} Returns `array`. */ function basePullAt(array, indexes) { var length = array ? indexes.length : 0, lastIndex = length - 1; while (length--) { var index = indexes[length]; if (length == lastIndex || index !== previous) { var previous = index; if (isIndex(index)) { splice.call(array, index, 1); } else { baseUnset(array, index); } } } return array; }   需要说明的是，这里的splice方法的原型是Array.prototype.splice，该方法同Array.prototype.slice的区别是，splice会修改原数组的内容，而slice不会修改原数组的内容，而仅仅做的是一次浅拷贝。\n还需要说明一下的是baseUnset：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  /** * The base implementation of `unset`. * * @private * @param {Object} object The object to modify. * @param {Array|string} path The property path to unset. * @returns {boolean} Returns `true` if the property is deleted, else `false`. */ function baseUnset(object, path) { path = castPath(path, object) object = parent(object, path) return object == null || delete object[toKey(last(path))] } export default baseUnset   这个方法其实很简单，就是删除对象中的某一个属性/键。\n所以Lodash的整个_.remove的脉络就捋清楚了，按照惯例，我们需要稍微简化一下这个函数，把核心逻辑抽取出来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  function remove(list,predicated){ var indexes = []; for(var i=0;i \u0026amp;lt; list.length;i++){ if(predicated(list[i])){ indexes.push(i); } } for(var idx = indexes.length -1; idx \u0026amp;gt;=0;idx--){ Array.prototype.splice.call(list,indexes[idx],1); } return list; } var a = [1,2,3,4]; remove(a,function(a){if (a == 3) return true; else return false;}); console.log(a); // [1,2,4]   恩，感觉好像也挺好用的。\n但是我们不能止步于此，作为一个热衷函数式编程的程序员，最终目标是代码中没有循环没有分支。我们看看Ramda.js是怎么实现的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  /** * Removes the sub-list of `list` starting at index `start` and containing * `count` elements. _Note that this is not destructive_: it returns a copy of * the list with the changes. * \u0026amp;lt;small\u0026amp;gt;No lists have been harmed in the application of this function.\u0026amp;lt;/small\u0026amp;gt; * * @func * @memberOf R * @since v0.2.2 * @category List * @sig Number -\u0026amp;gt; Number -\u0026amp;gt; [a] -\u0026amp;gt; [a] * @param {Number} start The position to start removing elements * @param {Number} count The number of elements to remove * @param {Array} list The list to remove from * @return {Array} A new Array with `count` elements from `start` removed. * @example * * R.remove(2, 3, [1,2,3,4,5,6,7,8]); //=\u0026amp;gt; [1,2,6,7,8] */ module.exports = _curry3(function remove(start, count, list) { var result = Array.prototype.slice.call(list, 0); result.splice(start, count); return result; });   其实Ramda就是对splice进行了curry化，什么也没有做，毫无参考价值。没有达到我们的预期，所以只能自己动手了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  function remove2(list,predicated){ return _remove(list,list.length-1,predicated); } function _remove(list,idx,predicated){ if(predicated(list[idx])){ list.splice(idx,1); } if (idx == 0){return list;}else{ _remove(list,idx-1,predicated); } } //调用 var a = [1,2,3,4]; remove2(a,function(a){if (a == 3) return true; else return false;}); console.log(a); //[1,2,4]   感觉舒服多了，对于JavaScript而言没有分支语句是不可能的，但是可以把所有的循环用递归取代，感觉代码也简洁了许多，函数式能够让人以另一个角度思考问题，真的是一个很好的编程范式。\n   结语  最近工作非常忙，也没有时间写第三篇连载，忙里抽空用午休时间将本文写完了。成文比较匆忙难免有一些谬误望各位看官海涵，也希望能够直接指出我文章中的错误，感激不尽！\n   敬请期待  本系列文章还有后续内容，包括数组和集合的操作，以及对象的操作，具体还没有想好涉及哪方面内容，总之敬请期待！\n","date":"2017-09-21","permalink":"https://chenquan.me/posts/lodash-source-code-analysis-3/","tags":["javascript","lodash"],"title":"Lodash 源码分析（三）Array"},{"categories":null,"contents":"这是Lodash源码分析的第二篇文章，我们在第一篇Lodash 源码分析（一）“Function” Methods中介绍了基本的_.after，_.map，以及复杂的_.ary函数的实现以及我们自己的自定义轻量级版本。大概清楚了Lodash的整个代码脉络。这次我们继续分析，这次我们讲讲_.reduce和_.curry。\n   _.reduce  我一直觉得，如果能够理解_.map和_.reduce的实现，那么任何复杂的函数都不在话下。我们已经介绍了_.map的实现，知道了_.map函数中是如何处理集合，并将其逐个进行函数处理的。我们知道在_.map函数中会把三个参数传到给定的函数中，分别是array[index]，index和array。这次我们看看_.reduce函数。\n众所周知，_.reduce函数能够将一个集合进行”折叠”。”折叠”理解起来比较抽象。我们可以通过代码作为样例说明一下：\n1 2 3  const _ = require(\u0026#34;lodash\u0026#34;); _.reduce([1,2,3],function(a,b){return a+b}); // 6   如果你不知道_.reduce到底是怎么工作的，那么你可以看看我写的这篇文章从Haskell、JavaScript、Go看函数式编程。我们今天的目的是看看lodash是如何实现_.reduce的，以及和我们函数式的实现的区别。\n我们看到lodash源代码是这样的：\n1 2 3 4 5 6  function reduce(collection, iteratee, accumulator) { var func = isArray(collection) ? arrayReduce : baseReduce, initAccum = arguments.length \u0026amp;lt; 3; return func(collection, getIteratee(iteratee, 4), accumulator, initAccum, baseEach); }   在官方的注释中说，对于对象，遍历顺序是无法保证的。我们不考虑这么复杂的情况，先看看Array的情况。其次，我们在调用_.reduce的时候没有传入第三个accumulator参数，那么函数可以简化为：\n1 2 3  function reduce(collection, iteratee, accumulator) { return arrayReduce(collection, getIteratee(iteratee, 4), accumulator, true, baseEach); }   在看看arrayReduce函数：\n1 2 3 4 5 6 7 8 9 10 11 12  function arrayReduce(array, iteratee, accumulator, initAccum) { var index = -1, length = array == null ? 0 : array.length; if (initAccum \u0026amp;\u0026amp; length) { accumulator = array[++index]; } while (++index \u0026amp;lt; length) { accumulator = iteratee(accumulator, array[index], index, array); } return accumulator; }   这里的accumulator是初始累加值，如果传入，则”折叠”在其基础上进行，就上面的最简单的例子而言，如果传入第三个参数是2，那么返回值就会使8。\n1 2 3  const _ = require(\u0026#34;lodash\u0026#34;); _.reduce([1,2,3],function(a,b){return a+b},8); // 8   所以arrayReduce函数就是给定一个初始值然后进行迭代的函数。我们真正需要关注的函数式iteratee函数，即getIteratee(func, 4)这里的func就是我进行重命名之后的自定义函数。\n这个getIteratee函数在介绍_.map的时候就进行介绍了，在func是一个function的情况下，就是返回func本身。\n所以我们可以把整个reduce函数简化为如下版本：\n1 2 3 4 5 6 7 8 9 10 11  function reduce(array, func, accumulator) { var index = -1, length = array == null ? 0 : array.length; if (length) { accumulator = array[++index]; } while (++index \u0026amp;lt; length) { accumulator = func(accumulator, array[index], index, array); } return accumulator; }   其实看上去很像一个”递归“函数，因为前面一次的运算结果将会用于下一次函数调用，但又不是递归函数。我们其实完全可以写一个递归版本的reduce：\n1 2 3 4 5 6 7 8 9  function reduce(array,func,accumulator){ accumulator = accumulator == null ? array[0]:accumulator; if (array.length \u0026amp;gt;0){ var a = array.shift(); accumulator = func(a,accumulator); return reduce(array,func,accumulator); } return accumulator }   工作的也不错，但在分析过程中，发现lodash一直在避免修改原参数的值，尽量让整个函数调用时无副作用的。我觉得这个思想在开发过程中也有很多值得借鉴的地方。\n   _.curry  了解过函数式编程的同学一定听过大名鼎鼎的柯里化，在Lodash中也有一个专门用于柯里化的函数_.curry。这个函数接受一个函数func和这个函数的部分参数，然后返回一个接受剩余参数的函数func'。\n我们看看这个函数是怎么实现的：\n1 2 3 4 5 6  function curry(func, arity, guard) { arity = guard ? undefined : arity; var result = createWrap(func, WRAP_CURRY_FLAG, undefined, undefined, undefined, undefined, undefined, arity); result.placeholder = curry.placeholder; return result; }   我们又看到我们的老朋友createWrap了，其实这个函数我们在上一篇文章中分析过，但是我们那时候是分析_.ary函数的时候进行了精简，这次我们看看createWrap函数式怎么对_.curry函数进行处理的(将无关逻辑进行精简)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  function createWrap(func, bitmask, thisArg, partials, holders, argPos, ary, arity) { var isBindKey = 0 var length = 0; ary = undefined ; arity = arity === undefined ? arity : toInteger(arity); length -= holders ? holders.length : 0; var data = getData(func); var newData = [ func, bitmask, thisArg, partials, holders, partialsRight, holdersRight, argPos, ary, arity ]; if (data) { mergeData(newData, data); } func = newData[0]; bitmask = newData[1]; thisArg = newData[2]; partials = newData[3]; holders = newData[4]; arity = newData[9] = newData[9] === undefined ? func.length : nativeMax(newData[9] - length, 0); result = createCurry(func, bitmask, arity); var setter = data ? baseSetData : setData; return setWrapToString(setter(result, newData), func, bitmask); }   这里面的关键就是createCurry函数了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  function createCurry(func, bitmask, arity) { var Ctor = createCtor(func); function wrapper() { var length = arguments.length, args = Array(length), index = length, placeholder = getHolder(wrapper); while (index--) { args[index] = arguments[index]; } var holders = (length \u0026amp;lt; 3 \u0026amp;\u0026amp; args[0] !== placeholder \u0026amp;\u0026amp; args[length - 1] !== placeholder) ? [] : replaceHolders(args, placeholder); length -= holders.length; if (length \u0026amp;lt; arity) { return createRecurry( func, bitmask, createHybrid, wrapper.placeholder, undefined, args, holders, undefined, undefined, arity - length); } var fn = (this \u0026amp;\u0026amp; this !== root \u0026amp;\u0026amp; this instanceof wrapper) ? Ctor : func; return apply(fn, this, args); } return wrapper; }   不得不说和createHybird函数十分相似,但是其中还有一个比较关键的函数，就是createRecurry，这个函数返回了一个能够继续进行curry的函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  function createRecurry(func, bitmask, wrapFunc, placeholder, thisArg, partials, holders, argPos, ary, arity) { var isCurry = bitmask \u0026amp; WRAP_CURRY_FLAG, newHolders = isCurry ? holders : undefined, newHoldersRight = isCurry ? undefined : holders, newPartials = isCurry ? partials : undefined, newPartialsRight = isCurry ? undefined : partials; bitmask |= (isCurry ? WRAP_PARTIAL_FLAG : WRAP_PARTIAL_RIGHT_FLAG); bitmask \u0026amp;= ~(isCurry ? WRAP_PARTIAL_RIGHT_FLAG : WRAP_PARTIAL_FLAG); if (!(bitmask \u0026amp; WRAP_CURRY_BOUND_FLAG)) { bitmask \u0026amp;= ~(WRAP_BIND_FLAG | WRAP_BIND_KEY_FLAG); } var newData = [ func, bitmask, thisArg, newPartials, newHolders, newPartialsRight, newHoldersRight, argPos, ary, arity ]; var result = wrapFunc.apply(undefined, newData); if (isLaziable(func)) { setData(result, newData); } result.placeholder = placeholder; return setWrapToString(result, func, bitmask); }   Lodash为了实现curry化，进行了多层的包装，为了实现返回的是划一的Lodash中定义的能够curry化的函数。\n这个函数要求接受相应的参数列表，即代码中的data。在curry化的过程中有一个非常重要的东西，就是占位符placeholder。在对curry化的函数进行调用时也可以用占位符进行占位：\n1 2 3 4 5 6 7 8 9 10  var curried = _.curry(abc); curried(1)(2)(3); // =\u0026amp;gt; [1, 2, 3] curried(1, 2)(3); // =\u0026amp;gt; [1, 2, 3] curried(1, 2, 3); // =\u0026amp;gt; [1, 2, 3] // Curried with placeholders. curried(1)(_, 3)(2); // =\u0026amp;gt; [1, 2, 3]   可以用下划线_作为占位符占位。我们且不看lodash为我们做的很多复杂的预处理和特殊情况的处理，我们就分析_.curry函数实现的主要思想。首先_.curry函数有一个属性存储了最初的函数的接受函数参数的个数。然后有一个参数数组用于存储部分参数，如果参数个数没有满足调用函数需要的个数，就继续返回一个重新curry化的函数。\n根据上面的思想我们可以写出一个简化的curry化代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  /** * *var abc = function(a, b, c) { * return [a, b, c]; *}; * *var curried = curry(abc); * *curried(1)(2)(3); * // =\u0026amp;gt; [1, 2, 3] * * curried(1, 2)(3); * // =\u0026amp;gt; [1, 2, 3] * * curried(1, 2, 3); * // =\u0026amp;gt; [1, 2, 3] * * // Curried with placeholders. * curried(1)(\u0026#34;_\u0026#34;, 3)(2) * 这就无法处理了 * // =\u0026amp;gt; [1, 3, 2] */ function curry(func){ function wrapper(){ func.prototype.that = func.prototype.that ? func.prototype.that : this; func.prototype.paramlength = func.prototype.paramlength ? func.prototype.paramlength: func.length ; func.prototype.paramindex = func.prototype.paramindex ?func.prototype.paramindex : 0; func.prototype.paramplaceholder = func.prototype.paramplaceholder ? func.prototype.paramplaceholder : Array(func.length); for (var i = 0 ; i \u0026amp;lt; arguments.length; i++) { if (arguments[i] == \u0026#39;_\u0026#39;){ continue; }else{ func.prototype.paramplaceholder[func.prototype.paramindex] = arguments[i]; func.prototype.paramindex += 1; } } if (func.prototype.paramindex == func.prototype.paramlength){ func.prototype.paramindex = 0; return func.apply(func.prototype.that,func.prototype.paramplaceholder) } return wrapper; } return wrapper; }   我们虽然可以借助Lodash的思想实现我们一个简单版本的curry函数，但是这个简单版本的函数有一个问题，那就是，这个函数是借助闭包实现的，在整个执行过程当中，只要被柯里化的函数没有执行结束，那么它就会一直存在在内存当中，它的一些属性也会一直存在。第二个问题是，没有办法实现Lodash的”真正”的占位符，只是在遇到”_”的时候将其跳过了。\n一个真正有效的柯里化函数实现起来有很多细节需要考虑，这就是Lodash存在的意义。我们应该在理解其实现原理的前提下，享受Lodash带来的便利。\n   小结  阅读Lodash源码真的能够了解很多代码实现上的细节，Lodash在性能优化上面做了很多工作，也给我们学习一个优秀的js库提供了非常好的参考。我在阅读Lodash源代码的过程中也会遇到很多不理解的地方。但是细细琢磨发其实它的代码还是非常清晰易懂的。\n   待续  下周将继续更新Lodash源码分析系列，接下来将会分析Lodash集合方法。\n © 版权所有，禁止一切形式转载。顺便宣传一下个人博客http://chenquan.me\n","date":"2017-08-29","permalink":"https://chenquan.me/posts/lodash-source-code-analysis-2/","tags":["javascript","lodash"],"title":"Lodash 源码分析（二）“Function” Methods"},{"categories":null,"contents":"Lodash一直是我很喜欢用的一个库，代码也十分简洁优美，一直想抽时间好好分析一下Lodash的源代码。最近抽出早上的一些时间来分析一下Lodash的一些我觉得比较好的源码。因为函数之间可能会有相互依赖，所以不会按照文档顺序进行分析，而是根据依赖关系和简易程度由浅入深地进行分析。因为个人能力有限，如果理解有偏差，还请直接指出，以便我及时修改。\n源码都是针对4.17.4版本的，源docs写得也很好，还有很多样例。\n   _.after  _.after函数几乎是Lodash中最容易理解的一个函数了，它一共有两个参数，第一个参数是调用次数n,第二个参数是n次调用之后执行的函数func。\n1 2 3 4 5 6 7 8 9 10 11  function after(n, func) { if (typeof func != \u0026#39;function\u0026#39;) { throw new TypeError(FUNC_ERROR_TEXT); } n = toInteger(n); return function() { if (--n \u0026amp;lt; 1) { return func.apply(this, arguments); } }; }   这个函数的核心代码就是：\n1  func.apply(this,arguments);   但是一定要注意，这个函数中有闭包的应用，就是这个参数n。n本应该在函数_.after返回的时候就应该从栈空间回收，但事实上它还被返回的函数引用着，一直在内存中：\n1 2 3 4 5  return function() { if (--n \u0026amp;lt; 1) { return func.apply(this, arguments); } };   所以一直到返回的函数执行完毕，n所占用的内存空间都无法被回收。\n我们再来看看这个apply函数，我们知道apply函数可以改变函数运行时的作用域了，那么问题来了，在在_.after函数中func.apply函数的this，是谁呢？这个东西我们没有办法从源码中看出来，因为this是在运行时决定的。那么this会变吗？如果会的话怎么变呢？这个问题我们需要先弄懂_.after函数怎么用。\n_.after函数调用后返回了另一个函数，所以对于_.after函数的返回值，我们是需要再次调用的。所以最好的场景可能是在延迟加载等场景中。当然为了简单起见我给出一个很简单的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  const _ = require(\u0026#34;lodash\u0026#34;); function foo(func ){ console.log(\u0026#34;invoked foo.\u0026#34;); func(); } var done = _.after(2,function bar(){ console.log(\u0026#34;invoke bar\u0026#34;); }); for( var i = 0; i \u0026amp;lt; 4; i++ ){ foo(done); }   正如我们前面说的，n的作用域是_.after函数内部，所以在执行过程中n会一直递减，因此输出结果应该是在调用两次foo之后调用一次bar，之后每次调用foo，都会调用一次bar。结果和我们预期的一致：\n1 2 3 4 5 6 7  invoked foo invoked foo invoke bar invoked foo invoke bar invoked foo invoke bar   那么我们再看看this指向的问题，我们修改一下上面的调用函数，让bar函数输出一下内部的this的一些属性：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  const _ = require(\u0026#34;lodash\u0026#34;); function foo(func ){ this.name = \u0026#34;foo\u0026#34;; console.log(\u0026#34;invoked foo: \u0026#34; + this.name ); func(); } var done = _.after(2,function bar(){ console.log(\u0026#34;invoke bar: \u0026#34; + this.name); }); for( var i = 0; i \u0026amp;lt; 4; i++ ){ foo(done); }   其实想来大家也应该能够猜到，在bar函数中输出的this.name也是foo：\n1 2 3 4 5 6 7  invoked foo: foo invoked foo: foo invoke bar: foo invoked foo: foo invoke bar: foo invoked foo: foo invoke bar: foo   这是因为bar的this应该指向的是_.after创建的函数的this,而这个函数是由foo函数调用的，因此this实际上指向就是foo。\n   _.map  _.map函数我们几乎随处可见，这个函数应用也相当广泛。\n1 2 3 4  function map(collection, iteratee) { var func = isArray(collection) ? arrayMap : baseMap; return func(collection, getIteratee(iteratee, 3)); }   为了简化问题，我们分析比较简单的情况：用一个func函数处理数组。\n1  _.map([1,2,3],func);   在处理数组的时候，lodash是分开处理的，对于Array采用arrayMap进行处理，对于对象则采用baseMap进行处理。\n我们先看数组arrayMap：\n1 2 3 4 5 6 7 8 9 10  function arrayMap(array, iteratee) { var index = -1, length = array == null ? 0 : array.length, result = Array(length); while (++index \u0026amp;lt; length) { result[index] = iteratee(array[index], index, array); } return result; }   这个函数是一个私有函数，第一个参数是一个需要遍历的数组，第二个参数是在遍历过程当中进行处理的函数；返回一个进行map处理之后的函数。\n在看我们需要进行遍历处理的函数iteratee，这个函数式通过getIteratee函数得到的：\n1 2 3 4 5  function getIteratee() { var result = lodash.iteratee || iteratee; result = result === iteratee ? baseIteratee : result; return arguments.length ? result(arguments[0], arguments[1]) : result; }   如果lodash.iteratee被重新定义,则使用用户定义的iteratee，否则就用官方定义的baseIteratee。需要强调的是，result(arguments[0],arguments[1])是柯里化的函数返回，返回的仍旧是一个函数。不可避免地，我们需要看看官方定义的baseIteratee的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  function baseIteratee(value) { // Don\u0026#39;t store the `typeof` result in a variable to avoid a JIT bug in Safari 9.  // See https://bugs.webkit.org/show_bug.cgi?id=156034 for more details.  if (typeof value == \u0026#39;function\u0026#39;) { return value; } if (value == null) { return identity; } if (typeof value == \u0026#39;object\u0026#39;) { return isArray(value) ? baseMatchesProperty(value[0], value[1]) : baseMatches(value); } return property(value); }   我们可以看出来，这个iteratee迭代者其实就是一个函数，在_.map中getIteratee(iteratee, 3)，给了两个参数，按照逻辑，最终返回的是一个baseIteratee，baseIteratee的第一个参数value就是iteratee,这是一个函数，所以，baseIteratee函数在第一个判断就返回了。\n所以我们可以将map函数简化为如下版本:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  function map(collection,iteratee){ return arrayMap(collection,getIteratee(iteratee,3)); } function arrayMap(array, iteratee) { var index = -1, length = array == null ? 0 : array.length, result = Array(length); while (++index \u0026amp;lt; length) { result[index] = iteratee(array[index], index, array); } return result; } function getIteratee() { var result = baseIteratee; return arguments.length ? result(arguments[0], arguments[1]) : result; } function baseIteratee(value) { if (typeof value == \u0026#39;function\u0026#39;) { return value; } }   可以看到，最终调用函数func的时候会传入3个参数。array[index],index,array。我们可以实验，将func实现如下：\n1 2 3 4 5 6  function func(){ console.log(“arguments[0] ” + arguments[0]); console.log(“arguments[1] ” + arguments[1]); console.log(“arguments[2] ” + arguments[2]); console.log(\u0026#34;-----\u0026#34;) }   输出的结果也和我们的预期一样，输出的第一个参数是该列表元素本身，第二个参数是数组下标，第三个参数是整个列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13  arguments[0] 6 arguments[1] 0 arguments[2] 6,8,10 ----- arguments[0] 8 arguments[1] 1 arguments[2] 6,8,10 ----- arguments[0] 10 arguments[1] 2 arguments[2] 6,8,10 ----- [ undefined, undefined, undefined ]   上面的分析就是抛砖引玉，先给出数组的分析，别的非数组，例如对象的遍历处理则会走到别的分支进行处理，各位看官有兴趣可以深入研究。\n   _.ary  这个函数是用来限制参数个数的。这个函数咋一看好像没有什么用，但我们考虑如下场景，将一个字符列表['6','8','10']转为整型列表[6,8,10]，用_.map实现，我们自然而然会写出这样的代码：\n1 2  const _ = require(\u0026#34;lodash\u0026#34;); _.map([\u0026#39;6\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;10\u0026#39;],parseInt);   好像很完美，我们输出看看：\n1  [ 6, NaN, 2 ]   很诡异是不是，看看内部到底发生了什么？其实看了上面的-.map函数的分析，其实原因已经很明显了。对于parseInt函数而言，其接收两个参数，第一个是需要处理的字符串，第二个是进制：\n1 2 3 4 5 6 7 8 9 10 11 12 13  /** * @param string 必需。要被解析的字符串。 * @param radix * 可选。表示要解析的数字的基数。该值介于 2 ~ 36 之间。 * 如果省略该参数或其值为 0，则数字将以 10 为基础来解析。如果它以 “0x” 或 “0X” 开头，将以 16 为基数。 * 如果该参数小于 2 或者大于 36，则 parseInt() 将返回 NaN */ parseInt(string, radix) /** 当参数 radix 的值为 0，或没有设置该参数时，parseInt() 会根据 string 来判断数字的基数。 举例，如果 string 以 \u0026#34;0x\u0026#34; 开头，parseInt() 会把 string 的其余部分解析为十六进制的整数。如果 string 以 0 开头，那么 ECMAScript v3 允许 parseInt() 的一个实现把其后的字符解析为八进制或十六进制的数字。如果 string 以 1 ~ 9 的数字开头，parseInt() 将把它解析为十进制的整数。 */   那么这样的输出也就不难理解了：\n处理第一个数组元素6的时候，parseInt实际传入参数(6,0),那么按照十进制解析，会得到6,处理第二个数组元素的时候传入的实际参数是(8,1)，返回NaN,对于第三个数组元素，按照2进制处理，则10返回的是2。\n所以在上述需求的时候我们需要限制参数的个数，这个时候_.ary函数就登场了，上面的函数这样处理就没有问题了：\n1 2  const _ = require(\u0026#34;lodash\u0026#34;); _.map([\u0026#39;6\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;10\u0026#39;],_.ary(parseInt),1);   我们看看这个函数是怎么实现的：\n1 2 3 4 5  function ary(func, n, guard) { n = guard ? undefined : n; n = (func \u0026amp;\u0026amp; n == null) ? func.length : n; return createWrap(func, WRAP_ARY_FLAG, undefined, undefined, undefined, undefined, n); }   这个函数先检查n的值，需要说明的是func.length返回的是函数的声明参数个数。然后返回了一个createWrap包裹函数，这个函数可以说是脏活累活处理工厂了，负责很多函数的包裹处理工作，而且为了提升性能，还将不同的判断用bitflag进行与/非处理，可以说是很用尽心机了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  /** * Creates a function that either curries or invokes `func` with optional * `this` binding and partially applied arguments. * * @private * @param {Function|string} func The function or method name to wrap. * @param {number} bitmask The bitmask flags. * 1 - `_.bind` 1 0b0000000000000001 * 2 - `_.bindKey` 0b0000000000000010 * 4 - `_.curry` or `_.curryRight`... 0b0000000000000100 * 8 - `_.curry` 0b0000000000001000 * 16 - `_.curryRight` 0b0000000000010000 * 32 - `_.partial` 0b0000000000100000 * 64 - `_.partialRight` 0b0000000001000000 * 128 - `_.rearg` 0b0000000010000000 * 256 - `_.ary` 0b0000000100000000 * 512 - `_.flip` 0b0000001000000000 * @param {*} [thisArg] The `this` binding of `func`. * @param {Array} [partials] The arguments to be partially applied. * @param {Array} [holders] The `partials` placeholder indexes. * @param {Array} [argPos] The argument positions of the new function. * @param {number} [ary] The arity cap of `func`. * @param {number} [arity] The arity of `func`. * @returns {Function} Returns the new wrapped function. */ function createWrap(func, bitmask, thisArg, partials, holders, argPos, ary, arity) { var isBindKey = bitmask \u0026amp; WRAP_BIND_KEY_FLAG; if (!isBindKey \u0026amp;\u0026amp; typeof func != \u0026#39;function\u0026#39;) { throw new TypeError(FUNC_ERROR_TEXT); } var length = partials ? partials.length : 0; if (!length) { bitmask \u0026amp;= ~(WRAP_PARTIAL_FLAG | WRAP_PARTIAL_RIGHT_FLAG); partials = holders = undefined; } ary = ary === undefined ? ary : nativeMax(toInteger(ary), 0); arity = arity === undefined ? arity : toInteger(arity); length -= holders ? holders.length : 0; if (bitmask \u0026amp; WRAP_PARTIAL_RIGHT_FLAG) { var partialsRight = partials, holdersRight = holders; partials = holders = undefined; } var data = isBindKey ? undefined : getData(func); var newData = [ func, bitmask, thisArg, partials, holders, partialsRight, holdersRight, argPos, ary, arity ]; if (data) { mergeData(newData, data); } func = newData[0]; bitmask = newData[1]; thisArg = newData[2]; partials = newData[3]; holders = newData[4]; arity = newData[9] = newData[9] === undefined ? (isBindKey ? 0 : func.length) : nativeMax(newData[9] - length, 0); if (!arity \u0026amp;\u0026amp; bitmask \u0026amp; (WRAP_CURRY_FLAG | WRAP_CURRY_RIGHT_FLAG)) { bitmask \u0026amp;= ~(WRAP_CURRY_FLAG | WRAP_CURRY_RIGHT_FLAG); } if (!bitmask || bitmask == WRAP_BIND_FLAG) { var result = createBind(func, bitmask, thisArg); } else if (bitmask == WRAP_CURRY_FLAG || bitmask == WRAP_CURRY_RIGHT_FLAG) { result = createCurry(func, bitmask, arity); } else if ((bitmask == WRAP_PARTIAL_FLAG || bitmask == (WRAP_BIND_FLAG | WRAP_PARTIAL_FLAG)) \u0026amp;\u0026amp; !holders.length) { result = createPartial(func, bitmask, thisArg, partials); } else { result = createHybrid.apply(undefined, newData); } var setter = data ? baseSetData : setData; return setWrapToString(setter(result, newData), func, bitmask); }   看上去太复杂了，把无关的代码削减掉：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  function createWrap(func, bitmask, thisArg, partials, holders, argPos, ary, arity) { // 0000000100000000 \u0026amp; 0000000000000010  // var isBindKey = bitmask \u0026amp; WRAP_BIND_KEY_FLAG;  var isBindKey = 0; var length = 0; // if (!length) {  // 0000000000100000 | 0000000001000000  // ~(0000000001100000)  // 1111111110011111  // \u0026amp;0000000100000000  // 0000000100000000 = WRAP_ARY_FLAG  // bitmask \u0026amp;= ~(WRAP_PARTIAL_FLAG | WRAP_PARTIAL_RIGHT_FLAG);  // bitmask = WRAP_ARY_FLAG;  // partials = holders = undefined;  // }  bitmask = WRAP_ARY_FLAG; partials = holders = undefined; ary = undefined; arity = arity === undefined ? arity : toInteger(arity); // because holders == undefined  //length -= 0;  // because isBindKey == 0  // var data = isBindKey ? undefined : getData(func);  var data = getData(func); var newData = [ func, bitmask, thisArg, partials, holders, partialsRight, holdersRight, argPos, ary, arity ]; if (data) { mergeData(newData, data); } func = newData[0]; bitmask = newData[1]; thisArg = newData[2]; partials = newData[3]; holders = newData[4]; arity = newData[9] = newData[9] === undefined ? func.length : newData[9]; result = createHybrid.apply(undefined, newData); var setter = data ? baseSetData : setData; return setWrapToString(setter(result, newData), func, bitmask); }   简化了一些之后我们来到了createHybrid函数，这个函数也巨复杂，所以我们还是按照简化方法，把我们用不到的逻辑给简化:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  function createHybrid(func, bitmask, thisArg, partials, holders, partialsRight, holdersRight, argPos, ary, arity) { var isAry = bitmask \u0026amp; WRAP_ARY_FLAG, isBind = bitmask \u0026amp; WRAP_BIND_FLAG, isBindKey = bitmask \u0026amp; WRAP_BIND_KEY_FLAG, isCurried = bitmask \u0026amp; (WRAP_CURRY_FLAG | WRAP_CURRY_RIGHT_FLAG), isFlip = bitmask \u0026amp; WRAP_FLIP_FLAG, Ctor = isBindKey ? undefined : createCtor(func); function wrapper() { var length = arguments.length, args = Array(length), index = length; while (index--) { args[index] = arguments[index]; } if (isCurried) { var placeholder = getHolder(wrapper), holdersCount = countHolders(args, placeholder); } if (partials) { args = composeArgs(args, partials, holders, isCurried); } if (partialsRight) { args = composeArgsRight(args, partialsRight, holdersRight, isCurried); } length -= holdersCount; if (isCurried \u0026amp;\u0026amp; length \u0026amp;lt; arity) { var newHolders = replaceHolders(args, placeholder); return createRecurry( func, bitmask, createHybrid, wrapper.placeholder, thisArg, args, newHolders, argPos, ary, arity - length ); } var thisBinding = isBind ? thisArg : this, fn = isBindKey ? thisBinding[func] : func; length = args.length; if (argPos) { args = reorder(args, argPos); } else if (isFlip \u0026amp;\u0026amp; length \u0026amp;gt; 1) { args.reverse(); } if (isAry \u0026amp;\u0026amp; ary \u0026amp;lt; length) { args.length = ary; } if (this \u0026amp;\u0026amp; this !== root \u0026amp;\u0026amp; this instanceof wrapper) { fn = Ctor || createCtor(fn); } return fn.apply(thisBinding, args); } return wrapper; }   把不需要的逻辑削减掉：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  function createHybrid(func, bitmask, thisArg, partials, holders, partialsRight, holdersRight, argPos, ary, arity) { var isAry = 1; function wrapper() { var length = arguments.length, args = Array(length), index = length; while (index--) { args[index] = arguments[index]; } var thisBinding = this, fn = func; length = args.length; if (isAry \u0026amp;\u0026amp; ary \u0026amp;lt; length) { args.length = ary; } return fn.apply(thisBinding, args); } return wrapper; }   好了，绕了一大圈，终于看到最终的逻辑了，_.ary函数其实就是把参数列表重新赋值了一下，并进行了长度限制。想想这个函数实在是太麻烦了，我们自己可以根据这个逻辑实现一个简化版的_.ary：\n1 2 3 4 5 6 7 8 9 10 11 12  function ary(func,n){ return function(){ var length = arguments.length, args = Array(length), index = length; while(index--){ args[index] = arguments[index]; } args.length = n; return func.apply(this,args); } }   试试效果：\n1  console.log(_.map([\u0026#39;6\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;10\u0026#39;],ary(parseInt,1)));   工作得很不错：\n1  [ 6, 8, 10 ]      小结  今天分析这三个函数就花了一整天的时间，但是收获颇丰，能够静下心来好好分析一个著名的开源库，并能够理解透里面的一些逻辑，确实是一件很有意思的事情。我会在有时间的时候把Lodash这个我很喜欢的库都好好分析一遍，尽我最大的努力将里面的逻辑表述清楚，希望能够简明易懂。\n","date":"2017-08-21","permalink":"https://chenquan.me/posts/lodash-source-code-analysis-1/","tags":["javascript","lodash","前端"],"title":"Lodash 源码分析（一）“Function” Methods"},{"categories":null,"contents":"对于实时系统而言，垃圾回收系统可能是一个极大的隐患，因为在垃圾回收的时候需要将整个程序暂停。所以在我们设计消息总线系统的时候，需要小心地选择我们的语言。Go一直在强调它的低延迟，但是它真的做到了吗？如果是的，它是怎么做到的呢？\n在这篇文章当中，我们将会看到Go语言的GC是如何实现的（tricolor algorithm，三色算法），以及为什么这种方法能够达到如此之低的GC暂停，以及最重要的是，它是否真的有效（对这些GC暂停进行benchmar测试，以及同其它类型的语言进行比较）。\n   From Haskell to Go  我们用pub/sub消息总线系统为例说明问题，这些系统在发布消息的时候都是in-memory存储的。在早期，我们用Haskell实现了第一版的消息系统，但是后面发现GHC的gabage collector存在一些基础延迟的问题，我们放弃了这个系统转而用Go进行了实现。\n这是我们有关Haskell消息系统的一些实现细节,在GHC中最重要的一点是它GC暂停时间同当前的工作集的大小成比例关系（也就是说，GC时间和内存中存储对象的数目有关）。在我们的例子中，内存中存储对象的数目往往都非常巨大，这就导致gc时间常常高达数百毫秒。这就会导致在GC的时候整个系统是阻塞的。\n而在Go语言中，不同于GHC的全局暂停(stop-the-world)收集器，Go的垃圾收集器是和主程序并行的。这就可以避免程序的长时间暂停。我们则更加关注于Go所承诺的低延迟以及其在每个新版本中所提及的延迟提升是否真的向他们所说的那样。\n   并行垃圾回收是如何工作的?  Go的GC是如何实现并行的呢？其中的关键在于tricolor mark-and-sweep algorithm 三色标记清除算法。该算法能够让系统的gc暂停时间成为能够预测的问题。调度器能够在很短的时间内实现GC调度，并且对源程序的影响极小。下面我们看看三色标记清除算法是如何工作的：\n假设我们有这样的一段链表操作的代码：\nvar A LinkedListNode; var B LinkedListNode; // ... B.next = \u0026LinkedListNode{next: nil}; // ... A.next = \u0026LinkedListNode{next: nil}; *(B.next).next = \u0026LinkedListNode{next: nil}; B.next = *(B.next).next; B.next = nil;     第一步  var A LinkedListNode; var B LinkedListNode; // ... B.next = \u0026LinkedListNode{next: nil};  刚开始我们假设有三个节点A、B和C，作为根节点，红色的节点A和B始终都能够被访问到，然后进行一次赋值B.next = \u0026amp;C。初始的时候垃圾收集器有三个集合，分别为黑色，灰色和白色。现在，因为垃圾收集器还没有运行起来，所以三个节点都在白色集合中。\n\n   第二步  我们新建一个节点D,并将其赋值给A.next。即：\nvar A LinkedListNode; var B LinkedListNode; // ... B.next = \u0026LinkedListNode{next: nil}; // ... A.next = \u0026LinkedListNode{next: nil};  需要注意的是，作为一个新的内存对象，需要将其放置在灰色区域中。为什么要将其放在灰色区域中呢？这里有一个规则，如果一个指针域发生了变化，则被指向的对象需要变色。因为所有的新建内存对象都需要将其地址赋值给一个引用，所以他们将会立即变为灰色。（这就需要问了，为什么C不是灰色？）\n\n   第三步  在开始GC的时候，根节点将会被移入灰色区域。此时A、B、D三个节点都在灰色区域中。由于所有的程序子过程(process，因为不能说是进程，应该算是线程，但是在go中又不完全是线程)要么事程序正常逻辑，要么是GC的过程，而且GC和程序逻辑是并行的，所以程序逻辑和GC过程应该是交替占用CPU资源的。\n\n   第四步 扫描内存对象  在扫描内存对象的时候，GC收集器将会把该内存对象标记为黑色，然后将其子内存对象标记为灰色。在任一阶段，我们都能够计算当前GC收集器需要进行的移动步数：2*|white| + |grey|,在每一次扫描GC收集器都至少进行一次移动，直到达到当前灰色区域内存对象数目为0。\n\n   第五步  程序此时的逻辑为，新赋值一个内存对象E给C.next，代码如下：\nvar A LinkedListNode; var B LinkedListNode; // ... B.next = \u0026LinkedListNode{next: nil}; // ... A.next = \u0026LinkedListNode{next: nil}; //新赋值 C.next = \u0026E *(B.next).next = \u0026LinkedListNode{next: nil};  按照我们之前的规则，新建的内存对象需要放置在灰色区域，如图所示：\n\n这样做，收集器需要做更多的事情，但是这样做当在新建很多内存对象的时候，可以将最终的清除操作延迟。值得一提的是，这样处理白色区域的体积将会减小，直到收集器真正清理堆空间时再重新填入移入新的内存对象。\n   第六步 指针重新赋值  程序逻辑此时将 B.next.next赋值给了B.next，也就是将E赋值给了B.next。代码如下：\nvar A LinkedListNode; var B LinkedListNode; // ... B.next = \u0026LinkedListNode{next: nil}; // ... A.next = \u0026LinkedListNode{next: nil}; *(B.next).next = \u0026LinkedListNode{next: nil}; // 指针重新赋值: B.next = *(B.next).next;  这样做之后，如图所示，C将不可达。\n\n这就意味着，收集器需要将C从白色区域移除，然后在GC循环中将其占用的内存空间回收。\n   第七步  将灰色区域中没有引用依赖的内存对象移动到黑色区域中，此时D在灰色区域中没有其它依赖，并依赖于它的内存对象A已经在黑色区域了，将其移动到黑色区域中。\n\n   第八步  在程序逻辑中，将B.next赋值为了nil,此时E将变为不可达。但此时E在灰色区域，将不会被回收，那么这样会导致内存泄漏吗？其实不会，E将在下一个GC循环中被回收，三色算法能够保证这点：如果一个内存对象在一次GC循环开始的时候无法被访问，则将会被冻结，并在GC的最后将其回收。\n\n   第九步  在进行第二次GC循环的时候，将E移入到黑色区域，但是C并不会移动，因为是C引用了E，而不是E引用C。\n\n   第十步  收集器再扫描最后一个灰色区域中的内存对象B，并将其移动到黑色区域中。\n\n   第十一步 回收白色区域  现在灰色区域已经没有内存对象了，这个时候就将白色区域中的内存对象回收。在这个阶段，收集器已经知道白色区域的内存对象已经没有任何引用且不可访问了，就将其当做垃圾进行回收。而在这个阶段，E不会被回收，因为这个循环中，E才刚刚变为不可达，它将在下个循环中被回收。\n\n   第十二步 区域变色  这一步是最有趣的，在进行下次GC循环的时候，完全不需要将所有的内存对象移动回白色区域，只需要将黑色区域和白色区域的颜色换一下就好了，简单而且高效。\n\n   GC三色算法小结  上面就是三色标记清除算法的一些细节，在当前算法下仍旧有两个阶段需要 stop-the-world：一是进行root内存对象的栈扫描；二是标记阶段的终止暂停。令人激动的是，标记阶段的终止暂停将被去除。在实践中我们发现，用这种算法实现的GC暂停时间能够在超大堆空间回收的情况下达到\u0026lt;1ms的表现。\n   延迟 VS 吞吐  如果一个并行GC收集器在处理超大内存堆时能够达到极低的延迟，那么为什么还有人在用stop-the-world的GC收集器呢？难道Go的GC收集器还不够优秀吗？\n这不是绝对的，因为低延迟是有开销的。最主要的开销就是，低延迟削减了吞吐量。并发需要额外的同步和赋值操作,而这些操作将会占用程序的处理逻辑的时间。而Haskell的GHC则针对吞吐量进行了优化，Go则专注于延迟，我们在考虑采用哪种语言的时候需要针对我们自己的需求进行选择，对于推送系统这种实时性要求比较高的系统，选择Go语言则是权衡之下得到的选择。\n   实际表现  目前而言，Go好像已经能够满足低延迟系统的要求了，但是在实际中的表现又怎么样呢？利用相同的benchmark测试逻辑实现进行比较：该基准测试将不断地向一个限定缓冲区大小的buffer中推送消息，旧的消息将会不断地过期并成为垃圾需要进行回收，这要求内存堆需要一直保持较大的状态，这很重要，因为在回收的阶段整个内存堆都需要进行扫描以确定是否有内存引用。这也是为什么GC的运行时间和存活的内存对象和指针数目成正比例关系的原因。\n这是Go语言版本的基准测试代码，这里的buffer用数组实现:\npackage main import ( \"fmt\" \"time\" ) const ( windowSize = 200000 msgCount = 1000000 ) type ( message []byte buffer [windowSize]message ) var worst time.Duration func mkMessage(n int) message { m := make(message, 1024) for i := range m { m[i] = byte(n) } return m } func pushMsg(b *buffer, highID int) { start := time.Now() m := mkMessage(highID) (*b)[highID%windowSize] = m elapsed := time.Since(start) if elapsed \u0026gt; worst { worst = elapsed } } func main() { var b buffer for i := 0; i \u0026lt; msgCount; i++ { pushMsg(\u0026b, i) } fmt.Println(\"Worst push time: \", worst) }  相同的逻辑，不同语言实现(Haskell/Ocaml/Racke、Java)，在同等测试条件下进行的测试结果如下：\n   Benchmark Longest pause (ms)     OCaml 4.03.0 (map based) (manual timing) 2.21   Haskell/GHC 8.0.1 (map based) (rts timing) 67.00   Haskell/GHC 8.0.1 (array based) (rts timing) 58.60   Racket 6.6 experimental incremental GC (map based) (tuned) (rts timing) 144.21   Racket 6.6 experimental incremental GC (map based) (untuned) (rts timing) 124.14   Racket 6.6 (map based) (tuned) (rts timing) 113.52   Racket 6.6 (map based) (untuned) (rts timing) 136.76   Go 1.7.3 (array based) (manual timing) 7.01   Go 1.7.3 (map based) (manual timing) 37.67   Go HEAD (map based) (manual timing) 7.81   Java 1.8.0_102 (map based) (rts timing) 161.55   Java 1.8.0_102 G1 GC (map based) (rts timing) 153.89    令人惊讶的是Java,表现得非常一般，而OCaml则非常之好，OCaml语言能够达到约3ms的GC暂停时间，这是因为OCaml采用的GC算法是incremental GC algorithm(而在实时系统中不采用OCaml的原因是该语言对多核的支持不好)。\n正如表中显示的,Go的GC暂停时间大约在7ms左右，表现也好，已经完全能够满足我们的要求。\n   一些注意事项   进行基准测试往往需要多加小心，因为不同的运行时针对不同的测试用例都有不同程度的优化，所以表现往往也有差异。而我们需要针对自己的需求来编写测试用例，对于基准测试应该能够满足我们自己的产品需求。在上面的例子中可以看到，Go已经完全能够满足我们的产品需求。 Map Vs. Array： 最初我们的基准测试是在map中进行插入和删除操作的，但是Go在对大型的map进行GC的时候存在Bug。因此在设计Go的基准测试的时候用可修改的Array作为Map的替代。Go map的Bug已经在1.8版本中得到了修复，但是并不是所有的基准测试都得到了修正，这也是我们需要正视的一些问题。但是不管怎么说，没有理由说GC时间将会因为使用map导致大幅度增长（除去bug和糟糕的实现之外）。 manual timing Vs. rst timing :作为另一个注意事项，有些基准测试则在不同的计时系统下将会有所差异，因为有些语言不支持运行时时间统计，例如Go，而有些语言则支持。因此，我们应该在测试时候都把计时方式设置为manual timing。 最后一个需要注意的事项是测试用例的实现将会极大地影响基准测试的结果，如果map的插入删除实现方式比较糟糕，则将会对测试结果造成不利影响，这也是用array的另一个原因。     为什么Go的结果不能再好点？  尽管我们采用的map bugfixed版本或者是array版本的go实现能够达到~7ms的GC暂停表现，这已经很好了，但是根据Go官方发布的“1.5 Garbage Benchmark Latency”](https://talks.golang.org/2015/go-gc.pdf) ， 在200MB的堆内存前提下，能够达到~1ms的GC暂停延时(经管GC暂停时间应该和指针引用数目有关而和堆所占用的容量无关但我们无法得到确切数据)。而Twitch团队也发布文章称在Go1.7中能够达到约1ms的GC延迟。\n在联系go-nuts mail list之后得到的答案是，这些暂停实验可能是因为一些未修复的bug导致的。空闲的标记worker可能会对程序逻辑造成阻塞，为了确定这个问题，我采用了go tool trace，一个可视化工具对go的运行时行为进行了跟踪。\n\n正如图所示，这里有近12ms的后台mark worker运行在所有的processor（CPU核?）中。这让我更加确信是上述的bug导致的该问题。\n   总结  这次调查的重点在于GC要么关注于低延迟，要么关注于高吞吐。当然这些也都取决于我们的程序是如何使用堆空间的(我们是否有很多内存对象？每个对象的生命周期是长还是短？)\n理解底层的GC算法对该系统是否适用于你的测试用例是非常重要的。当然GC系统的实际实现也至关重要。你的基准测试程序的内存占用应该同你将要实现的真正程序类似，这样才能够在实践中检验GC系统对于你的程序而言是否高效。正如前文所说的，Go的GC系统并不完美，但是对于我们的系统而言是可以接受的。\n尽管存在一些问题，但是Go的GC表现已经优于大部分同样拥有GC系统的语言了，Go的开发团队针对GC延迟进行了优化，并且还在继续。Go的GC确实是有可圈可点之处，无论是理论上还是实践中。\n 原文 Golang’s Real-time GC in Theory and Practice[en]\n ","date":"2017-08-20","permalink":"https://chenquan.me/posts/golang-gc-algorithm-3-colors/","tags":["gc","golang","垃圾回收"],"title":"Go实时GC——三色算法理论与实践"},{"categories":null,"contents":"在Go1.1版本中，加入了由Dmitry Vyukov贡献的新的调度器。新的调度器能够动态地调整Go程序的并发性能，而且表现非常出色。因此我想写篇文章介绍一下这个调度器。\n这篇博客里面大多数东西都已经被包含在了原始设计文档中了，这个文档的内容相当详实，但是过于技术化了。\n虽然有关新调度器所有东西都在那个设计文档中，但这篇博客有图片配图了，所以比设计文档清晰易懂多了。\n   Go运行时为什么需要调度器？  在我们开始研究新调度器之前，我们首先需要理解为什么需要调度器。既然操作系统已经能够为我们调度线程了，我们为什么又创造了一个用户实现的调度器？\nPOSIX的线程API是对现有Unix进程模型在逻辑上的一个强大大的扩展，使得线程得到了很多类似进程的控制能力。比如，线程有它自己的信号码，线程能够被赋予CPU affinity功能(就是指定线程只能在某个CPU上运行），线程能被添加到cgroups中，线程所占用的的资源也能够查询。这些额外特性大大增加了Go 程序在创建goroutine 时所需要的开销，是在线程多达100000的时候尤其明显。\n另外一个问题是，操作系统无法针对Go的设计模型给出很好的调度决策。比如，当运行一次垃圾收集的时候，Go的垃圾收集器要求所有线程都被暂停，并且要求内存要处于一致状态（consistent state）。这就要求所有等待所有正在运行的线程到同时达到一个一致点，而我们事先知道在这个一致点内存是一致的。\n当很多被调度的线程分散在随机点（random point）上时，你不得不等待他们中的大多数到达一致状态才能进行垃圾回收。而Go调度器却能够只在内存一致的时候进行调度，这就意味着我们只需要等待在一个CPU核心中的活跃线程达到一致即可。\n   来看看里面的各个角色（Our Cast of Characters）  目前有三个常见的线程模型。一个是N：1的，即多个用户空间线程运行在一个OS线程上。这个模型可以很快的进行上下文切换，但是不能利用多核系统（multi-core systems)的优势。另一个模型是1：1的，即可执行程序的一个线程匹配一个OS线程。这个模型能够利用机器上的所有核心的优势，但是上下文切换非常慢，因为它需要进行系统内核调用。\nGo试图通过M：N的调度器去获取这两种模型的全部优势。它在任意数目的OS线程上调用任意数目的goroutines。你可以快速进行上下文切换，并且还能利用你系统上所有的核心的优势。这个模型主要的缺点是它增加了调度器的复杂性。\n为了完成调度任务，Go调度器使用了三个实体：\n\n三角形表示OS线程，它是由OS管理的可执行程序的一个线程，而且工作起来特别像你的标准POSIX线程。在运行时代码里，它被成为M，即机器（machine）。\n圆形表示一个goroutine。它包括栈、指令指针以及对于调度goroutines很重要的其它信息，比如阻塞它的任何channel。在运行时代码里，它被称为G。\n矩形表示调度上下文。你可以把它看作是一个在单线程上进行线程调度的自定义版本。它是让我们从N:1调度器转到M:N调度器的重要部分。在运行时代码里，它被叫做P，即处理器（processor）。这部分将会进行较为详细的介绍。\n\n我们可以从上面的图里看到两个线程（M），每个线程都拥有一个上下文（P），每个线程都正在运行一个goroutine（G）。为了运行goroutines，一个线程必须拥有一个上下文。\n上下文的数目在启动时被设置为环境变量GOMAXPROCS的值或者通过运行时函数GOMAXPROCS()来设置。通常，在你的程序执行时它不会发生变化。上下文的数目被固定的意思是，只有GOMAXPROCS个上下文正在任意时刻上运行Go代码。我们可以使用它调整Go进程的调用使其适合于一个单独的计算机，比如一个4核的PC中可以在4个线程上运行Go代码。\n外部的灰色goroutines没在运行，但是已经准备好被调度了。它们被安排成一个叫做runqueue的列表。当一个goroutine执行一个go 语句的时候，goroutine就被添加到runqueue的末端。一旦一个上下文已经运行一个goroutine到了一个点上，它就会把一个goroutine从它的runqueue给pop出来，设置栈和指令指针并且开始运行这个goroutine。\n为了降低mutex竞争，每一个上下文都有它自己的运行队列。在Go调度器的早期版本中，Go的调度器只有一个全全局的运行队列，并且只有一个互斥锁(mutex)来保护它。这样的设计在32核的多CPU核机器上进行性能压榨时往往会得到较差的表现。\n只要有goroutine在运行，调度器就能够以一个稳定的状态运行。然而还是有一些情况能够改变这种状态。\n   系统调用细节  你可能想问，为什么一定要有上下文？我们能不能去掉上下文而把运行队列直接放到线程上运行？不可以，上下文存在的意义是，如果一个正在运行的线程因为一些原因需要阻塞，我们可以将其挂起并将线程上下文移交给别的线程。\n另一种需要阻塞的情况是，在进行系统调用的时候。因为一个线程在进行系统调用的时候不能既执行代码又进行阻塞等待（注：一个M控制多个G,在系统调用的时候不允许其中的某个G阻塞，而另一个G运行，因为系统调用必须是原子性的）。因此我们需要将该上下文冻结以便调度。\n\n从上图我们能够看出，一个线程放弃了它的上下文以便让别的的线程可以运行G0。调度器确保预先有足够的线程来运行所有的上下文。上图中的M1 可能是仅仅为了能够处理系统调用而创建的，也可能来自一个线程池。这个处于系统调用中的线程将会一直持有导致这次系统调用这的goroutine，因为从技术层面上来说，它仍然在执行，尽管可能阻塞在OS里了。\n当这个系统调用返回的时候，这个线程必须获取一个上下文来运行这个返回的goroutine。较为常见的操作是从其它线程中“偷取”一个上下文。如果“偷取”不成功，它就会把它的goroutine放回到一个全局运行队列中，然后把自己放回到线程池中然后进入睡眠状态。\n这个全局运行队列是各个上下文在运行完自己的本地运行队列后获取新goroutine的地方。上下文也会周期性的检查这个全局运行队列上的以获取goroutine。如果不这样做的话，全局运行队列上的goroutines由于饥渴可能导致无法执行而终止。\nGo程序要在多线程上运行就是因为要处理系统调用，哪怕GOMAXPROCS等于1。运行时使用调用系统调用的goroutines，而不是线程。\n   盗取工作  系统的稳定状态改变的另外一种情况是，当一个上下文运行完要被调度的所有goroutines的时。如果各个上下文的运行队列里的goroutine的数目不均衡，将会改变调度器状态。上述的情况会导致会导致一个上下文在执行完它的运行队列后就结束了，尽管系统中仍然有许多goroutine要执行。因此为了能够一直运行Go代码，一个上下文能够从全局运行队列中获取goroutine，但是如果全局运行队列中也没有goroutine了，那么上下文就不得不从其它地方获取goroutine了。\n\n这里的“其它地方”指的是就是其它的上下文！当一个上下文完成自己的全部任务后，它就会尝试“盗取”另一个上下文运行队列中一半的工作量。这将确保每个上下文总是有活干，同时也能够保证所有的线程都处于最大的负荷。\n   展望  关于调度器还有许多细节，像cgo线程、LockOSThread()函数以及与系统与网络poller的整合。这些已经超过这篇文章的要探讨的范围了，但是仍然值得去深入研究。在Go运行时库里，仍然有大量有意思的创建工作要做。\n 原文 Daniel Morsing\n翻译 Chen Quan\n Releated articles:\n Causal Profiling for Go Effective error handling in Go. The Go netpoller  ","date":"2017-08-19","permalink":"https://chenquan.me/posts/golang-thread-scheduler/","tags":["go","线程调度"],"title":"Go 线程调度器"},{"categories":null,"contents":"本文就是我在学习函数式编程的过程当中自己体悟到的一些东西，这里将用go,JavaScript以及Haskell三种语言来分析函数式编程的一些奥秘。JavaScript由于具有的一些优势能够让我们可以实现函数式编程，而go作为一种强类型语言，虽然灵活性又稍有欠缺，但是也能够完成一些高阶函数的实现，Haskell语言作为正统的函数式编程语言，为了解释说明问题，作为对比参照。\n   正文  函数式编程也算是经常看到了，它的一些优势包括：\n 不包括赋值语句(assignment statement),一个变量一旦初始化，就无法被修改(immutable) 无副作用，函数除了计算结果，将不会产生任何副作用 因为无副作用，所以任何表达式在任何时候都能够evaluate  虽然上面的优势看看上去好像很厉害的样子，但是，到底厉害在哪里呢？我们可以通过下面的例子进行说明：\n求和函数\nHaskell\n1 2 3 4  sum [1,2,3] -- 6 -- sum 的实现其实是 foldr (+) 0 [1,2,3]   在Haskell中flodr的函数定义是：\n1  foldr :: Foldable t =\u0026amp;gt; (a -\u0026amp;gt; b -\u0026amp;gt; b) -\u0026amp;gt; b -\u0026amp;gt; t a -\u0026amp;gt; b   函数实现是:\n1 2 3 4  -- if the list is empty, the result is the initial value z; else -- apply f to the first element and the result of folding the rest foldr f z [] = z foldr f z (x:xs) = f x (foldr f z xs)   这是一个递归实现，在函数式编程中，递归定义是十分常见的。\nfoldr函数其实做了这样的事情：foldr接受三个参数，第一个参数是函数f，第二个参数是初始值z,第三个参数是一个列表。如果列表为空则返回初始化值z，否则递归调用 foldr,需要说明的是函数f的类型是接受两个参数，返回一个值，两个参数类型都应该和z相同（强类型语言中）。\n在Haskell中我们能够看到一个列表能够这样被求和，那么在JavaScript中，我们是如何实现sum函数的呢？\nJavaScript\n首先我们实现js版本的foldr\n1 2 3 4 5 6 7 8 9 10 11  function foldr(f,z,list){ //为了简洁起见，把类型判断省略了  // Object.prototype,toString.call(list) === \u0026#39;[object Array]\u0026#39;  if(list === null || list.length == 0){ return z; } //这里的shift会改变参数的状态，会造成副作用  //return f(list.shift(),foldr(f,z,list));  //改用如下写法  return f(list[0],foldr(f,z,list.slice(1))); }   然后我们再实现js版本的(+):\n1 2 3  function add(a,b){ return a+b; }   那么我们的sum就变成：\n1 2 3  function sum(list){ return foldr(add,0,list); }   最后我们的js版的sum也可以这样用了：\n1 2  let a = [1,2,3]; sum(a); // 6   像js这样的弱类型的语言较为灵活，函数f可以任意实现，对于foldr函数也能够在多种数据类型之间复用，那么对于像go这样的强类型语言，结果又是怎么样的呢？\ngo\n同样地，我们实现以下go版本的foldr:\n1 2 3 4 5 6  func foldr(f func(a,b int) int,z int,list []int)int{ if len(list) == 0{ return z } return f(list[0],foldr(f,z,list[1:])) }   go因为有数组切片，所以使用起来较为简单，但是go又是强类型的语言，所以在声明函数的时候必须要把类型声明清楚。\n再实现一下f函数：\n1 2 3  func add(a,b int) int{ return a+b; }   依葫芦画瓢我们可以得到go版本的sum函数：\n1 2 3  func sum(list []int) int{ return foldr(add,0,list) }   可以看出来好像套路都差不多，真正在调用的时候是这样的：\n1 2 3 4  func main(){ a := []int{1,2,3} sum(a) // 6 }   在Haskell中是没有循环的，因为循环可以通过递归实现，在上文我们实现的sum函数中，也没有用到任何循环语句，这和我们原来的编程思维有所不同，刚开始我学写求和函数的时候，都是从for,while开始的，但是函数式给我打开了新世界的大门。\n有了上面的基础，我们发现在函数式编程中，代码的重用非常便利：\n求积函数\njavaScript\n1 2 3 4 5 6 7  function muti(a,b){ return a*b; } function product(list){ return foldr(muti,1,list); }   go\n1 2 3 4 5 6 7  func muti(a,b int) int{ return a*b; } func product(list []int) int{ return foldr(muti,1,list) }   Haskell\n1 2 3 4 5 6  foldr (*) 1 [1,2,3,4] -- 24 -- or  -- product 是Haskell预定义的函数 myproduct xs = foldr (*) 1 xs -- myproduct [1,2,3,4]    还有很多例如 anyTrue、allTrue的例子,以下仅给出js实现：\nanyTure\nJavaScript\n1 2 3 4 5 6 7  function or(a,b){ return a || b; } function anyTrue(list){ return foldr(or,false,list); }   调用：\n1 2  let b = [true,false,true]; console.log(anyTrue(b)); // true   allTure\nJavaScript\n1 2 3 4 5 6 7  function and(a,b){ return a \u0026amp;\u0026amp; b; } function allTrue(list){ return foldr(and,true,list); }   调用:\n1 2  let b = [true,false,true]; console.log(allTrue(b)); // false   当然我们可以看出来这个flodr函数贼好用，但是好像还是有点疑惑，它是怎么工作的呢？看了一圈，flodr就是一个递归函数，但其实在编程世界，它还有一个更加出名的名字——reduce。我们看看在js中是如何使用reduce实现sum函数的：\n求和函数reduce版\n1 2 3 4  const _ = require(\u0026#34;lodash\u0026#34;); _.reduce([1,2,3],function(sum,n){ return sum+n; });   在lodash官方文档是这么定义的：\n1 2  _.reduce alias _.foldl _.reduceRight alias _.foldr   好吧，我欺骗了你们，其实foldr应该对应reduceRight。\n那么foldl和foldr到底有什么不同呢？\n其实这两个函数的不同之处在于结合的方式不同，以求差为例：\nHaskell\n1 2 3 4  foldr (-) 0 [1,2,3] -- 输出: 2 foldl (-) 0 [1,2,3] -- 输出: -6   为什么两个输出是不同的呢？这个和结合方向有关:\nfoldr (-) 0 [1,2,3]\n相当于：\n1-(2-(3-0)) = 2\n而\nfoldl (-) 0 [1,2,3]\n相当于：\n((0-1)-2)-3) = -6\n结合方向对于求和结果而言是没有区别的，但是对于求差，就有影响了：\nJavaScript\n1 2 3 4 5 6  const _ = require(\u0026#34;lodash\u0026#34;); //reduce 相当于 foldl _.reduce([1,2,3],function(sum,n){ return sum-n; }); // 输出 -4   这个和说好的-6好像又不一样了，坑爹呢么不是？！这是因为，在lodash的实现中，reduce的初始值为数组的第一个元素,所以结果是1-2-3 = -4。\n那么我们看看reduceRight == foldr的结果：\nJavaScript\n1 2 3 4 5 6  const _ = require(\u0026#34;lodash\u0026#34;); //reduceRight 相当于 foldr _.reduceRight([1,2,3],function(sum,n){ return sum-n; }); // 输出 0   我们看到这个结果是``也算是预期结果，因为3-2-1=0。\n 注：上文为了易于理解和行文连贯，加入了一些我自己的理解。需要说明的是，在Haskell中，foldl1函数应该是和JavaScript的reduce(lodash)函数是一致的，foldl1函数将会把列表的第一个元素作为初始值。\n 现在我们总结一下foldr和foldl的一些思路：\n如果对列表[3,4,5,6]应用函数f初始值为z进行foldr的话，应该理解为：\n1 2 3 4 5  f 3 (f 4 (f 5 ( f 6 z))) -- 当 f 为 +, z = 0 上式就变为： 3 + (4 + (5 + (6 + 0))) -- 前缀(+)形式则为： (+)3 ((+)4 ((+)5 ((+)6 0)))   如果对列表[3,4,5,6]应用函数g初始值为z进行foldl的话，应该理解为：\n1 2 3 4 5  g(g (g (g z 3) 4) 5) 6 -- 当然我们也可以类似地把 g 设为 +, z = 0， 上式就变为： (((0 + 3) + 4) + 5) + 6 -- 改成前缀形式 (+)((+)((+)((+)0 3) 4) 5) 6   从上面的例子可以看出，左折叠(foldl)和右折叠(foldr)两者有一个很关键的区别，就是，左折叠无法处理无限列表，但是右折叠可以。\n前面我们说的都是用预定义的函数+,-,*…,（在函数式编程里，这些运算符其实也都是函数）用这些函数是为了能够让我们更加便于理解，现在我们看看用我们自己定义的函数呢？试试逆转一个列表：\nreverse\nHaskell\n1 2  flip\u0026#39; :: (a -\u0026amp;gt; b -\u0026amp;gt; c) -\u0026amp;gt; b -\u0026amp;gt; a -\u0026amp;gt; c flip\u0026#39; f x y= f y x   上面的flip'函数的作用就是传入第一个参数是一个函数，然后将两个参数的顺序调换一下(flip是预定义函数)。\nHasekll\n1  foldr flip\u0026#39; [] [1,2,3]   那么JavaScript的实现呢？\nJavaScript\n1 2 3 4 5 6 7 8 9 10 11 12 13  function flip(f, a, b){ return f(b,a); } //这个函数需要进行柯里化，否则无法在foldr中作为参数传入 var flip_ = _.curry(flip); function cons(a,b){ return a.concat(b); } function reverse(list){ return foldr(flip_(cons),[],list); }   调用结果又是怎么样的呢？\n1 2  console.log(reverse([1,2,3])) // [ 3, 2, 1 ]   好了，现在我们好像又看到了一个新东西——curry，柯里化。简单地说，柯里化就是一个函数可以先接受一部分参数，然后返回一个接受剩下参数的函数。用上面的例子来说，flip函数在被柯里化之后得到的函数flip_，可以先接受第一个参数cons然后返回一个接受两个参数a,b的函数，也就是我们需要的连接函数。\n在go语言里面，实现curry是一个很麻烦的事情，因此go的函数式编程支持还是比较有限的。\n接着我们试试如何取得一个列表的长度，实现一个length函数：\nlength\nHaskell\n1 2 3 4 5 6 7 8  -- 先定义实现一个count 函数 count :: a -\u0026amp;gt; b -\u0026amp;gt;c count a n = n + 1 -- 再实现一个length函数 length\u0026#39; = foldr (count) 0 -- 再调用 length\u0026#39; [1,2,3,4] -- 4   JavaScript\n1 2 3 4 5 6 7 8 9 10 11  //先定义一个count函数 function count(a,n){ return n + 1; } //再实现length函数 function length(list){ return foldr(count,0,list); } //调用 console.log(length([1,2,3,4])); // 4   就是这么简单，好了，reduce我们讲完了，然后我们看看map,要知道map函数是怎么来的，我们要从一个比较简单的函数先入手，这个函数的功能是把整个列表的所有元素乘以2:\ndoubleall\nhaskell\n1 2 3 4 5 6 7 8 9 10 11  -- 定义一个乘以2，并连接的函数 doubleandcons :: a -\u0026amp;gt; [a] -\u0026amp;gt; [a] doubleandcons x y = 2 * x : y doubleall x = foldr doubleandcons [] -- 调用 doubleall [1,2,3] -- 输出 -- [2,4,6]   JavaScript\n1 2 3 4 5 6 7 8 9 10 11  function doubleandcons(a,list){ return [a * 2].concat(list) } function doubleall(list){ return foldr(doubleandcons,[],list) } //调用 console.log(doubleall([1,2,3])); // [2,4,6]   再来看看go怎么写：\ngo\ngo 的尴尬之处在于，需要非常明确的函数定义，所以我们要重新写一个foldr函数，来接受第二个参数为列表的f。\n1 2 3 4 5 6  func foldr2(f func(a int,b []int) []int,z []int,list []int)[]int{ if len(list) == 0{ return z } return f(list[0],foldr2(f,z,list[1:])) }   然后我们再实现同上面相同的逻辑：\n1 2 3 4 5 6 7 8 9  func doubleandcons(n int,list []int) []int{ return append([]int{n * 2},list...) } func doubleall(list []int) []int{ return foldr2(doubleandcons,make([]int,0),list) } // doubleall([]int{1,2,3,4}) //[2 4 6 8]   go这门强类型编译语言虽然支持一定的函数式编程，但是使用起来还是有一定局限性的，起码代码复用上还是不如js的。\n接下来我们关注一下其中的doubleandcons函数，这个函数其实可以转换为这样的一个函数：\n1 2 3 4  fandcons f el [a]= (f el) : [a] double el = el * 2 -- 只传入部分参数，柯里化 doubleandcons = fandcons double   现在我们关注一下这里的fandcons,其实这里可以通用表述为Cons·f,这里的·称为函数组合。而函数组合有这样的操作：\n\u0026nbsp; \u0026nbsp;  需要注意的是等号右边是函数调用。\n 那么上面的我们的函数就可以表述为：\n\u0026nbsp; \u0026nbsp; 所以：\n\u0026nbsp; \u0026nbsp; 最终版本就是：\n\u0026nbsp; \u0026nbsp; 这里的foldr(Cons.double) 其实就是我们要的map double，那么我们的map的本来面目就是：\n\u0026nbsp; \u0026nbsp;  这里的Nil是foldr函数的初始值。\n 好了map已经现身了，让我们再仔细看看一个map函数应该怎么实现：\nmap\nHaskell\n1 2 3 4 5 6 7 8 9  fandcons :: (a-\u0026amp;gt;b) -\u0026amp;gt;a -\u0026amp;gt; [b] -\u0026amp;gt; [b] fandcons f x y= (f x):y map\u0026#39; :: (a-\u0026amp;gt;b) -\u0026amp;gt; [a] -\u0026amp;gt; [b] map\u0026#39; f x = foldr (fandcons f) [] x -- 调用  map\u0026#39; (\\x -\u0026amp;gt; 2 * x) [1,2,3] -- 输出 [2,4,6]   这里用了Haskell的lambda表达式，其实就是f的double实现。\n我们也看看js版本的实现：\nJavaScript\n1 2 3 4 5 6 7 8 9 10 11 12  function fandcons(f, el, list){ return [f(el)].concat(list); } //需要柯里化 var fandcons_ = _.curry(fandcons); function map(f, list){ return foldr(fandcons_(f),[],list); } //调用 console.log(map(function(x){return 2*x},[1,2,3,4])); // 输出[ 2, 4, 6, 8 ]   这些需要柯里化的go我都不实现了，因为go实现柯里化比较复杂。\n最后我们再看看map的一些神奇的操作：\n矩阵求和\nsummatrix\nHaskell\n1 2 3 4 5 6  summatrix :: Num a =\u0026amp;gt; [[a]] -\u0026amp;gt; a summatrix x = sum (map sum x) -- 调用 summatrix [[1,2,3],[4,5,6]] -- 21    这里一定要显式声明 参数a的类型，因为sum函数要求Num类型的参数\n JavaScript\n1 2 3 4 5 6 7 8 9 10  function sum(list){ return foldr(add,0,list); } function summatrix(matrix){ return sum(map(sum,matrix)); } //调用  mat = [[1,2,3],[4,5,6]]; console.log(summatrix(mat)); //输出 21      结语  在学习函数式编程的过程中，我感受到了一种新的思维模式的冲击，仿佛打开了一种全新的世界，没有循环，甚至没有分支，语法简洁优雅。我认为作为一名计算机从业人员都应该去接触一下函数式编程，能够让你的视野更加开阔，能够从另一个角度去思考。\n","date":"2017-08-12","permalink":"https://chenquan.me/posts/functional-programming-by-haskell-js-golang/","tags":["haskell","函数式编程"],"title":"从Haskell、JS、go看函数式编程"},{"categories":null,"contents":"三月的时候立下一个flag, 说是在一个月内学会haskell，现在已经8月了，终于有时间坐下来好好地看看haskell,一直以来我都执着于各种语言，现在已经掌握的语言包括: go, python, php, c, java, javascript, shell, 这些语言在不同的领域都是神兵利器，能够帮我解决不同的问题，但是haskell不一样，是一种我完全凭借兴趣去学习的语言。\n刚开始看趣学指南的时候，觉得这门语言语法太奇怪了，我看得很难受。慢慢发现这其实是一种思维定势，如果我的第一门入门语言是haskell的话，想必就不是这种态度了。 Haskell是一门非常迷人的语言，它的列表推导式真的很厉害，能够解决原来传统过程式语言需要写很多代码才能解决的问题。它给了你另外一种思考问题的方式，开阔视野。\n我觉得既然是计算机从业者，都应该去学学python和haskell两门语言，python将教会你什么如何让世界变美好，Haskell将告诉你这个世界是多么奇妙。如果你现在被冯·诺依曼式的架构侵染太深的话，学学Haskell吧，它会告诉你这世界上还有这样写代码的方式。\n   书籍  最棒的haskell免费入门书 haskell趣学指南\n   视频  下面是两小时入门haskell的内容，视频我也贴在下面了，但直接访问不了，你懂的。\n   讲义  下面是我翻译过的视频随堂讲义内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607  -- Haskell 是一种函数式编程语言 -- 在Haskell 中所有的值都是immutable 的，所以一旦一个变量被赋值之后，它就不会改变了 -- 函数可以作为另一个函数的参数 -- 递归函数在hankell中很普遍 -- Haskell没有for.while，以及典型的变量，但是它拥有常量 -- Haskell试试一种惰性求值的语言，只有在真正需要的时候再进行求值，以及错误检查 -- 最佳的Best free book -- http://learnyouahaskell.com/chapters -- 输入 ghci 到你的terminal中 import Data.List import System.IO -- This is haskell comment single line {- muti-line comment -} -- --------DATA types --------- -- Haskell 能够自己进行类型推定 -- Haskell 是一种静态类型语言，在编译之后无法改变类型 -- 值不可变(immutable) -- 你可以在gchi 中使用 :t 来查看数据类型 -- Int : 所有的数字范围在 -2^63 ~ 2^63 -- :: Int 表示这个类型是一个Int类型的数据 maxInt = maxBound :: Int minInt = minBount :: Int -- Integer 无限制的number类型 -- Float： 单精度浮点数 -- Double: 双精度浮点数 bigFloat = 3.99999999999+0.00000000005 -- Bool： True或者False -- Char: 一个unicode字符，用单引号括起来 -- Tuple: 能够存储多种数据类型的一组数据 (11 pts 精度) -- 你可以用这种方式声明一个常量 always5 :: Int always5 = 5 -- ------MATH----- -- 一些牛逼的操作 -- 求1到100的和 sumofValues = sum[1..1000] addEx = 5 + 4 subEx = 5 - 4 multEx = 5 * 4 divEX = 5 / 4 -- mod 是前缀操作符 modEx = mod 5 4 -- 你可以通过反引号将前缀函数变为中缀形式 modEx2 = 5 `mod` 4 -- 负数必须要用括号括起来 negNumEx = 5 + (-4) -- 如果你定义了一个Int类型的数，你必须要用fromIntegral 函数先处理之后才能够使用sqrt函数进行处理 num9 = 9 :: Int sqrtof9 = sqrt (fromIntegral 9) -- built in 的一些数学函数 piVal = pi ePow9 = exp 9 logOf9 = log 9 squared9 = 9 ** 2 truncateVal = truncate 9.999 roundVal = round 9.999 ceilingVal = ceiling 9.999 floorVal = floor 9.999 -- 当然还有一些基本数学函数： sin cos tan asin acos sinh tanh cosh asinh atanh acosh trueAndFalse = True \u0026amp;\u0026amp; False trueOrFalse = True || False notTrue = not(True) -- 记住你可以用 :t 在ghci得到数据的类型 -- 当然也可以在函数中用:t 判断数据的类型 -- :t(+) = Num =\u0026amp;gt; a -\u0026amp;gt; a -\u0026amp;gt; a -- Type a 是一种 num的类型， 传入两个a 类型的数据，然后得到一个a类型的数据 -- :t truncate = (RealFrac a, Integral b) =\u0026amp;gt; a -\u0026amp;gt; b -- ------ LIST ------ -- List是一个单向链表，只能够在前面添加数据 -- List能够存储一系列相同类型的数据 primeNumbers = [3,5,7,11] -- 在连接两个链表的时候会如果其中一个链表很大会导致连接速度很慢 morePrime = primeNumbers ++ [13,17,19,23,29] -- 你可以使用冒号 : 进行数据和列表连接, 一定要有一个列表，哪怕是空列表 favNums = 2 : 7 : 21 : 66 : [] -- 你可以弄一个列表的列表 multiList [[3,5,7],[11,13,17]] -- 在列表面快速添加一个元素  morePrime2 = 2 : morePrime2 -- 得到列表的长度 length morePrime -- 得到index 为1 的元素 morePrime !! 1 -- 得到第一个元素 head morePrime -- 得到最后一个元素 last morePrime -- 得到除了第一个元素之外的所有元素 tail morePrime -- 得到除了最后一个元素之外的所有元素 init morePrime -- 得到指定个数的列表前几个元素 take 3 morePrime -- 删除前几个元素 drop 2 morePrime -- 判断一个元素是否在列表中 3 `elem` morePrime -- or  elem 3 morePrime -- 列表中的最大值 maximum morePrime -- 列表中的最小值 minimin morePrime -- 列表求和 sum morePrime -- 列表求积 product morePrime -- 列表自动推导 zeroToten = [0..10] -- 步进2列表自动推导 step2list =[2,4..20] -- 字符列表自动推导 letterlist =[\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;] -- 字符列表步进2自动推导 letterlist =[\u0026#39;A\u0026#39;,\u0026#39;C\u0026#39;..\u0026#39;Z\u0026#39;] --无限列表,Haskell只计算你所需要的值 infinPow10 = [10,20..] -- repeat 可以得到一个重复值的列表 many2s = take 10 (repeat 2) -- replicate 能够指定重复次数和值 -- 3 重复10次 many3s = replicate 10 3 -- 循环列表，无限循环 cyclelis = take 10 (cycle [1,2,3,4,5]) -- haskell 较为高级的列表推导式 -- haskell 可以通过列表推导式实现高级条件数据筛选 -- 将1..10 的元素都乘以2然后返回一个列表 listtime2 = [x * 2 | x \u0026amp;lt;- [1..10]] -- 输出 [2,4,6,8,10,12,14,16,18,20] -- 将1..20中 3*x 小于50的剔除，输出剩下的3*x的列表 listtime3 = [x * 3 | x \u0026amp;lt;- [1..20], x * 3 \u0026amp;lt; 50] -- 输出 [3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48] -- 输出在 1..500中所有能够被 13和9整除的数 divisBy9N13 = [x | x \u0026amp;lt;- [1..500],x `mod` 13 == 0, x `mod` 9 == 0] -- 输出[117,234,351,468] -- 列表排序 -- 需要import Data.List sortedList = sort [9,8,1,2,3,4,7,6] -- zipWith 可以合并两个列表 sumOfLists = zipWith (+) [1,2,3,4,5] [6,7,8,9,10] -- 输出[7,9,11,13,15] --数据过滤，通过filter函数,能够保留满足条件的数据 listBiggerThan5 = filter (\u0026amp;gt;5) sumOfLists -- takeWhile 能够取出数据直到条件为false evenUp20 = takeWhile (\u0026amp;lt;=20) [2,4..] -- foldl 从左到右应用运算符 -- foldr 从右到左应用运算符 foldl (*) 1 [1,2,3,4] -- 24 -- ------TUPLES ------ -- Tuple 能够存储多种类型，但是其大小是固定的 randTuple = (1,\u0026#34;Random tuple\u0026#34;) bobsSmith = (\u0026#34;Bob Smith\u0026#34;,52) -- Get the first value bobsAge = fst bobsSmith -- Get the second value bobsAge = snd bobsSmith -- zip函数能够将两个列表压缩为tuples names =[\u0026#34;Bob\u0026#34;,\u0026#34;Mary\u0026#34;,\u0026#34;Tom\u0026#34;] address =[\u0026#34;123 Main\u0026#34;,\u0026#34;234 North\u0026#34;,\u0026#34;567 south\u0026#34;] nameNaddress = zip names address -- 输出 [(\u0026#34;Bob\u0026#34;,\u0026#34;123 Main\u0026#34;),(\u0026#34;Mary\u0026#34;,\u0026#34;234 North\u0026#34;),(\u0026#34;Tom\u0026#34;,\u0026#34;567 south\u0026#34;)] -- ------ FUNCTIONS ------- -- ghc --make haskelltut 将会编译你的程序并执行main函数 -- Functions 必须由小写字母开始 -- 我们能够使用let关键字定义一个函数 -- let num7 = 7 -- let getTriple x = x * 3 -- getTriple num7 = 21 -- main 函数能够在terminal中被调用的main函数 main = do -- 在一行中打印 putStrLn \u0026#34;What\u0026#39;s your name\u0026#34; -- 获取用户的输入并将其存储到name中 -- \u0026amp;lt;- 运算符能从IO操作中取得数据并放入变量中 name \u0026amp;lt;- getLine putStrLn (\u0026#34;hello\u0026#34; ++ name) -- 创建一个名为addMe 的函数 -- x 是一个参数，然后后面是类型签名 -- 传入的类型如果符合要求将会自动应用函数声明 -- 所有的函数都要求有返回值 -- 如果一个函数没有参数则称为一个定义或一个名称 --你可以通过如下的形式定义一个函数： -- funcName :: param1 -\u0026amp;gt; param2 -\u0026amp;gt; returnType addMe :: Int -\u0026amp;gt; Int -\u0026amp;gt; Int -- funcName param1 param2 = operations (Returned Value) -- 执行: addMe 4 5 addMe x y = x + y -- 如果没有类型声明则该函数能够处理浮点数 sumMe x y = x + y -- 当然你也可以定义一个tuple相加函数 addTuples :: (Int,Int) -\u0026amp;gt; (Int,Int) -\u0026amp;gt; (Int,Int) addTuples (x1,y1) (x2,y2) = (x1 + x2, y1 +y2) -- 你可以根据不同的值进行不同的操作 类似于switch case whatAge :: Int -\u0026amp;gt; String whatAge 16 = \u0026#34;You can drive\u0026#34; whatAge 18 = \u0026#34;You can vote\u0026#34; whatAge 21 = \u0026#34;You are a adult\u0026#34; -- default whatAge x = \u0026#34;Nothing Import\u0026#34; -- 定义一个我们期望输入以及输出的函数 factorial :: Int -\u0026amp;gt; Int -- 如果是0 则返回1 （递归函数) factorial 0 = 1 factorial n = factorial(n -1) -- 3 * factorial (2) : 6 -- 2 * factorial (1) : 2 -- 1 * factorial (0) : 1 -- 当然你也可以定义一个乘法的Factorial productFactorial n = product [1..n] -- 我们能够利用竖线来根据不同情况下的返回值 isOdd :: Int -\u0026amp;gt; Bool isOdd n -- if 如果是奇数 | n `mod` 2 == 0 = False -- else | otherwise = True -- 当然这个函数能够精简 isEven n = n `mod` 2 == 0 -- 多个条件的 if else  whatGrade :: Int -\u0026amp;gt; String whatGrade age | (age \u0026amp;gt;= 5) \u0026amp;\u0026amp; (age \u0026amp;lt;= 6) = \u0026#34;Kindergarten\u0026#34; | (age \u0026amp;gt;= 6) \u0026amp;\u0026amp; (age \u0026amp;lt;= 10) = \u0026#34;Elementary school\u0026#34; | (age \u0026amp;gt;= 10) \u0026amp;\u0026amp; (age \u0026amp;lt;= 14) = \u0026#34;Middle school\u0026#34; | (age \u0026amp;gt;= 14) \u0026amp;\u0026amp; (age \u0026amp;lt;= 18) = \u0026#34;High school\u0026#34; | otherwise \u0026#34;Go to college\u0026#34; -- 使用where能够帮我们处理条件 batAvgRating :: Double -\u0026amp;gt; Double -\u0026amp;gt; String batAvgRating hits atBats | avg \u0026amp;lt;= 0.200 = \u0026#34;Terrible Batting Average\u0026#34; | avg \u0026amp;lt;= 0.250 = \u0026#34;Average Player\u0026#34; | avg \u0026amp;lt;= 0.280 = \u0026#34;Your doing pretty good\u0026#34; | otherwise = \u0026#34;You\u0026#39;re a Superstar\u0026#34; where avg = hits / atBats -- 多条件判断不同的list状态 getListItems :: [Int] -\u0026amp;gt; String getListItems [] = \u0026#34;Your list is empty\u0026#34; getListItems (x:[]) = \u0026#34;Your list contains \u0026#34; ++ show x getListItems (x:y:[]) = \u0026#34;Your list contains \u0026#34; ++ show x ++ \u0026#34; and \u0026#34; ++ show y getListItems (x:xs) = \u0026#34;The first item is \u0026#34; ++ show x ++ \u0026#34; and the rest are \u0026#34; ++ show xs -- 我们也能够通过模式来定义一个函数 getFirstItem :: String -\u0026amp;gt; String getFirstItem [] = \u0026#34;Empty String\u0026#34; getFirstItem all@(x:xs) = \u0026#34;The first letter in \u0026#34; ++ all ++ \u0026#34;is\u0026#34; ++ [x] -- ------ 高阶函数 ------- -- 能够将一个函数像一个值一样传入到另一个函数中 times4 :: Int -\u0026amp;gt; Int times4 x = x * 4 -- map 能够将一个列表应用于另一个函数并求值 listTimes4 = map times4 [1,2,3,4,5] -- [4,8,12,16,20] -- 然后我们来定义一个map multBy4 :: [Int] -\u0026amp;gt; [Int] multBy4 [] = [] -- 将一个列表中的某个值取出然后乘以4再存入到另一个列表中 -- 这里的xs 类型是 [Int], 这是一个递归写法  multBy4 (x:xs) = times4 x : multBy4 xs -- 判断两个字符串是否相等 areStringEq :: [Char] -\u0026amp;gt; [Char] -\u0026amp;gt; Bool areStringEq [] [] = True areStringEq (x:xs) (y:ys) = x == y \u0026amp;\u0026amp; areStringEq xs ys areStringEq _ _ = False -- 将一个函数作为另一个函数的参数 -- (Int -\u0026amp;gt; Int) 代表我们需要一个参数是Int 返回值是Int的函数作为参数传入 doMult :: (Int -\u0026amp;gt; Int) -\u0026amp;gt; Int doMult func = func 3 num3Time4 = doMult times4 -- 返回一个函数 getAddFunc :: Int -\u0026amp;gt; (Int -\u0026amp;gt; Int) -- 我们能够值传入 getAddFunc x y = x + y -- 我们也能得到一个+3的函数 adds3 = getAddFunc 3 fourPlus3 = adds3 4 -- 我们也能够将这个函数用到map中 threePlusList = map adds3 [1,2,3,4,5] -- ------ LAMBDA ------ -- 创建一个匿名函数 也称为lambda 用 \\ 开始表示这是一个lambda (\\arguments -\u0026amp;gt; result) db1To10 map (\\x -\u0026amp;gt; x * 2) [1..10] -- ------- 条件 ------ -- 所有的if 都必须包含else doubleEvenNumber y = if (y `mod` 2 /= 0) then y else y * 2 -- 我们能够利用case表达式 getClass :: Int -\u0026amp;gt; String getClass n = case n of 5 -\u0026amp;gt; \u0026#34;Go to kindergarten\u0026#34; 6 -\u0026amp;gt; \u0026#34;Go to elementary school\u0026#34; _ -\u0026amp;gt; \u0026#34;Go to some place else\u0026#34; -- ------ MODULES ------- -- 我们能够将一组函数组织起来，集合成一个module，通过import能加载一个模块 -- 那么如何创建一个模块呢？ -- 1. 创建一个文件 -- 2. 将所有需要使用的函数包含进去 -- TODO -- 3. 在文件的顶部将所有需要导出的函数列出 -- ------ 枚举 ------ data BaseallPlayer = Pitcher | Catcher | Infield | Outfield deriving Show barryBonds :: BaseallPlayer -\u0026amp;gt; Bool barryBonds Outfield = True barryInOf print(barryBonds Outfield) -- ------ 自定义类型 ------ data Customer = Customer String String Double deriving Show --定义自定义类型的变量 tomSmith :: Customer tomSmith = Customer \u0026#34;Tom Smith\u0026#34; \u0026#34;123 Main st\u0026#34; 20.50 -- 定义一个需要使用该自定义类型的函数 getBalance :: Customer -\u0026amp;gt; Double getBalance (Customer _ _ b) = b tomSmithBal print(getBalance tomSmith) data RPS = Rock | Paper | Scissors shoot :: RPS -\u0026amp;gt; RPS -\u0026amp;gt; String shoot Paper Rock = \u0026#34;Paper Beats Rock\u0026#34; shoot Rock Scissors= \u0026#34;Rock Beats Scissors\u0026#34; shoot Scissors Paper= \u0026#34;Rock Beats Scissors\u0026#34; shoot Scissors Rocks= \u0026#34;Scissors Lose to Rocks\u0026#34; shoot Paper Scissors= \u0026#34;Paper Lose to Scissors\u0026#34; shoot Rock Paper= \u0026#34;Rock Loses to Paper\u0026#34; shoot _ _= \u0026#34;Error\u0026#34; data Shape = Circle Float Float Float | Rectangle Float Float Float Float deriving (Show) -- :t Circle = Float -\u0026amp;gt; Float -\u0026amp;gt; Float -\u0026amp;gt; Float area :: Shape -\u0026amp;gt; Float area (Circle _ _ r) = pi * r ^ 2 area (Rectangle x y x2 y2) = (abs (x2 - x)) * (abs (y2 - y)) -- 使用 $ 符号可以把$后面的表达式作为一个参数传入到前面的函数中 -- 使用 . 可以作为一个管道符号，将后面一个函数的输出作为前一个函数的输入 putStrLn . show $ 1 + 2 -- Get area of shapes  areaOfCircle = area (Circle 50 60 20) areaOfRectangle = area $ Rectangle 10 10 100 100 -- ------ Type Class ------- -- Num, Eq 和 Show 是类型类 -- Type Class相当于面向对象的接口 -- 多态函数（具有多种参数，多种数据类型，通常用类型定义 -- 例如， + 运算符就是用Num 类型类定义的 -- :t (+) :: Num a =\u0026amp;gt; a -\u0026amp;gt; a -\u0026amp;gt; a -- 这代表 a 是Num的一个实例，+ 能够处理两个a类型的数然后返回一个类型为Num的数 --自定义一个自定义类型类，并让其能够进行比较 data Emplyee = Emplyee { name :: String, position :: String, idNum :: Int, } deriving (Eq,Show) samSmith = Emplyee{name = \u0026#34;Sam Smith\u0026#34;,position = \u0026#34;Manager\u0026#34;,IdNum = 1000} pamMax = Emplyee{name = \u0026#34;Pam Max\u0026#34;,position = \u0026#34;Sales\u0026#34;,IdNum = 1001} isSamPam = smaSmith == pamMax samSimithData = show smaSmith -- 实现一个自己实现的Eq 和Show类型类的数据类型 data ShirtSize = S | M | L instance Eq ShirtSize where S == S = True M == M = True L == L = True _ == _ = False instance Show ShirtSize where show S = \u0026#34;Small\u0026#34; show M = \u0026#34;Medium\u0026#34; show L = \u0026#34;Large\u0026#34; -- Check if S is in the list -- 需要实现Eq才能够用elem 方法 smallAvail = S `elem` [S, M, L] -- Get string value for ShirtSize -- 需要实现Show 类型类 theSize = show S -- 自定义类型类 class MyEq a where areEqual :: a -\u0026amp;gt; a -\u0026amp;gt; Bool -- 实现自己定义的类型类 instance MyEq ShirtSize where areEqual S S = True areEqual M M = True areEqual L L = True areEqual _ _ = Fasle newSize = areEqual M M -- ------ I/O ------ sayHello = do putStrLn \u0026#34;What\u0026#39;s your name: \u0026#34; name \u0026amp;lt;- getLine putStrLn $ \u0026#34;Hello\u0026#34; ++ name -- File I/O writeToFile = do theFile \u0026amp;lt;- openFile \u0026#34;test.txt\u0026#34; WriteMode hPutStrLn theFile (\u0026#34;Random line of text\u0026#34;) hClose theFile readFromFile = do theFile2 \u0026amp;lt;- openFile \u0026#34;test.txt\u0026#34; ReadMode contents \u0026amp;lt;- hGetContents theFile2 putStr contents hClose theFile2 -- ------ EXAMPLE: FIBONACCI SEQUENCE ------ -- 1,1,2,3,5,8, ... -- 1 : 1 : 代表函数的开始 -- | 表示对于每个 a,b 并将其相加 -- \u0026amp;lt;- 存储 a + b 之和 -- tail : 得到除了第一个值之外的所有值 -- zip 将两个列表压缩，并得到一个tuple 列表 fib = 1 : 1: [a + b | (a , b) \u0026amp;lt;- zip fib (tail fib)] -- 对上式进行推演 -- 1. 首先 fib = 1 然后tail fib = 1 -- 2. 现在列表变成了 [1,1,2] 因为 a:1 + b:1 = 2 -- 3. 然后第二步, fib = 1 然后 (tail fib) = 2 -- 4. 现在列表变成了 [1,1,2,3] 因为 a: 1 + b: 2 = 3   ","date":"2017-08-09","permalink":"https://chenquan.me/posts/haskell-the-language-you-should-learn/","tags":["haskell","函数式编程"],"title":"Haskell — 你应该学的一门语言"},{"categories":["影评"],"contents":"   《全民目击》的英文片名更贴切     背景  抽了下午的两个小时看完了朋友强烈推荐的《全民目击》英文名《Silent Witness》。全片看下来可以说是导演已经创意非凡了，通过控辨双方的不同视角互换得到的迥然不同的剧情发展着实是我第一次看到，不禁拍案。看这个电影的时候我有一种意识想到了前段时间也挺火的《嫌疑人X的献身》，又是一部温情献身剧情，最后父爱的升华，讲真剧情设计还是有可圈可点之处的。比某些国产全剧都是尿点的电影高得不知道哪里去了。\n   评价  说说自己对各个演员的看法吧：\n郭富城：其实我看《寒战》系列的时候就觉得他挺适合演这种片子，但是有他在的电影总有一股浓浓的香港警匪片的气息，不知道是不是中毒太深，在庭审的时候一些愤怒和焦灼的情绪表现得很到位，但是在与片中的老婆（因为有戒指）沟通的时候觉得有些僵硬，比如他们坐在一起吃饭的时候，莫名其妙的争吵，莫名其妙的结束，不是很自然，有点牵强。\n\n孙红雷： 两个字，很棒。把一个老谋深算的奸商表现得很好，很切合剧中角色的性格，不得不说，孙红雷的演技我是一直觉得很棒的。\n他可以把一些细节表现得很好，比如让自己的女儿不要哭。\n\n亦或是在回答庭审问题时候的淡定自若的表情，都表现的很符合当时的情景。\n\n不得不说，孙红雷在片中饰演的林泰确实是一个很饱满的角色。\n余男：再说说余男，演一个女强人的确挺适合她，她的造型能够体现出一种倔强，一种骄傲的感觉出来，但我个人觉得这个角色演的有点过于男性化。\n特别是庭审的时候，她的表情很淡定，也很冷漠，我不知道为什么导演要选择她来演，可能和后面剧情的变化，以及一个律师的内心的变化比较贴合，毕竟女人内心更偏向于柔软。一个女强人形象后面有温柔的一面也符合逻辑。\n可是我要说的是，她全剧毫无亮点啊，谁演都可以啊。\n邓家佳：一直给我的印象就是爱情公寓的小姨妈，其实可以说演技还算可以，和一线影星还是有点差距，比如说在剧中的爸爸说开车撞人的是自己的时候，她的表现只能说是还行吧，有点僵，但是那个眼泪流的我觉得很不错。\n\n\n赵立新： 这个演员之前没有听说过，这部剧里其实表现得很好。把丰富的情感变化表现的很充足，从刚刚开始的淡定，到被律师激怒之后的失控，再到失误说出自己的秘密之后的恐惧与慌张，不得不说，演的还是不错的，但是不知道为什么后面在警察局里配审讯的时候就一言不发了，这就很尴尬，没什么表现了啊。\n\n主要角色都已经提及，下面说说一些我想说的人：\n谭松韵：知道谭松韵我可以说是一段故事了，但是这不是重点。重点是，谭松韵的造型我觉得很是诡异啊。\n这造型估计也没sei了，我觉得她还是比较适合清纯造型，毕竟短发配她的脸型，再加上她的眼妆，简直不要太不和谐好不好。\n其实作为一部剧情片，我觉得剧本已经写得很不错了，毕竟能够做到缓缓相扣，出人意料又在清理之中，能够把整个主线故事说圆了，如果你不去细细揣摩，应该是问题不大的，确实是大部分无尿点，除了结尾的一点鸡肋剧情之外。细细推敲这部剧还是有些bug的：\n 在开头的导播穿插播放不知道是和用意，是否是为了艺术效果。 在证人等候室的那个女的为什么要问那么多问题，问了之后就没有然后了，是不是为了做什么铺垫，但是后面再没有出现那个女的的戏份了，这就有点尴尬了。 郭富城的“老婆”的出现是为了渲染情感吗？如果是的话，好像也并没有起到很好的作用，为了塑造一个郭富城铁面无私的形象？ 在监控录像中，为什么看不到林泰下车的画面，感觉说不通。 林家司机作为一名15年的老司机，我觉得完全可以认出车的型号和来源，如果是杨丹（剧中受害者）受害，稍加一想完全可以知道这是自家人，如果按剧中逻辑，报警可能性几乎不存在。 鸡肋的结尾不知道是为了干什么，多了又觉得没什么卵用，去掉么又觉得好像也不太好。     总结  总体而言，这部电影是值豆瓣的7.6分的，特殊的叙事手法，挺好的剧情，以及孙红雷和郭富城的搭档都值得你花两个小时看完这部电影。而且这部13年的电影在各大视频网站上都是免费的，推荐大家去看看。然后我想说的是，这个和“全民目击”也没有半毛钱关系吧，反而我觉得“Slient Witness”反而更适合这部电影。\np-content/uploads/2017/07/tsy.jpg\n","date":"2017-07-30","permalink":"https://chenquan.me/archives/184/","tags":["影评"],"title":"《全民目击》的英文片名更贴切"},{"categories":null,"contents":"   mastering sed   我承认这是标题党，但是sed在平时工作中sed真的很有用，特别是像我这种需要写一些shell进行自动化工作的。\n    sed 快速入门  sed 作为流编辑神器，在写脚本的时候往往会发挥奇效，所以这里我将sed的一些关键用法介绍一下，目的在于实用，而不是全面。\n   \u0026lt;code\u0026gt;-i\u0026lt;/code\u0026gt; 参数  常常会看到会有sed -i这样的命令，-i的意思就是直接在源文件上进行操作。\n   动作  sed的动作是我们需要掌握的最为重要的内容 ：\n– a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行);\n– c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行;\n– d ：删除， d 后面通常不接任何字符;\n– i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；\n– p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行;\n– s ：替换，可以直接进行取代的工作通常这个 s 的动作可以搭配正则语法,语法很像vim的替换。\n   样例  测试文件内容：\n1 2 3 4 5 6 7 8  # cat song.txt my name is zhangshan his name is lisi my dog name is doglas today is good day we sang a song: lalalalala hahahahaha      \u0026lt;code\u0026gt;s\u0026lt;/code\u0026gt; 替换  sed \u0026quot;s/zhangshan/wangnima/g\u0026quot; song.txt   替换的方式和vim很像\n 输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;s/zhangshan/wangnima/g\u0026#34; song.txt my name is wangnima his name is lisi my dog name is doglas today is good day we sang a song: lalalalala hahahahaha   这样只会将修改之后的文件输出到标准输出，如果需要修改原来的文件的话，加上-i参数：\nsed -i \u0026quot;s/zhangshan/wangnima/g\u0026quot; song.txt  修改就会在源文件上生效\n用正则武装sed\n正则表达式是一件神兵利器，这里介绍一些较为简单的正则表达式：\n– ^ 表示一行的开头，如：/^my/ 这就是以单词my为开头的匹配\n– $ 表示一行的结尾，如: /:$/ 就是以字符:为结尾的匹配\n– \\\u0026lt; 表示一个词的开头，如\\\u0026lt;s 就是以s开头的词\n– \\\u0026gt; 表示一个词的结尾，如g\\\u0026gt; 就是以g为结尾的词\n– . 表示任意单个字符\n– * 表示一个字符可以出现0次或者多次\n– ? 表示一个字符可以出现1次或者多次\n– [] 集合，例如[abc]可以匹配a,或b,或c，经典例子：[a-zA-Z]匹配所有26个字母,^表示取反，如[^a]表示非a的字符。\n只替换第一行的内容\nsed \u0026quot;1s/name/nume/g\u0026quot;  输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;1s/name/nume/g\u0026#34; song.txt my nume is wangnima his name is lisi my dog name is doglas today is good day we sang a song: lalalalala hahahahaha   指定行号范围\nsed \u0026quot;1,2s/name/nume/g\u0026quot; song.txt  输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;1,2s/name/nume/g\u0026#34; song.txt my nume is wangnima his nume is lisi my dog name is doglas today is good day we sang a song: lalalalala hahahahaha   只替换每行第一个s\nsed \u0026quot;s/s/S/1\u0026quot; song.txt  输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;s/s/S/1\u0026#34; song.txt my name iS wangnima hiS name is lisi my dog name iS doglas today iS good day we Sang a song: lalalalala hahahahaha   只替换每行第2个s\nsed \u0026quot;s/s/S/2\u0026quot; song.txt  输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;s/s/S/2\u0026#34; song.txt my name is wangnima his name iS lisi my dog name is doglaS today is good day we sang a Song: lalalalala hahahahaha   只替换每行第2个之后的s\nsed \u0026quot;s/s/S/2g\u0026quot; song.txt  输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;s/s/S/2g\u0026#34; song.txt my name is wangnima his name iS liSi my dog name is doglaS today is good day we sang a Song: lalalalala hahahahaha   多次匹配\n将第一行的”wangnima”改成”zhangshan”然后把第二行的lisi改成”wangwu”\n1  sed \u0026#34;1s/wangnima/zhangshan/g; 2s/lisi/wangwu/\u0026#34; song.txt   输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;1s/wangnima/zhangshan/g; 2s/lisi/wangwu/\u0026#34; song.txt my name is zhangshan his name is wangwu my dog name is doglas today is good day we sang a song: lalalalala hahahahaha   等价命令\n1  sed -e \u0026#34;1s/wangnima/zhangshan/g\u0026#34; -e \u0026#34;2s/lisi/wangwu/\u0026#34; song.txt   `\u0026amp;` 的妙用\n可以用\u0026amp;表示被匹配到的字符，我们可以用这个trick 做一些事情：\n1  sed \u0026#34;s/name/@\u0026amp;@/g\u0026#34; song.txt   输出：\n1 2 3 4 5 6 7  my @name@ is wangnima his @name@ is lisi my dog @name@ is doglas today is good day we sang a song: lalalalala hahahahaha   圆括号匹配\n在正则表达式中，圆括号括起来的正则部分可以作为变量使用，sed中用\\1,\\2…表示匹配到的值：\n1  sed \u0026#34;s/\\([a-z]*\\) name is \\([a-z]*\\)/\\1:\\2/g\u0026#34; song.txt   输出\n1 2 3 4 5 6 7 8  # sed \u0026#34;s/\\([a-z]*\\) name is \\([a-z]*\\)/\\1:\\2/g\u0026#34; song.txt my:wangnima his:lisi my dog:doglas today is good day we sang a song: lalalalala hahahahaha   ","date":"2017-07-05","permalink":"https://chenquan.me/posts/mastering-in-diving/","tags":["sed"],"title":"精通 sed (mastering sed)"},{"categories":null,"contents":"","date":"2017-07-02","permalink":"https://chenquan.me/posts/traveling-in-wuzhen/","tags":["乌镇"],"title":"小船浮绿水——乌镇一日游"},{"categories":["影评"],"contents":" 《明日的我与昨日的你约会 ぼくは明日、昨日のきみとデートする (2016)》影评\n \n上次看这种很纯的爱情片应该是看得午夜场的《君の名は》了，因为之前已经提前看了抢先版，你的名字带给我的感触有点淡化。这部电影真的很催泪，我虽然泪腺不敏感，可是我突发有种很想写点什么的冲动。\n\n我为什么要用“今天过后，就是最熟悉的陌生人”作为标题呢，这不是一部虐心的电影，甚至很甜。整部电影从男女主美丽的邂逅，到相识相知相爱。几乎就是一段完美感情的标准范本。可是，巧妙的剧本把原本非常美好的事情，变得十分虐心，男主过的每一天，都是女主的昨天。所以，男主的每一天，对女主都意味着是过去的一天。男主的第一次牵手，是女主的最后一次牵手，男主的第一次告白，是女主经历的最后一次被告白。甜是最腻的甜，想到这一层，往往心中就有一丝丝遗憾，所谓悲剧，就是要这种令人艳羡的美好所衬托，才是最虐。\n小松菜奈饰演的女主福寿爱美，在剧中真的演的很棒，在最美的年龄，最美的微笑，真的很吸引观众。非常成功，作为一部纯爱电影，朴素但是真实的表现，几乎让我觉得自己仿佛还在那个年龄，还在期待那种恋情的错觉。着实代入感很强。\n\n这就是我认为的一部青春爱情片所应该有的东西，最纯的爱情才是所谓的青春，最疯的爱情都是冲动。我实在是欣赏不来国产青春爱情片不离叛逆堕胎的套路。如果非要给自己的青春和爱情留点温存，这样的感觉才是合适的。\n\n我的明天是你的昨天，两个相逆的时间线，让所有的美好都带了一些遗憾和伤感，所有的美好，对对方而言都是没有经历过的未来，爱美每天都要假装经历过那些，她肯定是痛苦的，但是她又是快乐的。他们俩的爱情，从另一个角度讲。似乎在不断地重演，双方的时间线在不断地循环，变成了一个环。我想，其实这也是这个剧本的巧妙之处，也许在未来，双方没有办法在一起，但在最美好的时候，你们相遇了，相爱了，可以按着既定的脚本去爱，每天都有新的感悟，我相信笑出来的那一刻，内心真的是幸福的。\n\n今天过后，我就只能看着你把亲昵的称呼改掉，从一个最熟悉，最深爱的人慢慢变成一个陌生人，这是最虐的地方，这也是，这部电影最感人的地方。\n\n当南山高寿初次和福寿爱美见面的时候，问道：“还能见面吗？”，福寿爱美，想到，明天对自己来说，同南山高寿就是不同时间线的陌生人了，内心痛苦挣扎。可是，她还是微笑地回答道：”当然可以，明天见！”。对于南山高寿而言，明天，是非常美好的一天啊。真是又甜蜜又虐心。\n这部电影，我真的很喜欢。10分。\n","date":"2017-06-25","permalink":"https://chenquan.me/archives/138/","tags":["影评"],"title":"今天过后，就是最熟悉的陌生人"},{"categories":null,"contents":"mock对象就是为了解决上面的问题而诞生的，mock(模拟)对象能够模拟实际依赖对象的功能，同时又不需要非常复杂的准备工作，你需要做的，仅仅就是定义对象接口，然后实现它，再交给测试对象去使用。\ngo-mock是专门为go语言开发的mock库，该库使用方式简单，支持自动生成代码，可以说是不可多得的好工具。下面我就简单地展示一下go-mock是如何工作的:\n首先你需要做的是将依赖下载到本地：\n1 2  go get github.com/golang/mock/gomock go get github.com/golang/mock/mockgen   第一个是代码依赖，第二个是命令行工具（特别好用）。\n下面用一个非常简单的例子来说明gomock是如何工作的：\n我在$GOPATH/src目录下新建一个项目：hellomock，在$GOPATH/src/hellomock目录下新建hellomock.go，并定义一个接口Talker:\n1 2 3 4 5  package hellomock type Talker interface { SayHello(word string)(response string) }   然后我们需要一个实现了Talker功能的结构体，假设我们有这样的场景，我们现在有一个迎宾的岗位，需要一个能够迎宾的迎宾员，当然这个迎宾员可以是一个人，或者是一只鹦鹉。那么我们需要做的是，定义一个Persion结构体（或者是鹦鹉或者是别的什么东西），并实现Talker接口：\nperson.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  package hellomock import \u0026#34;fmt\u0026#34; type Person struct{ name string } func NewPerson(name string)*Person{ return \u0026amp;Person{ name:name, } } func (p *Person)SayHello(name string)(word string) { return fmt.Sprintf(\u0026#34;Hello %s, welcome come to our company, my name is %s\u0026#34;,name,p.name) }   现在我们的Person已经实现了Talker接口，现在我们让他发挥作用吧！\n现在假设，我们有一个公司，公司有一个迎宾员，也就是我们的前台妹子，这个妹子实现了Talker接口.她能够自动向来的客人SayHello:\ncompany.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  package hellomock type Company struct { Usher Talker } func NewCompany(t Talker) *Company{ return \u0026amp;Company{ Usher:t, } } func ( c *Company) Meeting(gusetName string)string{ return c.Usher.SayHello(gusetName) }   我们的场景已经设计好了，那么我们传统的话，会如何实现单元测试呢？\ncompany_test.go\n1 2 3 4 5 6 7 8 9  package hellomock import \u0026#34;testing\u0026#34; func TestCompany_Meeting(t *testing.T) { person := NewPerson(\u0026#34;王尼美\u0026#34;) company := NewCompany(person) t.Log(company.Meeting(\u0026#34;王尼玛\u0026#34;)) }   测试之：\n1 2 3 4 5  /usr/local/go/bin/go test -v hellomock -run ^TestCompany_Meeting$ company_test.go:8: Hello 王尼玛, welcome come to our company, my name is 王尼美 ok hellomock 0.013s   现在我们构造一个王尼美还是很简单的，但是我们现在要用mock对象进行模拟,这时mockgen就登场了：\n1 2  ➜ hellomock\u0026amp;gt; mkdir mock ➜ hellomock\u0026amp;gt; mockgen -source=hellomock.go \u0026amp;gt; mock/mock_Talker.go   这个时候，将会生成mock/mock_Talker.go文件：\n需要注意的是，自动生成的文件同源文件在不同的包下，需要新建一个目录存放\n我们并不需要在意生成文件的内容，我们只需要知道如何去使用即可\nmock_Talker.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  // Automatically generated by MockGen. DO NOT EDIT! // Source: hellomock.go  package mock_hellomock import ( gomock \u0026#34;github.com/golang/mock/gomock\u0026#34; ) // MockTalker is a mock of Talker interface type MockTalker struct { ctrl *gomock.Controller recorder *MockTalkerMockRecorder } // MockTalkerMockRecorder is the mock recorder for MockTalker type MockTalkerMockRecorder struct { mock *MockTalker } // NewMockTalker creates a new mock instance func NewMockTalker(ctrl *gomock.Controller) *MockTalker { mock := \u0026amp;MockTalker{ctrl: ctrl} mock.recorder = \u0026amp;MockTalkerMockRecorder{mock} return mock } // EXPECT returns an object that allows the caller to indicate expected use func (_m *MockTalker) EXPECT() *MockTalkerMockRecorder { return _m.recorder } // SayHello mocks base method func (_m *MockTalker) SayHello(name string) string { ret := _m.ctrl.Call(_m, \u0026#34;SayHello\u0026#34;, name) ret0, _ := ret[0].(string) return ret0 } // SayHello indicates an expected call of SayHello func (_mr *MockTalkerMockRecorder) SayHello(arg0 interface{}) *gomock.Call { return _mr.mock.ctrl.RecordCall(_mr.mock, \u0026#34;SayHello\u0026#34;, arg0) }   接下来看看如何去使用这个mock对象，新建一个单元测试：\n1 2 3 4 5 6 7 8 9  func TestCompany_Meeting2(t *testing.T) { ctl := gomock.NewController(t) mock_talker := mock_hellomock.NewMockTalker(ctl) mock_talker.EXPECT().SayHello(gomock.Eq(\u0026#34;王尼玛\u0026#34;)).Return(\u0026#34;这是自定义的返回值，可以是任意类型。\u0026#34;) company := NewCompany(mock_talker) t.Log(company.Meeting(\u0026#34;王尼玛\u0026#34;)) //t.Log(company.Meeting(\u0026#34;张全蛋\u0026#34;)) }   测试之：\n1 2 3  /usr/local/go/bin/go test -v hellomock -run ^TestCompany_Meeting2$ company_test.go:21: 这是自定义的返回值，可以是任意类型。 ok hellomock 0.015s   可以看到，返回的是我们在mock对象上定义的返回值。\n需要说明的一点是，mock对象的SayHello可以接受的参数有gomock.Eq(x interface{})和gomock.Any()，前一个要求测试用例入餐严格符合x，第二个允许传入任意参数。比如我们在注释掉的测试中传入了”张全蛋”，结果报错，测试失败：\n1 2 3 4  /usr/local/go/bin/go test -v hellomock -run ^TestCompany_Meeting2$ controller.go:113: no matching expected call: *mock_hellomock.MockTalker.SayHello([张全蛋]) exit status 1 FAIL hellomock 0.007s   本文作为抛砖引玉，gomock还有很多高级用法，希望大家能够自行探索。\n 参考文章：\nhttps://github.com/golang/mock/blob/master/README.md\nhttps://github.com/grpc/grpc-go/blob/master/Documentation/gomock-example.md\n 转载请注明出处\n","date":"2017-06-22","permalink":"https://chenquan.me/posts/use-gomock-to-mock-testing/","tags":["golang","单元测试"],"title":"用gomock进行mock测试"},{"categories":null,"contents":"椭圆曲线加密：安全性以及RSA对比  原标题：Elliptic Curve Cryptography: breaking security and a comparison with RSA 这篇文章是ECC：优雅入门系列的第四篇文章，我们已经在前面的文章中知道了一个椭圆曲线上的离散对数离散对数问题是如何在保障加密安全的过程中扮演重要角色的。但是，如果你还记得的话，我们曾经提到说目前还没有严格的数学证明来证明计算离散对数的复杂性:我们相信这件事情是“困难”的，但是我们并不确定。在这篇文章的开篇，我们将尝试去解释在目前的技术能力下，完成这件事情是多么地“困难”。 然后，在这篇文章的第二部分，我们将尝试去回答这样的问题，为什么我们需要椭圆曲线？RSA（或者其他基于模运算的加密系统）不是也挺好的吗？ 攻破离散对数问题 我们将会在下文中看到两个最为有效的用来计算椭圆曲线上的离散对数的算法：baby-step,giant-step算法，以及Pollard's方法。 在开始之前，我还是简单提醒一下，所谓的离散对数问题就是给定两个点P和Q，找到这样的证书x使得满足式子 $Q = xP$。其中的点是椭圆曲线子群中的点，这个椭圆曲线应该拥有基点G和阶n。\nBaby-step,giant-step 在描述这个算法的细节之前，我们先快速地看一下这个问题：我们始终能够写出这样的整数x使得$x = am+b$，其中a,m,b是三个任意整数。举例来说，我们可以写出 $10 = 2\\cdot 3+4$。 根据上面的规则，我们可以给出如下的等式来描述离散对数问题：\n$$Q = xP \\ Q = (am +b)P \\ Q = amP + bP \\ Q - amp = bP$$\n“baby-step,giant-step”其实是一个二分算法。不同于暴力破解( brute-force attack)(通过枚举所有有可能的点xP得到我们需要的Q)，我们可以\u0026quot;稍微\u0026quot;地减少计算量，通过计算bP和Q-amP直到我们找到相似值。这个算法的具体步骤如下：\n 计算 $m = \\lceil \\sqrt{n} \\rceil$ 对于所有的$b$属于$0,\u0026hellip;,m$计算$bP$并将结果存储到hash表中. 对于所有的$a$属于$0,\u0026hellip;,m$: 计算$amP$ 计算$Q - amP$ 检查这个hash表并观察是否存在这样的点使得$Q - amP = bP$ 如果这样的点存在，那么我们就找到了这样的x 满足$x = am +b$  就像你看到的一样，我们计算点$bP$的时候步进非常小，就像baby走路一样，系数b慢慢增加($1P,2P,3P,\u0026hellip;$)。然后，在这个算法的第二部分，我们计算$amP$的时候步进又很大，因为系数am的变化规律是($1mP,2mP,3mP,\u0026hellip;$，其中m是一个大整数)。  baby-step,giant-step算法：通过小步进初始化一些点在hashmap中进行存储，然后我们计算大步进并比较新的点是否在hashmap中能够被找到，计算离散对数相当于是一个重新排序的事情。 为了能够理解这个算法是怎么工作的，首先我们先忘记点$bP$是如何缓存的，而是考虑等式 $Q = amP + bP$。看看如下的步骤： - 当$a=0$的时候我们看看$Q$是否等于$bP$，其中$b$是$0$到$m$之间的某个整数。那么在实际上我们就是在比较$Q$和$0P$到$mP$之间是否相等。 - 当$a = 1$ 时，我们检查$Q$是否等于$mP + bP$。其中这里的$Q$就相当于同 $mP$到$2mP$之间的值比较。 - 当$a=2$我们需要比较$Q$和$2mP$和$3mP$之间的点。 - ... - 当 $a = m-1$,我们其实在比较Q同$(n-1)mP$到$m^2P = nP$之间的点。 所以终上所述，我们其实是在检查所有的从$0P$变化到$nP$的点。（这么多就足够了，这已经是所有的可能情况了）这需要我们计算至多$2m$次加法和乘法（包括baby-step中的m和giant-step中的）。 当然我们知道在hash表中找一个元素需要花费$O(1)$的复杂度，所以我们可以轻易地知道，这个算法需要花费的时间和空间复杂度都是$O(\\sqrt{n})$(或者是$O(2^{\\frac{k}{2}})$)，虽然这还是需要很多时间，但是比穷举是好多了。 Baby-step,giant-step实战 为了能够说明$O(\\sqrt{n})$是有意义的，我们以标准曲线为例子:prime192v1(也叫做secp192r1,ansiX9p192r1),这个曲线拥有阶数n = 0xffffffff ffffffff ffffffff 99def836 146bc9b1 b4d22831。这个数的开根号大约是$7.922816251426434 \\cdot 10^{28}$。 想象一下为了在hash表中存储$\\sqrt{n}$个点，假设每个点需要32byte,我们的hash表大约需要$2.5\\cdot 10^{30}$byte的内存，网上查了一下，将全世界的存储能力加起来以zettabyte（$10^{21} byte$）为单位来衡量，我们的hash表需要的存储空间这里还是大约相差10倍左右。就算我们的一个点只需要存储1 byte，还是无法存储这些点。 这非常的令人惊讶，但是更加令人惊讶的是，曲线prime192v1几乎是所有曲线中阶数最小的了。secp521r1(另一个NIST中规定的标准曲线)阶数大约是$6.9 \\cdot 10^{156}$。\n亲自动手实践baby-step，giant-step 安装惯例，我写了一个python程序利用baby-step,giant step算法计算离散对数。明显这个脚本只对低阶的椭圆曲线有效，如果你想用曲线secp521r1那你就等着收到一个MemoryError吧。 脚本的输出结果应该如下: Curve: y^2 = (x^3 + 1x - 1) mod 10177 Curve order: 10331 p = (0x1, 0x1) q = (0x1a28, 0x8fb) 325 * p = q log(p, q) = 325 Took 105 steps  Pollard's $\\rho$ Pollard's $\\rho$是另一个计算离散对数的算法。这个算法和babe-step,giant-step算法有相同的复杂度$O(\\sqrt{n})$，但是它的空间复杂度只有$O(1)$.如果baby-step,giant-step算法因为内存空间限制无法解决的问题，Pollard算法是否能解决呢？ 在Pollard算法中，我们将会去解决稍稍不同的问题，给定$P$和$Q$，找到这样的整数$a$，$b$，$A$和$B$满足$aP+bQ = AP + BQ$。 一旦这四个整数都被找到了，我们可以用等式$Q = xP$来找到这个x: $$aP +bQ = AP + BQ \\ aP+bxP = AP + BxP \\ (a + bx)P = (A+Bx)P \\ (a - A)P = (B - b)xP$$ 限制我们可以得到$P$的值，但是在计算它的值之前，我们需要重新说明一下，我们的子群是循环的并且具有阶n因此，在计算的时候需要加上模n作为系数： $$a -A \\equiv (B-b)x (\\mod n) \\ x = (a-A)(B - b)^{-1} \\mod n$$ Pollard\u0026rsquo;s $\\rho$算法的理论其实很简单，我们定义一个(a,b)的伪随机序列，这个序列产生的值可以用于产生点$aP + bQ$，因为$P$和$Q$都是同一个循环子群中的元素，所以产生的一系列点$aP+bQ$也是循环的。\n上面的说明的意思就是只要我们遍历我们的伪随机序列$(a,b)$，早晚我们就会发现这是一个换，所以我们可以肯定一定存在这样的值对$(A，B)$使得等式$aP+bQ = AP+BQ$。 不同的值对代表同一个点，这点就在我们计算对数的时候起到作用。 问题是：我们如何高效地得到这样的环？\nTortoise and Hare(龟兔赛跑) 为了能够找出我们的换，我们可以尝试所的a和b的可能值，通过pairing funciton,但是这里却有$n^2$个可能的值对，那么我们的算法的复杂度就会是 $O(n^2)$，比暴力穷举的复杂度还搞。 但是这里还是有稍微快点的算法的：龟兔算法（tortoise and hare algorithm）也叫Floyd's cycle-finding algorithm。下面的图片展示了这个算法是怎么工作的，这就是Pollard's rho算法的核心部分了。  我们现在用曲线$y^2 \\equiv x^3 + 2x+3 (\\mod 97)$以及点$P = (3,6)$和$Q = (80,87)$，这两个点都是属于这个5阶子群的。 我们用不同的速度遍历这个值对序列知道我们找到两个不同的值对(a,b)和(A,B)能够得出相同的但。在这个例子我们相同的值对是(3,3)和(2,0)，通过这两个点我们可以算出$x = (3-2)(0-3)^{-1} \\mod 5 =3$,而且这确实是正确的 $ Q = 3P $. 基本地，我们将我们的随机数生成序列得到(a,b)值对，然后计算得到相应的点序列$aP+bQ$。这个值对序列可能是也可能不是循环的，但是点序列确实循环的，因为$P$和$Q$都是由相同的基点产生的，从子群理论中我们知道仅仅在其上用加法和乘法我们是无法“逃出”子群的。 现在我们用两个小动物来做比喻，乌龟和兔子，乌龟和兔子将会找到两个相同的点，但是却是由不同的系数对产生的。更加详细地，乌龟将会找到值对(a,b)而兔子将会找到值对(A,B)，最后会满足$ aP+bQ = AP+BQ $\n如果你的随机数序列是由某种算法（或者就是固定存储的），那么我们就可以简单地退出这个龟兔理论所需要的空间复杂度为 $O(\\log n)$，然而计算时间复杂度却没有那么简单，但是我们可以通过计算概率的方式得到时间复杂度就和我们前面说的一样是$O(\\sqrt{n})$。\n与Pollard's $\\rho$亲密接触 我已经写了一个用于计算Pollard's rho的python脚本，虽然这个脚本没有完整地实现Pollard's rho算法，但是就是有稍稍的改进（我用了更加高效的随机数生成算法生成值对序列）。这个脚本包含了一些非常有用的注水，如果你对这个算法感兴趣的话，去阅读这些注释吧。 这个脚本和baby-step, giant step一样都只能工作在小规模的曲线上，而且和上面的输出结果是一样的。 与Pollard's $\\rho$实战 我们前面说到baby-step, giant step算法无法应用到实际当中，因为它需要巨大的内存空间，然而Pollard's rho却不需要那么大的内存空间，那么实际上呢？ Certicom在1988年发起了一项挑战，计算比特位数为109到359的椭圆曲线的离散对数。到今天为止，也只有109bit长度的曲线被成功攻破，这次尝试是在2004年发起的，在维基百科上的评论是：\n he prize was awarded on 8 April 2004 to a group of about 2600 people represented by Chris Monico. They also used a version of a parallelized Pollard rho method, taking 17 months of calendar time. 我们已经说过了，prime192v1是目前来说“最小”的椭圆曲线了，我们也说过Pollard's rho 的时间复杂度是$O(\\sqrt{n})$,如果我们用Chris Monico相同的技术(相同的算法，在相同的硬件和相同的机器上)，我们需要花多久来计算一次在prime192v1上的离散对数呢？ $$17\\ months \\times \\frac{\\sqrt{2^{192}}}{\\sqrt{2^{109}}} \\approx 5\\cdot 10^{13} \\ months$$ 这个数字足以在某种程度上说明破解离散对数问题在目前的技术能力上的难度了。\n对比一下Pollard's rho 和Baby-step giant step算法 我决定将baby-step giant-step和Pollard's rho一起同暴力穷举算法合在一起放到第[四个脚本](fourth script)中做一个比较，看看它们之间的性能。 第四个脚本会在“小型”曲线上用不同的算法计算所有的点，并报告一共耗时多少：  Curve order: 10331 Using bruteforce Computing all logarithms: 100.00% done Took 2m 31s (5193 steps on average) Using babygiantstep Computing all logarithms: 100.00% done Took 0m 6s (152 steps on average) Using pollardsrho Computing all logarithms: 100.00% done Took 0m 21s (138 steps on average) 正如我们所期待的，暴力穷举同其它两个算法相比就非常地慢，Baby-step giant step算法是最快的，而Pollard's rho算法比Baby-step giant step算法慢三倍（虽然看上去它用的内存空间最小且步数最小）。 当然我们在看看计算步数，暴力穷举平均花费了5193步，非常接近10331/2（曲线阶数的一半），Baby-step和Pollard\u0026rsquo;s rho花费了152步和138步两个步数都很接近10331的平方根(101.64)。\n最终结论 我们讨论了很多算法，同时也给出了相当的数据。但是我们一直都有一个问题，就是：算法能够在很多方面加以提升，比如硬件，特殊针对于计算离散对数的硬件也会出现。 实际上在今天提升硬件的方法看上去是不实际的，并不是说这个方法不可行，而是我们又更加好的办法（记住，现在还没有严谨的证明离散对数的复杂性）。\n##Shor\u0026rsquo;s algorithm 如果现在的技术不可行，那么未来的技术呢？好吧，现在有些事变得有些令人担忧：实际上存在一种量子算法能够在多项式时间内解出离散对数问题，这个算法的时间复杂度是O((log_n)^3)，空间复杂度是$O(log_n)$。\n之所以量子算法能够有这么高的效率，得益于它的状态叠加。在传统的计算机中，内存单元（例如bit）只有两种状态1或者0，没有中间状态。但是相对的，量子计算机的内存单位（被称为qubit）的状态却是无法被确定的，这同薛定谔的猫理论类似，只要在其被测量的时候才能知道其具体状态，状态叠加并不是说这个qubit可以同时为0和1,而是我们可能发现0也可能发现1。而量子算法就是基于每个qubit的可能性构建的。\n其实这隐含了一个qubit的特性，就是我们可以在同一时刻输入多个可能的输入值。举例来说我们可以告诉量子计算机这里有一个数x均匀地分布在0到n-1之间，这仅仅需要$\\log n$个quebit而不是$n\\log n$个bit。然后我们可以让量子计算机计算标量乘法得到$xP$。结果会在我们给定的0P到(n-1)P之间叠加——也就是说，我们如果现在测量我们的qubit们我们将会得到$0P$到$(n-1)P$之间的某个值，而概率是1/n。\n上面的只是让你知道状态叠加的强大力量，，但是Shor\u0026rsquo;s算法目前还是没有办法实现的，实际上其实更加复杂。其中比较复杂的一点是，当我们能够“模拟”同时有n种状态的时候，我们需要输出的是一个答案而不是一堆答案（即，我们需要知道单一的对数，而不是一堆可能错误的对数）。\nECC 和RSA 现在让我们忘了该死的量子计算，这个问题目前还不能作为一个严重的问题，现在我们需要讨论的问题是在RSA还不错的情况下，为什么要用椭圆曲线呢？ 一个由NIST给出的快速答案，它给出了一个同等安全等级下的密钥大小对比表：\n   RSA Key size (bits) ECC key size (bits)     1024 160   2048 224   3072 256   7680 384   15360 521    需要注意的是，RSA的key大小和ECC的key大小并没有相应的线性关系（换句话说，如果我们将RSA的密钥大小加倍，并不能相应地得到双倍的ECC的密钥）。这个表告诉我们ECC不仅仅需要较小的内存，而且公钥的生成和签名更快。 但是为什么是这样的呢？这是因为在椭圆曲线上的计算离散对数的最快的方法就是 Pollard's rho and baby-step giant-step但是在RSA体系下我们却有更加快的算法。一个较为实用的算法是：general number field sieve,一个通过因式分解计算离散对数的算法。general number field sieve是目前整数因式分解最快的算法。 所有基于模运算的加密系统都有相同的问题，包括DSA, D-H 和ElGamal。\nNSA的隐藏威胁 现在我们需要讨论一下更加难的部分。目前我们已经讨论了算法和数学的部分，现在我们讨论一下复杂的人和事。 如果你还记得的话，上一篇文章我们说到有一些椭圆曲线是很脆弱的，为了解决来着可疑来源的曲线的可信任问题，我们加入了一个随机种子到主要参数当中。而且我们可以看到那些NIST的标准曲线都是可验证的随机的。\n如果我们阅读维基百科\u0026ldquo;nothing up my sleeve\u0026rdquo;我们可以看到如下内容：\n MD5的随机数是从正弦整数中产生的 Blowfish的随机数是从$\\pi$的第一个数字产生的 RC5的随机数是从$e$和黄金比例中产生的 这些数字都是随机的因为这些数字都是非均匀分布的。这些都是毋庸置疑的，因为这些都是公开的。 但是现在的问题是：NIST曲线的随机数是从哪里来的呢？答案是我们不知道，而且这些随机种子也没有公开。  是否存在这样的可能:NIST已经发现了一类“足够大的”弱椭圆曲线，并尝试了一些可能的随机种子直到生成一个脆弱的曲线？我无法回答这个问题，但是这是一个正当的且重要的问题。我们知道NIST成功地规范化了一个\u0026quot; vulnerable random number generator \u0026ldquo;(一个奇怪的基于椭圆曲线的若随机数生成器)。可能他们也成功地规范化了一组弱椭圆曲线，但是是不是这样我们无从得知。\n理解“可信随机”和“安全”是两码事，这个和对数问题的困难程度或者我们的密钥长度无关，如果我们的算法被攻破了，我们几乎什么都做不了。\n考虑到这些，RSA是成功的，因为它不需要任何特殊的领域参数，而且这些领域参数也不会被篡改。RSA(和其他的模运算加密系统一样)在我们无法自己生成领域参数且无法信任授权方时，是一个很好的选择。值得一提的是，TLS用的也是椭圆曲线算法，它基于ECDHE和ECDSA算法，证书基于prime256v1（也称secp256p1）上。\nThat's all 我希望你能够喜欢这个系列的文章。我的目标是让你能够对椭圆曲线加密系统有一个基本的了解，知道其术语代表的都是什么意思。如果我达成了我的目标，那么目前依旧能够理解现在的基于ECC的加密系统是怎么工作的，同时你也能够通过阅读一些“不怎么优雅”的文献来加深对这些知识的理解。当我在完成这个系列的文章的时候我跳过了很多细节，用了最简单的术语和知识为了是你能够快速理解。但是我比较担心的是你无法理解网络上的一些较为复杂的术语，我认为这系列的文章能够在简单性和完整性上做到了较好的均衡。 一定要注意仅仅阅读了这系列的文章并不能让你实现一个安全的ECC加密系统：这需要了解很多安全方面的重要细节。还记得requirements for Smart\u0026rsquo;s attack和Sony\u0026rsquo;s mistake——这些例子都告诉我们产生一个不安全的算法是多么简单，以及攻破这些算法是多么容易。\n所以，如果你对ECC有着更深层次的兴趣，应该去看些什么呢？\n首先，我们已经看到了素数域上的Weierstrass curves，但是你必须要知道其他的域和曲线，典型的有：\n Koblitz curves over binary fields. 这些曲线定义在$y^2 + xy = x^3 ax^2 +1$()其中a 是0或1),它的有限域包括了$2^m$个元素（其中$m$是一个素数），这个曲线能够明显地提高加法和乘法的效率，标准的Koblitz曲线有nistk163, nistk283 和nistk571 (三个曲线分别定义在 163, 283 和 571 位的域上)。 Binary curves. 这个曲线和 Koblitz curves 很相似，形如 and are in the form $x^2+xy=x63+x^2+b$ (其中$b$ 是一个从随机数). 正如其名，binary curves 被限制到二进制域中。主要的标准曲线有 nistb163,nistb283 和 nistb571。目前有观点认为 Koblitz 和 Binary curves不像 prime curves那么安全。 Edwards curves, 形如$x^2+y^2=1+dx^2y^2$ (其中 $d$ 可以是 0 或 1). 它的特点除了其加法和标量乘法的速度都很快之外，还有它点加的公式都是相同的，在某些情况下可以描述为 $(P≠Q,P=Q, P=−Q, ...)$。这个特点能够抵御side-channel攻击，这个攻击通过零标量乘法的时间来猜测标量因子。( This feature leverages the possibility of side-channel attacks, where you measure the time used for scalar multiplication and try to guess the scalar coefficient based on the time it took to compute.) Edwards curves 相对来说比较新 ( 2007被发表) 目前还没有权威机构类似于Certicom 或者 NIST对其进行标准化。 Curve25519 and Ed25519 are two particular elliptic curves 是分别为ECDH和ECDSA专门设计的曲线，就像Edwards curves, 这两个曲线非常地快速，并能够抵御side-channel 攻击。 并且正如 Edwards curves,这两个曲线还没有被正式标准化，我们还不能够在典型的引用中找到他们的身影（除了OpenSSL，它在2014年支持了Ed25519曲线）。  如果你对ECC的实现细节比较感兴趣，我建议你去阅读以下OpenSSL和GnuTLs的源代码。 最后如果你对ECC背后的数学细节感兴趣，而不是安全高效的算法，你需要知道如下知识：\n Elliptic curves are algebraic varieties with genus one. Points at infinity are studied in projective geometry and can be represented using homogeneous coordinates (although most of the features of projective geometry are not needed for elliptic curve cryptography).  以及不要忘记学习finite fields 和 field theory。\n这些就是你需要查询或者你会感兴趣的全部主题了。\n现在，这个系列已经官方完结了。\n","date":"2017-06-15","permalink":"https://chenquan.me/posts/elliptic-curve-cryptography_a-gentle-introduction_4/","tags":["cryptography","ecc","ecdsa"],"title":"Elliptic Curve Cryptography: a gentle introduction (4)"},{"categories":null,"contents":"这篇文章是系列文章ECC：优雅入门中的第三篇文章。\n在之前的文章中我们知道了什么是椭圆曲线以及我们定义了一些群律为了能够在椭圆曲线上做一些数学运算。然后我们将椭圆曲线重新在一个素数的有限模整数域中进行描述。在这次重新描述过程中，我们知道了椭圆曲线是如何生成一个循环子群的，以及我们介绍了一些基本的概念：基点(base point)，阶(order)和辅因子(cofactor)。\n最后，我们了解了在有限域上的标量乘法是一个“简单”的问题，但是反过来的离散对数问题却是一个“困难”的问题。现在我们一起看看这些东西是如何应用到加密算法中的。\n主要参数(Domain parameters) 我们的椭圆曲线算法将会在一个定义在有限域上的的椭圆曲线的子群中运行。因此我们需要如下的几个参数： - 素数P 描述了有限域的大小。 - 系数a和b 用于描述椭圆曲线的方程。 - 基点G用于生成我们的子群 - 阶n ，子群的阶 - 辅因子h ，子群的辅因子 一言以蔽之，我们算法中所需要的主要参数是一个6元组(p,a,b,G,n,h)。\n随机曲线 当我们说离散对数问题是一个“困难的问题”时，其实并不完全正确。这里有一些椭圆曲线是非常脆弱的，这些椭圆曲线能够被一些具有特殊目的的算法快速解决。例如，所有的曲线满足 $p = hn$ (这种曲线的有限域的阶和椭圆曲线的阶相同)能够被Smart's attack轻而易举地攻破,这个攻击方式能够在多项式时间内将离散对数问题解决。 现在，假设我给你一些曲线的主要参数。这里就会存在一个问题，就是可能我已经发现了一些脆弱的能够被轻而易举地攻破的曲线，但是没有其它人知道，而且我能够建立一个“”快速”的算法，很快地得到我刚刚给你的离散对数的答案。那么我该如何说服你相信我，或者说我并不知道任何的弱点呢？也就是说我该如何证明，这个曲线是安全的（在某种意义上说，这不能被我以特殊的目的攻击）？\n为了能够解决这类问题，我们常常会增加一个主要参数：种子S（seed S）。这是一个随机数用来生成系数a和b，或者是基点G，或者两者。这些参数通过对S计算hash生成。Hash，众所周知，非常容易计算，但是逆运算是非常\u0026quot;困难\u0026quot;的。  通过种子生成一个随机曲线的示意图：一个随机数的hash是用来计算曲线的不同参数的。 如果我们想要作弊，通过主要参数中得到随机种子，我们需要解决一个\"hard\"问题：hash逆推 一个通过种子生成的曲线被称为通过随机验证。这个利用hash生成参数的理论被称为\"nothing up mu sleeve\",这个理论被广泛应用在密码学中。 这个技巧能够提供一些保障，保证这个曲线不是被作者精心设计的隐藏的脆弱的。实际上，如果我给你一个曲线和以和种子，它就意味着我不能够任意地选择参数a和b,而你也相对的可以肯定这个曲线不能够被我利用特定的目的攻击。而这个“相对的”的原因我将在下一篇文章中进行解释。 一个用于生成和检查随机曲线的标准算法已经在ANSI X.9.62中进行描述，这个算法是基于SHA-1。如果你对这个感兴趣，你可以阅读a specification by SECG了解生成可信的随机曲线算法（搜索 \u0026ldquo;Verifiably Random Curves and Base Point Generators\u0026rdquo;）。\n我写了一个简单的python脚本用于验证所有的随机曲线，这个脚本是基于OpenSSL的。我非常建议你去看看。\n椭圆曲线加密(Elliptic Curve Cryptography) 我们在前面花了很多时间，但是最后我们还是到这一节了！废话不多说，简单粗暴： 1. 私钥是一个随机的整数，从${1,...,n-1}$中选择(其中n 是子群的阶)。 2. 公钥是一个点H满足条件H = dG(其中G是这个子群的基点) 看到了嘛？如果我们知道d和G（以及其他的主要参数），找到最终的H是很\u0026quot;简单\u0026quot;的。但是如果我们知道H和G，找到私钥d是困难的，因为这需要我们解决一个离散对数问题。\n现在我们将介绍两个基于上述条件的公钥加密算法：ECDH(Elliptic curve Diffie-Hellman)用于加密，另一个是ECDSA(Elliptic Curve Digital Signature Algorithm)用于数字签名。\n利用ECDH加密 ECDH 是Diffie-Hellman算法。这实际上是一个密钥交换协议,而不仅仅是一个加密算法。简而言之，ECDH定义了密钥应该如何生成，以及在双方之间如何交换。至于如何用这个密钥加密数据由我们自己决定。 那么我们简单地描述一下这个如何解决这个问题的过程（通常我们用Alice and Bob描述）。当Alice和Bob之间需要安全地交换信息，但是第三者(中间人)威胁到他们，他可能截获消息，但是无法解密这些信息。这是TLS底层的一个基本理论，这里仅仅给你一个例子：\n这里是整个密钥交换的过程：\n 首先,Alice和Bob生成他们各自的公钥和私钥，我们假设私钥$d_A$和公钥$H_A = d_AG$是Alice的，以及私钥$d_B$和公钥$H_B = d_BG$是Bob的。需要说明的是，Alice和Bob都使用相同的主要参数，在相同的有限域上的相同椭圆曲线的相同基点G。 Alice 和Bob再不安全的网络上交换各自的公钥H_A 和H_B，中间人将会截获$H_A$和$H_B$，但是无法得到$d_A$和$d_B$，除非它能够解决离散对数问题。 Alice 计算$S=d_A\\cdot H_B$(用她自己的私钥和Bob的公钥)，然后Bob计算$S = d_B\\cdot HA$(用他自己的私钥和Alice的公钥)，这里需要说明的是Alice和Bob来说都是算出来的$S$都是相同的，因为：  $$ S = d_AH_B = d_A(d_BG) = d_B(d_AG) = d_BH_A $$ 中间人虽然知道$H_A$和$H_B$（当然也可能知道所有的主要参数），但是仍然无法得到共享密钥$S$，这被称为 Diffie-Hellman问题，它可以被如下问题描述：\n 给定三个点 P , aP 和bP, abP的答案是什么？ 或者等价的： 给定三个整数$k$,$k^x$,$k^y$，$k^{xy}$的答案是什么？ （后面的这个问题是在原始的Diffie-Hellman算法中被采用，基于模运算） ecdh Diffie-Hellman密钥交换，Alice和Bob能够“轻易”地计算出共享密钥，但是中间人需要解决一个“困难”的问题。 在Diffie-Hellman问题背后的理论在可汗学院有一个超棒的介绍，但这个仅仅是在模运算的体系上的介绍，(而不是在椭圆曲线上的介绍)。 椭圆曲线上的Diffie-Hellman问题被认为是一个\"hard\"的问题，他被认为其难度和离散对数相同，尽管没有有效的证明。我们知道的是这个问题没有办法\"harder\"了，因为解决这个问题相当于解决Diffie-Hellman问题。 现在，Alice和Bob已经得到对称密钥的，他们可以通过对称加密交换密钥了。\n举例来说，他们可以用S的x坐标值作为AES或者3DES对称加密的密钥。这就和TLS的做法差不多，区别是TLS将x坐标加上了一些连接的标识码并做了一次hash。\n接触一下ECDH 我写了另一个简单的pyhton脚本用来计算椭圆曲线上的公/私钥对和共享密钥。 不同于我们之前提到的任何例子，这和脚本用了一个标准的曲线，而不是我们前面用的在很小的域中的简单曲线。这个曲线是 secp256k1，来自SECG(the \u0026ldquo;Standards for Efficient Cryptography Group\u0026rdquo;, founded by Certicom)，这和比特币数字签名用的曲线是同一个，这里是这个曲线的一些主要参数：\n $p = 0xffffffff ffffffff ffffffff ffffffff ffffffff ffffffff fffffffe fffffc2f$ $a = 0$ $b = 7$ $x_G = 0x79be667e f9dcbbac 55a06295 ce870b07 029bfcdb 2dce28d9 59f2815b 16f81798$ $y_G = 0x483ada77 26a3c465 5da4fbfc 0e1108a8 fd17b448 a6855419 9c47d08f fb10d4b8$ $n = 0xffffffff ffffffff ffffffff fffffffe baaedce6 af48a03b bfd25e8c d0364141$ $h = 1$ (这些值是我从OpenSSL 的源代码中找到的)  当然，你可以随意地修改脚本，改成你需要的曲线和相应的参数，但需要记住曲线需要满足素数域，和非尖锐(Weierstrass normal form)的形式，否则这个脚本将不会工作。\n这个脚本虽然简单，但是却包含了所有我们前面提到的算法：点加(point addition ), double and add,ECDH，我推荐你阅读并且运行它，运行结果将如下形式：\nCurve: secp256k1 Alice's private key: 0xe32868331fa8ef0138de0de85478346aec5e3912b6029ae71691c384237a3eeb Alice's public key: (0x86b1aa5120f079594348c67647679e7ac4c365b2c01330db782b0ba611c1d677, 0x5f4376a23eed633657a90f385ba21068ed7e29859a7fab09e953cc5b3e89beba) Bob's private key: 0xcef147652aa90162e1fff9cf07f2605ea05529ca215a04350a98ecc24aa34342 Bob's public key: (0x4034127647bb7fdab7f1526c7d10be8b28174e2bba35b06ffd8a26fc2c20134a, 0x9e773199edc1ea792b150270ea3317689286c9fe239dd5b9c5cfd9e81b4b632) Shared secret: (0x3e2ffbc3aa8a2836c1689e55cd169ba638b58a3a18803fcf7de153525b28c3cd, 0x43ca148c92af58ebdb525542488a4fe6397809200fe8c61b41a105449507083)  短暂ECDH (Ephemeral ECDH) 你可能在有些地方看到ECDHE而不是ECDH，这里在ECDHE中的\"E\"代表\"Ephemeral\"，表示这次密钥交换是临时的而不是永久的。 ECDHE在实际中的应用也很广泛。在TLS中，客户端和服务器在连接建立的时候快速生成公私钥对。这些密钥都是经过TLS证书签名（为了授权）的，然后在双方交换。\n##用ECDSA签名(Signing with ECDSA) 我们现在又有一个新的剧本：Alice 需要用她的私钥($d_A$)签名一个消息，然后Bob需要用Alice的公钥($H_A$)验证这个消息是否真实。除了Alice没有人能够创建一个合法的签名，但是所有的人都能够验证这个消息的真实性。\n再一次地，Alice和Bob利用相同的主要参数，现在我们来研究新的算法——ECDSA,利用椭圆曲线实现的一种数字签名算法的变体。\nECDSA利用消息的hash工作，而不是消息本身。hash算法的选择取决于我们自身，但是明显地，我们需要选择一个密码学安全的hash算法。消息的hash应该缩短到其bit位数和n(子群的阶)的bit位数相同,被缩短之后的hash应该是一个整数，我们用$z$来表示。\n对Alice来说这个算法主要有以下几步：\n 选择一个随机整数k，这个整数$k$从${1,\u0026hellip;,n - 1 }$中选择(其中 $n$是子群的阶) 计算点$P = kG$(其中$G$是子群的基点) 计算数$r = x_p \\mod n$ 其中$x_p$是点P的横坐标。 如果$r=0$，从新选择一个k然后再试验一次。 计算 $s = k^{-1}(z + rd_A) mod n$ 其中$d_A$是Alice的私钥同时$k^-1$是k模nd的相反幂(where $d_A$ is Alice\u0026rsquo;s private key and $k^{−1}$ is the multiplicative inverse of $k$ modulo $n$) 如果$s = 0$,选择一个新的k然后再试一次。  这里的(r,s)对就是签名了。\n Alice 用自己的私钥$d_A$和一个随机数$k$对hash进行签名。Bob利用Alice的公钥$H_A$验证消息的合法性。 需要强调的是，可以通过(r,s)对得到公钥。 用简洁明了的话总结，这个算法首先生成了一个密钥(k)。这个密钥隐藏到r当中了，这点是利用点乘得到的（我们知道点乘是\"easy\"的，但是反过来却是\"困难\"的）。r 通过公式$s = k^{-1}(z + rd_A) \\mod n$同消息的摘要绑定到一起了。 再说明一下，为了计算s,我们需要计算 $k \\mod n$的相反幂，我们在前面的文章中有做解释,只有当n是素数的时候才能够奏效。如果子群拥有一个非素数阶,ECDSA算法将不可用。几乎所有的标准曲线拥有素数阶并不是偶然，而是因为非素数阶的曲线不能够适用于ECSDA算法。\n验证签名(Verifying signatures) 为了验证签名，我们需要Alice的公钥$H_A$,hash(被缩短)$z$和签名(r,s)。  计算第一个整数$u_1 = s^{-1}z \\mod n$ 计算第二个整数$u_2 = s^{-1}r \\mod n$ 计算点 $P= u_1G + u_2\\cdot H_A$  如果式子$r = x_P \\mod n$成立，则验签通过 算法的正确性 第一眼我们没有办法看出这个算法的正确性，所以我们把所有的等式重新分析一下，将会豁然开朗。 让我们从$P = u_1G + u_2H_A$开始，从公钥的定义可知，$H_A = d_AG(其中 $d_A$是 私钥)$。我们可以得到：\n$$ P = u_1G + u_2G \\ = u_1G + u2d_AG \\ = (u_1 + u_2d_A)G $$\n利用$u_1$和$u_2$的定义，我们可以得到：\n$$ P = (u_1 + u_2d_A)G \\ = (s^{-1}z + s^{-1}rd_A)G \\ = s^{-1}(z +rd_A)G $$\n这里为了简洁起见我们忽略了 \u0026ldquo;mod n\u0026rdquo;,因为G生成的循环子群都拥有阶n，因此“mod n”是多余的。\n之前我们定义$s = k^{-1}(z + rd_A) \\mod n$.两边都乘以一个$k$并除以一个$s$,我们可以得到一个新的等式 $k = s^{-1}(z+rd_A) \\mod n$.将这个结果替换到我们先前得到P的等式中，我们得到：\n$$ P = s^{-1}(z + rd_A)G \\ =kG $$\n** 这个结果同我们在步骤2中生成签名的算法一样！当生成签名并且验证它们之时，我们都需要计算相同的点P**,并且用的是不同的公式集。这就是为什么这个算法能够work的原因。\n###接触一下ECDSA 当然，我也写了一个简单的[python脚本]来演示签名的生成和验证。这个脚本有一些代码同ECDH的脚本相同，具体来说就是有关主要参数和公私钥对生成的部分。\n这里是这个脚本输出的内容：\nCurve: secp256k1 Private key: 0x9f4c9eb899bd86e0e83ecca659602a15b2edb648e2ae4ee4a256b17bb29a1a1e Public key: (0xabd9791437093d377ca25ea974ddc099eafa3d97c7250d2ea32af6a1556f92a, 0x3fe60f6150b6d87ae8d64b78199b13f26977407c801f233288c97ddc4acca326) Message: b'Hello!' Signature: (0xddcb8b5abfe46902f2ac54ab9cd5cf205e359c03fdf66ead1130826f79d45478, 0x551a5b2cd8465db43254df998ba577cb28e1ee73c5530430395e4fba96610151) Verification: signature matches Message: b'Hi there!' Verification: invalid signature Message: b'Hello!' Public key: (0xc40572bb38dec72b82b3efb1efc8552588b8774149a32e546fb703021cf3b78a, 0x8c6e5c5a9c1ea4cad778072fe955ed1c6a2a92f516f02cab57e0ba7d0765f8bb) Verification: invalid signature  正如你看到的，这个脚本首先将消息进行签名（\u0026ldquo;Hello!\u0026ldquo;的byte串），然后验证了这个签名，然后它尝试用同一个签名和不同的消息(\u0026ldquo;Hi there\u0026rdquo;)进行验证，然后失败了。最后它用相同的消息但是采用了另一个随机的公钥进行验签，再一次失败了。\nk的重要性 当生成ECDSA的签名的时候，保持密钥k的保密性是非常重要的，如果我们在所有的签名中使用相同的k，过着我们的随机数生成器是可以预测的，那么攻击者就能够到私钥！ 这是sony前几年犯的一个错误，简单地说，那时候的PS3本来只能运行Sony用ECDSA算法签名的游戏，本来就算我写了一个PS3上的游戏我们没有办法将其发布，除非我得到了Sony的签名，但是这个过程的问题是，这个签名Sony是用相同的k生成的。 (Apparently, Sony\u0026rsquo;s random number generator was inspired by either XKCD or Dilbert.)\n在这种情况下，我们能够轻易地通过两个签名过的游戏恢复得到Sony的私钥，我们提取出这两个游戏的hash（$z_1$和$z_2$）以及他们的签名$(r_1,s_1)$和$(r_2,s_2)$，以及他们的主要参数：\n 首先，我们可以得到 $r_1 = r_2$（因为 $r = x_P \\mod n $和$P = kG$ 对于两个签名而言是相同的）。 得到 $s_1 - s_2 \\mod n = k^{-1} (z_1 - z_2) \\ mod n$ （这个结论是从s的公式中推出的） 现在在等式两边都乘以k:$ k(s_1 - s_2) \\mod n = (z_1 - z_2) \\mod n$ 两边同时除以($s_1-s_2$)来得到$k = (z_1 -z_2)(s_1 - s_2)-1 \\mod n$  最后一个等会能够让我们通过两个hash和其对应的签名计算出k，现在我们可以通过下面的共识计算出s： $$ s = k^{-1}(z + rd_s) \\mod n \\Rightarrow d_s = r^{1}(sk -z) \\mod n $$\n就算k不是固定的但是是可预测的话，用类似的技术也能够得到私钥。\n未完待续 接下来，我将会发布这个细节的第四篇文章，它将讨论如何解决离散对数问题，以及一些在椭圆曲线加密中较为重要的问题，以及ECC和RSA算法的区别，不要错过！","date":"2017-06-15","permalink":"https://chenquan.me/posts/elliptic-curve-cryptography_a-gentle-introduction_3/","tags":["cryptography","ecc","ecdsa"],"title":"Elliptic Curve Cryptography: a gentle introduction (3)"},{"categories":null,"contents":"这篇文章是ECC:优雅入门系列的第二篇文章\n在前面的文章中，我们已经知道了椭圆曲线是如何在实数域内定义一个群的。特别地我们已经定义了一种点加的规则:给定三个齐次的点，它们的和为0 $(P+Q+R=0)$。我们已经通过一个几何方法和一种代数方法来描述计算点的加法。\n然后我们介绍了一个扩展标量乘法$(nP = P+P+…+P)$我们也找到了一种较为简单的计算标量乘法的”简便“算法:double and add。\n现在我们将我们的椭圆曲线严格限制到有限域（finiite fields）中，而不是在实数域中，我们看看会有什么变化。\np的整数模域(The field of integers modulo p) 首先，有限域，是一个有限数字元素的集合。一个有限域的例子就是p的证书模。其中p是一个素数。这通常会被称为$\\mathbb{Z}/p,GF(p)$ 或者$\\mathbb{F}_p$。我们会在后面用到这些符号。 在域中我们有两个双目运算，加法($+$)和乘法( $\\times$ )。这两者都是闭集，满足结合律以及交换律。对于这两个操作，存在这样的唯一元素，有且仅有唯一的相反元素。最终，乘法可以分配到加法中：$ x\\times (y+z) = x \\times y+x \\times z $。\n这个p的整数模域包括了所有的从0到p-1的整数。加法和乘法也都能够在模运算中表现良好，这里有$\\mathbb{F}_{23}$的一些样例操作：\n 加法 $(18 + 9)\\mod 23 = 4$ 减法 $(7 - 14)\\mod 23 = 16$ 乘法 $4\\cdot7\\mod 23 = 5$ 相反加 $-5\\mod 23 = 18$  特别地: $(5+(-5))\\mod 23 = (5+18)\\mod 23 = 0$   相反幂 $9^{-1} \\mod 23 = 18$  特别地： $9\\cdot9^{-1}\\mod 23 = 9 \\cdot 18\\ mod\\ 23 = 1$    如果这些等式对你来说不是很熟悉，那么你可能需要恶补一下有关模运算的有关知识，你可以到可汗学院学习一下。 前已经阐明，p 的模整数是一个域，并且其拥有的所有属性都已经列出。需要注意的是，p 必须是素数，这点非常重要。4的模整数就不是一个域：2没有相反幂（例如，等式 $2\\cdot x\\mod 4 = 1$没有解）\n模p的除法（division modulo p） 我们马上就要在$\\mathbb{F}_p$上定义椭圆曲线，但是在做这件事之前，我们需要对在$\\mathbb{F}_p$的上的除法有一个清晰的认识，我们可以简单地认为:$x/y=x\\cdot y^{-1}$，或者，简而言之，x除以y就等于x乘以y的相反幂，这个事实没有什么特别的，但是它给了我们最简单的得到除法结果的方法：找到一个数的相反幂，然后做一次简单的乘法。 计算相反幂有一个”挺简单“的方法，那就是采用\u0026quot;extended Euclidean 算法\u0026quot;，这个算法的时间复杂度是$O(log_p)$（或者$O(k)$，如果考虑到位长度）在最差的情况下。\n我不会展开对\u0026quot;extended Euclidean\u0026ldquo;算法的讲解，因为这和我们的主题无关，但我在这里提供了一个能够工作的python实现代码：\ndef extended_euclidean_algorithm(a, b): \"\"\" Returns a three-tuple (gcd, x, y) such that a * x + b * y == gcd, where gcd is the greatest common divisor of a and b. This function implements the extended Euclidean algorithm and runs in O(log b) in the worst case. \"\"\" s, old_s = 0, 1 t, old_t = 1, 0 r, old_r = b, a while r != 0: quotient = old_r // r old_r, r = r, old_r - quotient * r old_s, s = s, old_s - quotient * s old_t, t = t, old_t - quotient * t return old_r, old_s, old_t def inverse_of(n, p): \"\"\" Returns the multiplicative inverse of n modulo p. This function returns an integer m such that (n * m) % p == 1. \"\"\" gcd, x, y = extended_euclidean_algorithm(n, p) assert (n * x + p * y) % p == gcd if gcd != 1: # Either n is 0, or p is not a prime number. raise ValueError( '{} has no multiplicative inverse ' 'modulo {}'.format(n, p)) else: return x % p  $\\mathbb{F}_p$上的椭圆曲线(Elliptic curves in $\\mathbb{F}_p$) 现在我们已经先把所有需要的先修知识都进行说明了，现在我们可以把椭圆曲线转移到$\\mathbb{F}_p$上了。在前文我们提到点集的描述是： $ {(x,y) \\in \\mathbb{R}^2\\ | y^2 \\ =x^3 + ax + b , 4a^3 + 27b^2 ≠ 0 } \\cup {0} $\n现在变成：\n$ { (x,y) \\in (\\mathbb{F}_p)^2\\ | \\ y^2\\equiv x^3 + ax + b (\\mod p), 4a^3+27b^2 \\not\\equiv(\\mod p) } \\cup {0} $\n其中0依旧认为是无限远的点，以及a和b是在$\\mathbb{F}_p$上的两个整数。  曲线 $y^2 \\equiv x^3−7x+10(\\mod p)$ 其中点 $p=19,97,127,487$。 需要注意的是，对于所有的$x$，当$y=p/2$,至多存在两个点。  曲线 $y^2\\equiv x^3(\\mod 29)$ 是singler的，它有三个点在$(0,0)$。这不是一条有效的椭圆曲线。 前面描述的是连续的曲线，在xy平面上没有相交点，而且我们也可以证明，就算将其放到我们的有限域中，椭圆曲线在域$\\mathbb{F}_p$依旧是一个阿贝拉群(abelian group)。 点加(Point addition) 明显地，我们需要将我们的加法的定义改变一下，为了能够让它在域$\\mathbb{F}_p$也能够工作得很好。在实数域下，我们称三个齐次的点之和为零($P+Q+R=0$)。我们可以保留这个定义，但是在$\\mathbb{F}_p$上，怎么样的三个点是齐次的呢？ 如果有一条直线可以穿过三个点，那么我们可以将三个点称为齐次的，现在，我们当然不能和在$\\mathbb{R}$一样定义。我们可以这样说，在$\\mathbb{F}_p$上的直线是一个点的集合$(x,y)$，这个点集满足条件$ax+by+c\\equiv 0 (\\mod p)$（这其实是一个标准的直线方程，我们就是加了\"$(\\mod p)$\"而已。  在曲线$y^2 \\equiv x^3− x + 3(\\mod 127)$, 其中 $P=(16,20)$以及 $Q=(41,120)$上的点加. 注意，直线$ y \\equiv 4x+83(mod127)$是如何“重复”地在平面中相连的。 给定前提，现在在一个群中，点加仍然有如下我们已知的属性：  $Q+0=0+Q=Q$ (这个由唯一元素(identity)定义可得)。 给定一个非零点$Q$, 其相反数$−Q$是一个拥有相同的横坐标但是具有相反的纵坐标的点。或者如果你愿意的话，$-Q = （x_Q,-y_Q \\mod p）$ 例如，一个曲线在域$\\mathbb{F}_{29}$上，有这样的一点$Q=(2,5)$ ,那么其相反点$-Q=(2,-5\\mod 29)=(2,24)$ 同样的，$P+(-P)=0$（从相反元素的定义可得）  代数和(Algebraic sum) 用代数方法计算点加和的方法和前文其实相同，除了我们需要在末尾加上 $\\mod p$ 的修饰语。因此，给定 $P = (x_p,y_p),Q=(x_Q,y_Q)$ 和 $R = (x_R,y_R)$ ，我们可以通过下面的式子计算$P+Q=-R$ $$ x_R = (m^2-x_P-x_Q)\\mod p \\ y_R = [y_P + m(x_R -x_P)]\\mod p \\ = [y_Q + m(x_R -x_Q)]\\mod p $$\n如果 $P\\neq Q$,斜率m可以通过如下公式得到：\n$$ m=(y_P - y_Q)(x_P-x_Q)^{-1}\\mod p$$\n如果$P=Q$,我们有：\n$$ m=(3x_p^2+a) \\cdot (2y_p)^{-1} \\mod p$$\n这个公式并没有多大的变化并不是巧合，实际上这些公式在所有的域中都适用，有限域或者无限域（除了$\\mathbb{F}_2$和$\\mathbb{F}_3$,这两个是特殊情况）。现在我感觉我需要提供一些证明，但是问题是，证明这个群法则需要用到一些比较复杂的数学概念。然而我找到了一个只用到了一些基础的概念的证明proof from Stefan Friedl，如果你对证明这个公式为什么对几乎所有的域都适用感兴趣的话，可以阅读它。\n回到我们的主题——我们不会说明几何方法：因为这中间存在一些问题。举例来说，在前面的文章中说，我们计算$P+P$我们需要得到曲线上在点P上的切线。但是在不连续的域上，再”切线“的概念就没有什么意义了，我们可以花点时间去解决它们，但是可能会比较复杂，并且不太实用。\n但是，你可以通过交互工具这个我为了点加专门写的小程序来了解更多。\n椭圆曲线的阶（The order of an elliptic curve group） 我们知道定义在有限域中的椭圆曲线具有有限个点，一个非常重要的问题待我们去解决：到底有多少点？ 首先，我们把群里的点的个数称为群的阶（order）。 将x所有从0到p-1的可能值都试一下并不是用来计算点的个数的可行办法，以因为这需要O(p)步，如果p是一个大素数，那么这是一个\u0026quot;hard\u0026quot;问题。\n幸运的是，我们又一个更快地计算群的阶数的算法：Schoof\u0026rsquo;s 算法，我们不想陷入这个算法的细节——我们只需要知道它能够在多项式时间内把阶数算出来，这就够了。\n标量乘法和循环子群(Scalar multiplication and cyclic subgroups) 在实数域中，乘法可以定位为： $$\\underbrace{P+P+\\cdots+P}_{n\\ times}$$ 然后，我们可以继续采用double and add algorithm来计算乘法，总计时间复杂度为$O(log_n)$，(或者$O(k)$其中k是n的bit位数)。我也写了一个交互工具用于演示这个过程。 在$\\mathbb{F}_p$上的椭圆曲线的乘法是具有一些有趣的属性。我们拿曲线$y^2\\equiv x^3 +2x +3 (\\mod 97)$和其上的点$P=(3,6)$。然后我们计算一下P的乘法。  点 $P=(3,6)$ 倍数只有五个独立的点$(0, P, 2P, 3P, 4P)$ 这是一个不断循环的环。从中可以非常简单地看出在椭圆曲线上的标量乘法和模代数运算中加法的相似性。  0P=0 1P=(3,6) 2P=(80,10) 3P=(80,87) 4P=(3,91) 5P=0 6P=(3,6) 7P=(80,10) 8P=(80,87) 9P=(3,91) ...  从这里我们可以很快的看出两个特点： 1. P的倍数只有五种，其他在椭圆曲线上的点不会出现。 2. 他们是“重复的环”。 我们可以得出如下结论\n 5kP=0 (5k+1)P=P (5k+2)P=2P (5k+3)P=3P (5k+4)P=4P  适用任意的整数k。注意到这五个等式可以被“压缩”到一个等式，这要用到模运算符号： $$kP=(k \\mod 5)P$$。\n不仅仅是这些，我们也可以快速地验证：这五个点在一个闭集中。换句话说，不管我们是加上0,2P,3P或者4P,得到的结果都是这五个点钟的某一个。再重复一遍，其它的椭圆曲线上的点将不会出现在结果中。\n这个结论其实适用于所有的点，不仅仅是点P = (3,6)。实际上我们可以通过一个公式来描述通用的P :\n$$ nP+mP = \\underbrace{P + \\cdots + P}{n \\ times } + \\underbrace{ P+ \\cdots + P}{ m \\times} = (n+m) P $$\n这个公式可以描述为：如果我们将两个P的倍数相加，我们将会得到一个P的倍数，（例如，P的倍数是加法闭集的）。这点就足证明P的倍数是一个有椭圆曲线上的点形成的群的循环子群\n“子群”是另一个群的子集，一个“循环子群”是一个所有的元素会循环重现的子群，就像我们前面展示的例子一样。那么这个点P就被称为这个循环子群的生成者(generator)或者基点(base point)。\n循环子群是ECC和其它加密系统的基石。在下一篇文章中将会进行更加详细的解释。\n子群的阶(Subgroup order) 我们可以问问我们自己由点P生成的子群的阶是多少（或者，相等的，P的阶是多少），为了回答这个问题，我们不能够采用前面提到的Schoof算法。因为这个算法只适用于椭圆曲线整个群中，而不能够在子群中工作。在解决这个问题之前，我们先修一点额外的知识:  目前，我们定义了一个群的阶就是这个群中点的个数。这个定义还是有效的，但是在循环子群中，我们需要给一个新的等价定义：点P的阶是一个最下的能够让等式nP=0的正整数。实际上，如果你看到前面的例子，我们的子群包括5个点，而且我们有$5P=0$。 P的阶在Lagrange's theorem中有详细阐述，主要的意思是：子群的阶是父群的阶的一个因数，换句话说，如果一个椭圆曲线包括N个点，那么它的一个子群就包括n个点，而且，n是N的一个因数。  上面的两条特性结合起来我们能够找出一个基于点P的子群的阶： 1. 利用Schoof算法计算椭圆曲线的阶N 2. 找到所有N的因数 3. 对于所有N的的因数n，计算nP 4. 找到最小能够让$nP=0$成立的n，这就是子群的阶。 举一个例子：对于曲线$$y^2 = x^3 - x + 3$$是在域$\\mathbb{F}_{37}$上的曲线，它具有阶$N=42$，它的子群可能有如下的阶：n = 1,2,3,6,7,21或者42。如果我们尝试$P = (2,3)$，我们可以看到$P\\neq0,2P\\neq0,\u0026hellip;,7P=0$,因此点P的阶就是$n=7$。\n需要注意的是：一定要是最小的因数，而不是随意的一个。如果我们随便找一个，我们发现$n=14$也是成立的，但是只是P的一个倍数。\n另一个例子：椭圆曲线定义在$\\mathbb{F}_{29}$上，形式为$y^2 = x^3 - x + 1$，它的阶为 $N = 37$,这是一个素数。它的子群只可能有阶1或者37。正如你猜测的，当$n=1$时，子群只包括一个无穷点，当$n=N$子群包括所有椭圆曲线上的点。\n找到一个基点(Finding a base point) 在我们的ECC算法中，我们需要一个高阶的子群。所以通常地，我们会选择一个椭圆曲线，并计算它的阶(N)，选择一个大因数作为子群的阶(n)最后我们再去找一个合适的基点。也就是说，我们不会先选择一个基点再计算它的阶数，而是用相反的方式，先选择一个合适的阶然后在寻找相应的基点。那么我们是怎么做到的呢？ 首先，我们需要加点料。Lagrange定理可以推出$h=N/n$一定是一个整数（因为n是N的一个因数）。这个h有一个名字叫做子群辅因子。 现在对于椭圆曲线上的任意一点我们有NP=0。这是因为N是所有候选n的倍数。利用辅因子的定义，我们可以得出如下的定理：\n$$n(hP) = 0$$\n现在假设 n 是一个素数（具体原因会在下一篇文章中解释，我们比较钟爱素数阶）。分析一下上面的公式，然后我们给出 $G = hP$，这个公式可以得到一个阶为n的子群（除了 $G = hP = 0$，这个时候子群的阶为1)。\n根据这个我们可以给出算法的大纲：\n 计算椭圆曲线的阶N 选择一个合适的子群阶n,为了算法能够工作，这个数必须是一个素数且必须是N的一个因数。 计算辅因子 $h = N/n$ 在曲线上选择一个随意的点P 计算$G = hP$ 如果G是0,那么重复第4步。直到我们找到一个子群的基点 能够让这个子群的阶为n以及其辅因子为h。  注意这个算法只能够在n为素数的时候工作，如果n不是素数，那么G的阶将会是n的一个因数。 离散对数(Discrete logarithm) 正如我们在连续的椭圆曲线中讨论的，如果我们已知P和Q，那么如何知道这样的k使得$Q = kP$？ 这个问题就是我们将要讨论的椭圆曲线上的椭圆曲线问题，这被称为\u0026quot;hard\u0026quot;的问题，目前还没有能够利用经典计算机在多项式时间内计算出来的算法。但是也没有能够证明该难度的数学证明。\n这个离散对数问题也在其他加密系统中有相当的应用，例如数字签名算法( Digital Signature Algorithm ,DSA)，Diffie-Hellman密钥交换算法和ElGamal算法——他们用相同的算法绝非偶然。不同的是，在我们的离散对数系统中需要将模运算代替原来的标量乘法，那么离散对数问题就可以用下面的问题描述：我们已知a和b，是否能够求得这样的k使得$b = a^k \\mod p$?\n这些问题都是“离散的”，因为他们限制在有限域（更加精确的，循环子群）中，并且也是“对数”的，因为它和通常的对数算法相似。\n那么是什么让ECC这么青睐离散对数问题呢？就现在而言，主要原因是：在椭圆曲线上的离散对数问题是\u0026quot;harder\u0026quot;的，相对于其它相似的应用在密码学中的问题而言。同其它加密系统相比，在相同的安全级别上，它可以节约整数k的位数。更多的细节我们会在第四篇文章中进行阐述。\n未完待续 下一篇文章将是这个系列的第三篇文章，它将介绍ECC算法：包括密钥对生成，ECDH和ECDSA。这将是这个系列中最有趣的一篇，不要错过！  转载请注明出处 ","date":"2017-06-14","permalink":"https://chenquan.me/posts/elliptic-curve-cryptography_a-gentle-introduction_2/","tags":["cryptography","ecc","ecdsa"],"title":"Elliptic Curve Cryptography: a gentle introduction (2)"},{"categories":null,"contents":"现在我们可以在TLS,PGP,SSH等应用中看到椭圆曲线加密系统的踪影，这三个应用几乎构建起了现代web应用和信息技术世界的基石，更不要说Bitcoin以及其他的加密货币。\n在ECC流行之前，几乎所有的公钥算法都是基于RSA,DSA以及DH，这些算法都是基于同余理论构建的。RSA以及其同类算法现在还是很流行，往往配合ECC一同使用，相对于RSA这种原理很容易被人理解，也很容易实现的算法而言，ECC的内部机理对大多数人而言似乎是一种未解之谜。\n我将会用这一系列的文章带领大家进入椭圆曲线加密的全新世界，我的目标不是详细地说明ECC的全部细节(这些在网上随手都能搜索到)，而是要提供一个简单易懂的教程说明什么是ECC，以及为什么它是安全的，当然我也不想花费大量的时间去进行数学证明。我将会提供一些有用的样例然后用一些生动的动态交互工具和脚本去展现它们。\n特别地，这里有些点事我将要提及的：\n为了理解这里都在说些什么，你需要预先理解一些理论：几何和现代代数，并且需要了解对称加密和非对称加密算法，最后你需要对什么是easy的问题，什么是hard的问题有一个清晰的认识。\n准备好了吗？让我们开始吧！\n   椭圆曲线 （Elliptic Curves）  首先，从什么是椭圆曲线（Elliptic Curver）开始，Wolfram MathWorld给了一个完整定义: 但是我们不玩这个，简单来说，椭圆曲线就是一个可以通过等式： $$ y^2 = x^3 + ax + b $$\n描述的点集。\n其中$4a^3+27b^2 \\neq 0$\n 因为需要排除掉 singular curves 也称奇异曲线。\n 上述的等式叫做椭圆曲线_Weierstrass normal form_\n 上图是不同椭圆曲线的形状（ b=1, a从2变化到-3 ）\n  singularities 曲线:\n左边, a curve with a cusp（尖点）:$y^2=x^3$\n右边, a curve with a self-intersection（自交）$y^2 = x^3 - 3x + 2$上述两者均为不合法的椭圆曲线\n 根据不同的a和b，椭圆曲线可能在平面上展现出不同的形状，很明显，椭圆曲线是关于x轴对称的。\n在我们的设计当中，我们还需要一个_无穷点_ Point at infinity（或者称为理想点），现在我们用符号 0 作为作为我们的无穷点。\n我们将上面的椭圆曲线的定义再明确一下，并加入刚刚的无穷点，我们可以得到如下的公式：\n$$ \\lbrace (x, y) \\in \\mathbb{R}^{2} \\mid y^{2}=x^{3}+a x+b, 4 a^{3}+27 b^{2} \\neq 0 \\rbrace\\cup\\lbrace0\\rbrace $$\n   群（Group）  群(Goup)在数学上的定义是能够进行所谓“加法”的二元运算的一种集合，通常我们将这种运算用符号+来表示。如果集合$\\mathbb{G}$想要成为一个”群”,它必须有以下四个属性：\n 闭包（closure）律:如果a和b都是集合$\\mathbb{G}$的成员，那么a+b也是集合的$\\mathbb{G}$成员； 结合律（associativity $(a+b)+c=a+(b+c)$ 需要存在一个孤立点（identity element）0 满足$a+0=0+a=a$ 每个元素都有一个相反数，例如所有的元素a都存在这样的元素b使得$a+b=0$。  如果这个群还满足：\n交换律(commutativity): $a+b = b + a$  这个群就被称为 abelian group 。\n按照我们对加法的传统理解，我们可以得出以下结论：整数集合$\\mathbb{Z}$是一个群(而且是一个阿贝尔群(abelian group))，但是自然数集合$\\mathbb{N}$不是一个群，它不满足第四条属性。\n群可以说是非常的nice,因为如果我们证明了上述四条属性的正确性，那么我们可以免费得到一堆有用的属性，例如如果一个独立元素是唯一的那么其一定有一个相反元素也是唯一的，换句话说，对任意的元素a都存在唯一的元素b使得$a+b=0$当然我们也可以写作$b=-a$，不管是直接的还是间接的，这些定理都将在后面的内容中起到非常重要的作用。\n   椭圆曲线的群律(The group law for elliptic curves)  我们可以在椭圆曲线上找到一个群，这个可以描述为：\n 群中的所有元素都是椭圆曲线上的点。 孤立点是无穷``。 其中点P的相反数是另一个相对于X轴对称的点。 加法遵从以下规则：给出三个在一条直线上的非零点P,Q,R，三个点之和$P+Q+R=0$。   三个点之和为0\n 需要注意的是最后一条规则，我们仅仅需要三个点，这三个点需要在一条直线上，但是不需要关心他们的顺序，也就是说，如果P,Q，R在一条直线上，那么：$P+(Q+R)=Q+(P+R)=R+(P+Q)=\\cdots=0$\n显然，我们的 + 运算符同时满足结合律（associative）和交换律（commutative），这其实是个阿贝尔群。\n到此为止，一切都很棒！但是，我们到底应该怎么去计算两个任意点的和呢？\n   几何加法  多亏了阿贝尔群的定理，我们可以得到下列的结果：$P+Q+R=0\\ or\\ P+Q=R$。利用这个等式，我们可以利用几何方法去计算P和Q两个点的和：如果我们画一条穿过P点和Q点的直线，这条直线将会和椭圆曲线相交于第三个点R(这其实隐含在P,Q,R在一条直线上这个前提中).如果我们取R的相反数-R，这个就是我们要的 P+Q的结果。\n 画一条穿过P点和Q点的线，这条线将会穿过点R,而R的对称点-R就是P+Q的结果。\n 这个几何方法是行得通的，但是我们还是需要细化一下，尤其是我们需要回答几个问题：\n  如果P=0或者Q=0?显然我们没有办法画出一条线(因为0不在xy平面上)，但是我们的定义是0是我们的孤立点，P+0=P以及Q+0=Q，这个对任意的P和Q都成立。\n  如果P = -Q?在这种情况下，穿过两点的直线是垂直的，而且不会与椭圆曲线相交于第三个点，但是因为P是Q的相反元素，我们有$P+Q=P+(-P)=0$这条定理。\n  如果P = Q？在这种情况下，可以画出无数条穿过两点的现，这样让事情变得有点复杂，所以我们考虑一种简化情况，$Q'\\neq P$，如果Q'无限接近P,将会发生什么？\n   随着两个点越来越接近，这条线渐渐变为了椭圆曲线的切线\n 随着Q'无限趋近于P，穿过p和Q'的线渐渐变成了曲线的切线，换句话说，我们可以说p + p = -R，其中R是曲线和曲线在P点的切线的交点。\n  如果P≠Q但是恰好又没有第三个点R怎么办？这种情况其实很像上面的那种情况，实际上这种情况其实就是经过P，Q两点的直线恰好是曲线的切线。  如果我们的直线交于两点，换句话说，这条直线是曲线的切线，显然,最终结果是两点中的某一点的对称点。\n 我们假设P是切点，在上面的情况中，我们可以得到P+P=-Q，可以转变为:P+Q=-P，如果，在另一种情况下，Q是切点，正确的等式将会变为P+Q=-Q。\n     代数加法(Algebraic addition)  如果我们让计算机做上述的点加运算，我们需要将上述的几何加法转变为算术加法，将上面的规则转换为用一组公式来描述看上去非常简单，实际上确是非常乏味的，因为它要求解三次方程。所以我们跳过无聊的部分，直接给出结果。\n首先我们先排除烦人的特殊情况。我们已经知道P+(-P)=0，而且我们也知道P+0=0+P=P，所以在我们的公式中，我们仅仅考虑非零,非对称的两个点，$P=\\left(x_{P}, y_{P}\\right)$ 和$Q=\\left(x_{Q}, y_{Q}\\right)$。\n如果P和Q是不同的(x_P≠x_Q)，那么穿过这两点的线的斜率是：\n$$ m=\\frac{y_{P}-y_{Q}}{x_{P}-x_{Q}} $$\n这条直线和椭圆曲线的交点是第三点R(x_R,y_R)\n$$ x_{R}=m^{2}-x_{P}-x_{Q} \\newline y_{R}=y_{P}+m\\left(x_{R}-x_{P}\\right) $$\n或者，相等地：\n$$ y_{R}=y_{Q}+m\\left(x_{R}-x_{Q}\\right) $$\n因此，$\\left(x_{P}, y_{P}\\right)+\\left(x_{Q}, y_{Q}\\right)=\\left(x_{R},-y_{R}\\right)$(请注意符号并谨记 P+Q=-R)\n如果我们想要检查这个结果是否是正确的，我们需要检查R是否在曲线上，以及P,Q,R是否在一条直线上。检查三个点是否在一条直线上比较琐碎。但是检查R是否在曲线上还是比较简单的，我们需要解三次方程，当然还是很无趣。\n所以我们换种方式，我们用例子来验证一下，根据我们的visual tool，给出P = (1,2)和Q = (3,4)，这两个点都是在曲线:$y^2=x^3-7x+10$上的，这两点的和是P+Q=-R=(-3,2),让我们看看等式是否成立： $$ \\begin{aligned} m \u0026amp;=\\frac{y_{P}-y_{Q}}{x_{P}-x_{Q}}=\\frac{2-4}{1-3}=1 \\newline x_{R} \u0026amp;=m^{2}-x_{P}-x_{Q}=1^{2}-1-3=-3 \\newline y_{R} \u0026amp;=y_{P}+m\\left(x_{R}-x_{P}\\right)=2+1 \\cdot(-3-1)=-2 \\newline \u0026amp;=y_{Q}+m\\left(x_{R}-x_{Q}\\right)=4+1 \\cdot(-3-3)=-2 \\end{aligned} $$ 确实是正确的。\n我们注意到，这些等式在P点或是Q点是切点的时候也是work的。\n让我们试试P=（-1，4）Q=(1,2)\n$$ \\begin{aligned} m \u0026amp;=\\frac{y_{P}-y_{Q}}{x_{P}-x_{Q}}=\\frac{4-2}{-1-1}=-1 \\newline x_{R} \u0026amp;=m^{2}-x_{P}-x_{Q}=(-1)^{2}-(-1)-1=1 \\newline y_{R} \u0026amp;=y_{P}+m\\left(x_{R}-x_{P}\\right)=4+-1 \\cdot(1-(-1))=2 \\end{aligned} $$\n我们通过可视化工具visual tool也可以得到同样的结果。\nP=Q的情况需要稍稍特殊对待：求$x_R和y_R$的方程和上面是一样的，但是对于$x_P=x_Q$这种情况，我们需要换一个求斜率的公式： $$ m=\\frac{3 x_{P}^{2}+a}{2 y_{P}} $$\n注意到，就像我们所期待的，这个用来计算m的表达式是由以下公式派生的第一个式子：\n$$ y_{P}=\\pm \\sqrt{x_{P}^{3}+a x_{P}+b} $$\n为了证明这个结果的有效性，只需要证明R是否是曲线上的点，以及经过P和R两点的是否与曲线有且仅有两个焦点。但是我们不去证明这点，我们还是通过一个例子来说明，降低难度：\nP=Q=(1,2)\n$$ \\begin{aligned} m \u0026amp;=\\frac{3 x_{P}^{2}+a}{2 y_{P}}=\\frac{3 \\cdot 1^{2}-7}{2 \\cdot 2}=-1 \\newline x_{R} \u0026amp;=m^{2}-x_{P}-x_{Q}=(-1)^{2}-1-1=-1 \\newline y_{R} \u0026amp;=y_{P}+m\\left(x_{R}-x_{P}\\right)=2+(-1) \\cdot(-1-1)=4 \\end{aligned} $$\n我们已经知道答案的情况下$P+P=-R=(-1,-4)$，这是正确的 (就不要纠结是怎么预先得到答案的，原作者为了降低难度，简单示例了一下，想要了解更多，点击链接即可，译者注)\n   标量乘法(Scalar multiplication)  在加法的基础上，我们可以定义一个新的操作：标量乘法（scalar multiplication）,可以表示为：\n$$ n P=\\underbrace{P+P+\\cdots+P}_{n \\text { times }} $$\n其中n是自然数，我也写了一个可视化工具用来演示标量乘法，你当然也可以试试。\n就从公式上看，看上去计算nP需要n次加法运算，假设n拥有k个二进制数字，我们的算法复杂度将会是$O\\left(2^{k}\\right)$，当然这有点不太ok,但是实际上还有更加高效的算法。\n其中有一种算法叫做 double and add算法，这种算法的理论可以通过例子更好地进行解释。假设我们有一个数字n = 151，它的二进制值是：10010111。那么这个二进制值可以通过2的幂次和进行计算：\n$$ \\begin{aligned} 151 \u0026amp;=1 \\cdot 2^{7}+0 \\cdot 2^{6}+0 \\cdot 2^{5}+1 \\cdot 2^{4}+0 \\cdot 2^{3}+1 \\cdot 2^{2}+1 \\cdot 2^{1}+1 \\cdot 2^{0} \\newline \u0026amp;=2^{7}+2^{4}+2^{2}+2^{1}+2^{0} \\end{aligned} $$\n（我们通过将n的每一位二进制数n乘以一个2的幂次）将n表示出来：\n也就是说，我们可以得到：\n$$ 151 \\cdot P=2^{7} P+2^{4} P+2^{2} P+2^{1} P+2^{0} P $$\ndouble and add 算法主要步骤如下：\n 得到一个P 将P乘以2所以我们得到2P 将2P+P（为了得到$2^{1} P+2^{0} P$） 再将$2^{1} P$乘以2得到$2^{2} P$ 将其相加得到$2^{2} P+2^{1} P+2^{0} P$ 将$2^{2} P$乘以2得到$2^{3} P$ 不要将$$2^{3} P$$执行相加步骤 将$2^{3} P$乘以2得到$2^{4} P$ 将其加上我们的结果(所以我们得到了$2^{4} P+2^{2} P+2^{1} P+2^{0} P$) …  最后，我们可以通过_double and add_算法计算出$151 \\cdot P$的结果\n如果这还不够清楚，那么这里有一个python脚本实现了这个算法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  def bits(n): \u0026#34;\u0026#34;\u0026#34; Generates the binary digits of n, starting from the least significant bit. bits(151) -\u0026gt; 1, 1, 1, 0, 1, 0, 0, 1 \u0026#34;\u0026#34;\u0026#34; while n: yield n \u0026amp; 1 n \u0026gt;\u0026gt;= 1 def double_and_add(n, x): \u0026#34;\u0026#34;\u0026#34; Returns the result of n * x, computed using the double and add algorithm. \u0026#34;\u0026#34;\u0026#34; result = 0 addend = x for bit in bits(n): if bit == 1: result += addend addend *= 2 return result   如果double和add的复杂度均为_O(1)_那么这个算法的复杂度为_O(log n)_或者为_O(k)_如果这个bit的长度是确定的)，这非常棒。这比原来的_O(n)_算法好多了!\n   对数（Logarithm）  给定的_n_和P，我们至少能够在一个多项式时间内将_Q = nP_计算出来，但是如果反过来呢？如果我们预先知道Q和P然后需要算出n?这个问题就是典型的对数问题，我们将之称为”对数“而不是”除法“是为了整合其他的加密系统（同样的我们也用幂来取代乘法）。\n我并不知道任何用来解对数问题的”easy“算法，然而playing with multiplication 展现了一些简单的模式。举例而言：现在有一个曲线$y^{2}=x^{3}-3 x+1$ 以及点 $P=(0,1)$。我们可以快速地验证是否正确。如果_n_是奇数,_nP_就在曲线的左半平面；如果n是偶数那么_nP_就在右半平面。如果我们做一些更加深入的实验，我们可能会找到更多的套路，最终能够带领我们写出一种能够在曲线上高效计算对数的算法。\n但是这里仍然存在很多对数问题：离散问题(the discrete logarithm)。如果我们减小我们椭圆曲线的域，那么纯量相乘仍然是”easy”的，而离散对数却是”hard”问题，这个对偶关系这正是椭圆曲线加密系统最重要的一块砖瓦。\n   待续  接下来我们将会讨论有限域(finite fields)以及离散对数问题，敬请期待。\n 参考文献：\nhttp://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/\n ","date":"2017-06-12","permalink":"https://chenquan.me/posts/elliptic-curve-cryptography_a-gentle-introduction_1/","tags":["blocachain","cryptography","ecc"],"title":"Elliptic Curve Cryptography: a gentle introduction (1)"},{"categories":null,"contents":" http://chenquanme.oss-cn-shanghai.aliyuncs.com/blog/puket.mp4\n  普吉岛\n2017年5月\nhyperchain 团建\n ","date":"2017-06-10","permalink":"https://chenquan.me/posts/diving-in-puket-/","tags":["摄影","普吉岛","视频"],"title":"普吉岛之旅 - 雨季去浮潜是怎样的体验"},{"categories":null,"contents":" Puket, Thailand\n2017-05-23/2017-05-28\nSony a6000 / iPhone SE\n ","date":"2017-06-09","permalink":"https://chenquan.me/posts/hello-tailand-traveling-in-puket/","tags":["摄影","泰国"],"title":"你好泰国! 带着索尼 A6000 去普吉岛拍照"},{"categories":null,"contents":"Hi, I’m a backend developer focusing on fintech and blockchain, and I love photography and traveling.\nEmail: terasum[at]163.com\ngithub\n","date":"2017-06-07","permalink":"https://chenquan.me/about/","tags":null,"title":"About"},{"categories":null,"contents":"在这篇文章中我将介绍一些在过去一年中（无论是工作还是个人项目中）较为成功的开发模型，这里不会介绍很多项目的细节，仅仅是介绍分支策略和发布管理。\n整个模型都是专注于Git 作为一个源代码版本管理工具进行的。\n   Why git  尽管Git和中央源代码管理系统之间的优劣讨论从未停止过，期间的争论也很激烈，但是作为一个开发者，我更习惯用Git作为我的版本管理工具，Git彻底改变了开发者合并和分支的思维习惯。在传统的CVS/Subversion系统中 ，合并分支是一件非常恐怖的事情，你需要非常小心。\n但是在Git中，这些行为变得非常地简单并且安全，这几乎变成了程序员每天工作流中的核心部分。就拿书来说，讲CVS/Subversion系统的书中，分支和合并几乎是在后面的章节中进行介绍的（为高级用户准备的）但是在几乎所有介绍Git的书中，分支和合并的内容都放到了前三章作为基础内容进行介绍。\n   Decentrailzed but centrailzed  （标题我译为分布且集中，但是好像并不是很恰当）\n在我们的日常工作中，工作较好的模型是存在一个中心化的truth”（可信）仓库，需要注意的是，这个仓库仅仅是在逻辑上被设置为一个中央仓库（因为Git是一个DVCS\u0026lt;分布式版本控制系统\u0026gt;，它在技术层面并没有一个实际意义上的中央仓库）我们将这个“中央”仓库称为origin，几乎所有的Giter都对这个名字非常熟悉。\n所有的开发者都从origin 拉取代码，并将修改push到origin仓库中。除了到中央仓库push-pull之外，开发者还可以从其他的子团队分支中拉取代码。举例来说，在开发一个非常重大的feature的时候，可能会有多个开发者共同合作，很多时候push到“中央”仓库可能还太早，这个时候这些子团队分支就起到作用了。就像上图所示，这些子团队分支就像Alice-Bob，Alice-David和Clair-David子团队分支。\n从技术上来说，让Alice 定义一个新的名叫Bob的远程分支，并将其指向Bob的仓库，并不需要多少成本，反之亦然。\n   The main branches  （主分支策略）\n这个开发模型在很大程度上是由现有的模型演变而来，就像下图所示。核心仓库有两个无限生命周期的分支：\n master develop  在origin仓库的master分支相信对所有的Git user一定非常熟悉，与master分支同时存在的分支并行发展，被称为develop分支。\n我们通常将origin/master分支作为源代码的HEAD，并将其标记为production-ready（可供发布）状态。\n我们把origin/develop分支作标记为最新开发进度的HEAD，所有在develop分支上所做的改变都将体现在下个发布版本中。有的人将这个分支为“集成分支”，很多自动化的预发布版本都是从这个分支编译而来。\n当develop分支上的源代码达到了一个可以发布的点的时候，所有的变更都将以某种方式合并回master并将打上一个发布tag。当然具体是怎么做的将在今后进行讨论。\n因此，每次将变更合并到master就意味着新的生成版本发。所以这件事需要非常严格地进行审核，从理论上讲，我们可以在每次commit到maser分支的时候，用git hooks进行自动化编译，并自动化地将我们的软件发布到我们的生产服务器。\n   Supporting branches  除了main分支和develop分支，我们的开发模型还拥有很多支持分支来帮助团队成员之间进行协作，这些分支可以方便跟踪新特性，为生产发布做准备和快速热修复产品中的问题。不像前面提到的主要分支，这些分支最终都会本删除，所以这些分支的生命周期是有限的。\n支持分支主要分为以下三类：\n Feature branches Release branches Hotfix branches  这些分支都有其自身的目标，并遵守相应严格的规则去确定哪个分支是其的源分支，那些分支是其需要合并进入的分支。\n实际上从技术层面这些分支和普通分支一样并不是特殊的分支。这些分支的分类取决于我们如何去使用它。这些分支实际上就是普通的Git分支。\n   Feature branches  可能检出的分支：\n develop  需要并入的分支：\n develop  分支可以命名为：\n除了：master,develop,release-*或者hotfix-*之外的名称\nFeature分支（有的时候也成为主题分支）是在开发新的特性或者是发布新的版本的时候使用的。当开始一个新的特性的开发的时候，往往新特性的内容在当时是不可见的，但是实质上，feature将会一直存在直到该feature开发完毕，最终将会被合并回develop分支（为了能够真正地将新特性加入到上游分支，）或者被废弃（例如令人失望的尝试）。\nFeature 分支往往只存在在开发仓库中，而不是在origin仓库中。\n   Creating a feature branch  当你需要开始开发一个新的feature的时候，你可以从develop分支检出新的feature分支：\n1 2 3  $ git checkout -b myfeature develop 此时将会切换到新的分支 \u0026#34;myfeature\u0026#34;      在develop分支中加入新的feature  当你结束一个新的特性开发之后，往往需要将该分支合并到develop中将新特性真正加入到上游发布版本中：\n1 2 3 4 5 6 7 8  $ git checkout develop Switched to branch \u0026#39;develop\u0026#39; $ git merge --no-ff myfeature Updating ea1b82a..05e9557 (Summary of changes) $ git branch -d myfeature Deleted branch myfeature (was 05e9557). $ git push origin develop   这里的no-ff选项往往会通过创建一个新的commit开进行合并，就算这个合并能够用fast-forward方式进行处理。这样就可以避免丢失一些在开发过程中的提交历史信息，通过这种方式能够正确地组织相应的commit以及将feature很好地进行合并。我们可以通过下图进行比较理解：\n在后面的例子中，我们是无法看到在实现分支的时候的所有提交历史的，你只能通过阅读相应的log开确定提交历史，当然回滚整个feature（例如一组commit）变成了一件非常头痛的事情，当然如果你用课no-ff选项，这些事情都变得很简单。\n当然，这会创建一些新的(空的)commit,但是得到的好处远远能够抵消这点不足。\n   Realease branches  可能由以下分支检出：\n develop  必须要合并回以下分支：\n develop master  分支命名规则：\n release-*  Release分支为新产品的发布做准备，其允许做一些一些最终的修改(last-minute dotting of i’s and t’s)。进一步说明，该类型的分支，允许少量的bug-fix以及添加一些为发布准备的说明性信息（例如版本号，编译日期等等），当release分支上的所有工作都做完之后，develop分支就可以完全为下一个大型的release接受新的提交了。\n这里有一个比较关键的时间点，就是何时从develop分支检出新的release分支，这个时间点往往是develop分支已经能够完全反映当前发布版本所需要的所有特性。此时至少所有的需要在发行版本中所包含的（release-to-be-build）特性都已经被合并到develop分支中了，需要提一下的是，所有下个版本的新特性都不能够合并到develop分支，直至新的release被检出（branch off）。\n显然，发布版本号是在为了即将到来的新的发布版本开启一个新的release分支的时候指定的——换句话说不要太早。这样做的原因是，当develop中有一些被标记为next release的变更，但是并不明确next release最终会变成0.3还是1.0，只有release分支开始才被确定。一个新的release分支的启用往往是在版本号被确定之后。\n   创建一个release分支  发布分支是从develop分支中创建的。举例来说，当前的生产分支是1.1.5，这个时候有一个大的发布版本即将发布。此时develop分支应该应该已经准备好并处于next release状态，这个时候我们应该已经确定了具体的发布版本号为1.2（而不是1.1.6或者2.0），所以我们检出新的分支，并给予发布分支一个名称用于标识新的版本号。\n1 2 3 4 5 6 7 8  $ git checkout -b release-1.2 develop Switched to a new branch \u0026#34;release-1.2\u0026#34; $ ./bump-version.sh 1.2 Files modified successfully, version bumped to 1.2. $ git commit -a -m \u0026#34;Bumped version number to 1.2\u0026#34; [release-1.2 74d9424] Bumped version number to 1.2 1 files changed, 1 insertions(+), 1 deletions(-)   在创建一个新的分支并切换到该分支之后，我们运行了一个bump-version.sh的脚本，这个脚本将会修改一些文件并让目前的工作空间变成能够反映新的版本特性。（当然这些修改也可以手动进行——这里的重点是需要进行一些修改）然后，这个已经被确认的版本号就会被提交。\n   结束一个release分支  当一个release分支已经准备好成为一个真正的发布版本，一些相应的动作需要被执行。首先这个发布分支需要合并到master分支当中（切记，所有提交到master分支上的变更都是可发布的）。然后，这个master分支上的commit应该打一些标记便于将来快速查询，当然这些信息需要简单易懂。\n最后，这些在release分支上的变更应该合并回develop分支，这样在后面的分支才能够拥有在这个release分支上的bugfix。\n前两步的命令如下：\n1 2 3 4 5 6  $ git checkout master Switched to branch \u0026#39;master\u0026#39; $ git merge --no-ff release-1.2 Merge made by recursive. (Summary of changes) $ git tag -a 1.2   目前，release分支已经完成了它的使命，并打了一个tag便于今后查询。\n 说明，你也可以利用-s或者-u命令为你打的tag进行一些数字签名。\n 为了保有在release分支上的变更，我们需要将这些变更合并回develop分支。当然，用git命令：\n1 2 3 4 5  $ git checkout develop Switched to branch \u0026#39;develop\u0026#39; $ git merge --no-ff release-1.2 Merge made by recursive. (Summary of changes)   这些步骤可能引起一些合并冲突（当然这是很正常的，因为我们已经修改了版本号）。如果这样，就fix这些冲突，并commit它。\n到这里为止，这个release分支真真正正完成了它所有的使命，这个时候我们就可以把这个分支删除了，因为它已经失去了利用价值了。\n1 2  $ git branch -d release-1.2 Deleted branch release-1.2 (was ff452fe).      Hotfix分支  可能检出的分支：\n master  需要并入的分支：\n develop和master  分支可以命名为：\nhotfix-*\nHotfix与release分支十分相似，因为它们都是为了新产品发布而准备的，尽管这个分支往往是计划外的。它们往往会在一个非修复不可的，非常紧急的或者不期望看到的行为或者bug的时候生产版本中诞生。当生产版本上一个非常严重的bug需要立即修复的时候，一个hotfix分支就被检出了，这个分支可以从master分支的相应tag中检出。\n这样做的关键在于，在develop分支上工作的团队不需要中断当前的工作，与此同时，另一个人可以快速修复生产版本上的bug。\n   创建一个hoxfix分支  Hoxfix分支是从master中诞生的，举例来说，当前的生产版本是1.2,而且该版本在实际业务中正运行着，但是有一些bug需要修复，而此时在develop分支上的版本还不稳定。我们就可以检出一个hotfix分支来解决这个问题。\n1 2 3 4 5 6 7  $ git checkout -b hotfix-1.2.1 master Switched to a new branch \u0026#34;hotfix-1.2.1\u0026#34; $ ./bump-version.sh 1.2.1 Files modified successfully, version bumped to 1.2.1. $ git commit -a -m \u0026#34;Bumped version number to 1.2.1\u0026#34; [hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1 1 files changed, 1 insertions(+), 1 deletions(-)   不要忘记将该版本号进行标记。\n然后修复bug并通过一个或多个commit进行提交。\n1 2 3  $ git commit -m \u0026#34;Fixed severe production problem\u0026#34; [hotfix-1.2.1 abbe5d6] Fixed severe production problem 5 files changed, 32 insertions(+), 17 deletions(-)      结束一个hotfix分支  当bug结束修复，这个bugfix分支需要重新合并到master分支中，而且也要同时合并到develop分支中，这是为了确保这些bugfix能够真正合并到下个发布版本中。这里就和结束release分支很像了。\n1 2 3 4 5 6  $ git checkout master Switched to branch \u0026#39;master\u0026#39; $ git merge --no-ff hotfix-1.2.1 Merge made by recursive. (Summary of changes) $ git tag -a 1.2.1    说明，你也可以利用-s或者-u命令为你打的tag进行一些数字签名。\n 然后将bugfix分支合并到develop分支中：\n1 2 3 4 5  $ git checkout develop Switched to branch \u0026#39;develop\u0026#39; $ git merge --no-ff hotfix-1.2.1 Merge made by recursive. (Summary of changes)   这里有一个例外，如果此时有一个release分支存在，这个hotfix就需要合并到release分支中，而不是develop分支。将hotfix合并到release分支中，是因为，在release分支结束的时候，最终也会合并入develop分支中，这样最终的结果也是讲bugfix安全地合并到了develop分支中。\n最后删除hotfix分支：\n1 2  $ git branch -d hotfix-1.2.1 Deleted branch hotfix-1.2.1 (was abbe5d6).      总结  虽然在这个分支模型中，没有什么令人震惊的奇技淫巧，但开篇的那个大图所标示的分支模型已经被证明在实际开发中能够起到非常大的作用。这个模型提供了一个优雅的协作范式，它易于理解，并能够让整个团队一直保持易于理解并便于协作的状态。\n这里有一副PDF版本的高质量模型图，将它下载下来并作为一个快速查询图用吧！\n当然这里还有一个gitflow的keynote，你可以下载下来用哦;)\n","date":"2017-04-16","permalink":"https://chenquan.me/posts/a-successful-git-branch-model/","tags":["git"],"title":"一个成功的git分支模型"},{"categories":null,"contents":"我一直想写一篇文章，用来说明如何按照我自己的喜好来进行Sublime Text(目前版本是V3)的安装和设置。\n这篇文章更多的是为我自己写的一篇指南，以免未来的什么时候我忘记了。如果这篇文章能够帮到你，那就更棒啦！\n   步骤1: 下载并且安装  你可以从以下链接下载最新版本：\n Sublime Text 2 Sublime Text 3  目前我使用的版本是ST3 build 3065 更新于2014年8月29日\n 译者 用的版本是st3 build 3083\n    步骤2: 安装Package Control (插件管理工具)  如果想安装其他的插件，需要先安装这个Package Control 插件，请遵照这个链接中的安装教程：\nPackage Control Install\n注意：如果你在使用代理，可能需要手动下载并安装这个包，安装完成之后，需要重启下sublime text (出于安全因素的考虑)\n译者 ：\n由于考虑到国内不一定能打开安装链接，我把安装代码贴在下面，请在st3中按快捷键’ctrl+`’然后再弹出的输入框中输入以下代码并回车（2015/8/25）：\n1 2 3 4 5  import urllib.request,os,hashlib; h = \u0026#39;eb2297e1a458f27d836c04bb0cbaf282\u0026#39; +\u0026amp;lt;/blockquote\u0026amp;gt; \u0026#39;d0e7a3098092775ccb37ca9d6b2e4b7d\u0026#39;; pf = \u0026#39;Package Control.sublime-package\u0026#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( \u0026#39;http://packagecontrol.io/\u0026#39; + pf.replace(\u0026#39; \u0026#39;, \u0026#39;%20\u0026#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(\u0026#39;Error validating download (got %sinstead of %s), please try manual install\u0026#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), \u0026#39;wb\u0026#39; ).write(by)    更新: 这个脚本是每天更新的，复制过去脚本会失效，请用上面的安装链接践行安装。（2017-04-01）\n    步骤2（附）: Package Control的代理设置  如果你需要在代理环境中使用Sublime Text ，那么你需要配置一下代理来让Package Control 正常工作。\n将以下代码加入到\nPreferences \u0026gt; Package Settings \u0026gt; Package Control \u0026gt; Settings – User\n中：\n1 2 3 4 5 6  //proxy settings \u0026#34;http_proxy\u0026#34;: \u0026#34;your.proxy.fqdn:80\u0026#34;, \u0026#34;https_proxy\u0026#34;: \u0026#34;your.proxy.fqdn:80\u0026#34;, \u0026#34;proxy_username\u0026#34;: \u0026#34;my_username\u0026#34;, \u0026#34;proxy_password\u0026#34;: \u0026#34;my_password\u0026#34;,   注意:\n你可能需要修改80端口为别的你代理的端口（比如8080）， 你可以在IE的代理设置中找到这个端口 (译者用的是ss，视实际情况而定)。\n完成之后，重启下Sublime Text\n   步骤3: 安装Sublime Text的插件  现在，你可以安装所有能让Sublime Text更酷的插件! 以下的步骤是安装所有插件需要的步骤：\n Cmd+shift+P (OSX) / Ctrl+Shift+P (Win/Linux) 输入：Install Packsge 搜索你需要的插件名称 按回车进行安装     我在Sublime Text中使用的插件有：   CSS Comb Emmet1 Emmet CSS Snippets FileDiffs (only on Windows machines) Gist Predawn2 PowerShell SASS SCSS Sidebar Enhancements SublimeLinter VBDotNet vbScript WordPress   1: To use Emmet you need PyV8 installed. By default when installing Emmet, PyV8 will attempt to download and install automatically. If it doesn’t you an download it from here Emmet PyV8.\n如果想要使用emmet插件需要安装PyV8依赖，一般情况下可以自动安装完成，如果它傲娇了，请务必点击上面的链接下载自行安装。\n2: You must install Predawn prior to configuring User Settings in Step 5 below (otherwise the settings in the Colour Scheme \u0026amp; Theme section will fail).\nFor more information about PreDawn and the options available have a look here Predawn GitHub\n如果你想要更改颜色主题，请一定要按照步骤5中的用户配置进行相应配置。\n    步骤4: 编辑asp页面以包含aspx 文件   由于这部分并不重要，是作者自己个人的偏好，并没有普遍性，所以不翻译了\n    步骤5: Sublime Text的用户设置：  想要改变SublimeText的默认设置，你需要把你的个人设置添加到User Settings文件中，你可以在Preferences \u0026amp;gt; Settings – User找到并编辑这个文件：\n下面的是我自己添加的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // Colour Scheme \u0026amp; Theme (for ST3)  \u0026#34;color_scheme\u0026#34;: \u0026#34;Packages/User/predawn (SL).tmTheme\u0026#34;, \u0026#34;theme\u0026#34;: \u0026#34;predawn-DEV.sublime-theme\u0026#34;, // Typography  \u0026#34;font_face\u0026#34;: \u0026#34;Source Code Pro\u0026#34;, \u0026#34;font_size\u0026#34;: 13, \u0026#34;font_options\u0026#34;: [\u0026#34;no_round\u0026#34;], \u0026#34;highlight_line\u0026#34;: true, \u0026#34;caret_extra_width\u0026#34;: 1, \u0026#34;caret_style\u0026#34;: \u0026#34;phase\u0026#34;, \u0026#34;tab_size\u0026#34;: 2, \u0026#34;translate_tabs_to_spaces\u0026#34;: true, \u0026#34;word_wrap\u0026#34;: false, // Whitespace, Matching, Copy \u0026amp; Auto-Complete  \u0026#34;copy_with_empty_selection\u0026#34;: false, \u0026#34;drag_text\u0026#34;: false, \u0026#34;match_brackets_content\u0026#34;: false, \u0026#34;match_selection\u0026#34;: false, \u0026#34;match_tags\u0026#34;: false, \u0026#34;translate_tabs_to_spaces\u0026#34;: true, \u0026#34;trim_trailing_white_space_on_save\u0026#34;: true, // Interface \u0026amp; Behavior  \u0026#34;close_windows_when_empty\u0026#34;: false, \u0026#34;draw_minimap_border\u0026#34;: true, \u0026#34;enable_tab_scrolling\u0026#34;: false, \u0026#34;hot_exit\u0026#34;: false, \u0026#34;overlay_scroll_bars\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;open_files_in_new_window\u0026#34;: false, \u0026#34;preview_on_click\u0026#34;: false, \u0026#34;remember_open_files\u0026#34;: false, \u0026#34;scroll_past_end\u0026#34;: true, \u0026#34;scroll_speed\u0026#34;: 5.0, \u0026#34;show_full_path\u0026#34;: false, // Other Settings  \u0026#34;ignored_packages\u0026#34;: [ \u0026#34;Vintage\u0026#34; ],   注意:上面的是Sublime Text 3的设置，如果你正在使用ST2你需要把Colour Scheme \u0026amp; Theme 部分修改成下面的\n1 2 3  //Colour Scheme Theme (for ST2) \u0026#34;theme\u0026#34;: \u0026#34;predawn.sublime-theme\u0026#34;, \u0026#34;color_scheme\u0026#34;: \u0026#34;Packages/Predawn/predawn.tmTheme\u0026#34;,      步骤6: 改变Sublime Text的图标  现在有很多的Sublime Text的炫酷图标，我也把我自己的改掉了，你可以按照下面的步骤进行修改：\nMac OSX\n略(有兴趣请大家翻原文看)\nWindows8\n 把新的图标拷贝到安装目录下(我的是D:\\SublimeText3\\) 右键点击 Sublime Text.exe 点击创建快捷方式 重命名快捷方式 右键点击快捷方式并选择属性 选择快捷方式栏 点击更改图标 选择新的图标 点击ok 右键点击快捷方式并放到开始中  \u0026gt; 尼玛简直太感动太详细了，我居然翻译完了。。。）     步骤7: 安装Sublime Text 的许可  土豪请随意购买许可。\n原文是教你如何购买许可的，当然这没啥意思，所以我找了几个许可大家随意取用(土豪请务必购买正版，请支持正版)：\n —– BEGIN LICENSE —– Andrew Weber Single User License EA7E-855605 813A03DD 5E4AD9E6 6C0EEB94 BC99798F 942194A6 02396E98 E62C9979 4BB979FE 91424C9D A45400BF F6747D88 2FB88078 90F5CC94 1CDC92DC 8457107A F151657B 1D22E383 A997F016 42397640 33F41CFC E1D0AE85 A0BBD039 0E9C8D55 E1B89D5D 5CDB7036 E56DE1C0 EFCC0840 650CD3A6 B98FC99C 8FAC73EE D2B95564 DF450523 —— END LICENSE —— —– BEGIN LICENSE —– K-20 Single User License EA7E-940129 3A099EC1 C0B5C7C5 33EBF0CF BE82FE3B EAC2164A 4F8EC954 4E87F1E5 7E4E85D6 C5605DE6 DAB003B4 D60CA4D0 77CB1533 3C47F579 FB3E8476 EB3AA9A7 68C43CD9 8C60B563 80FE367D 8CAD14B3 54FB7A9F 4123FFC4 D63312BA 141AF702 F6BBA254 B094B9C0 FAA4B04C 06CC9AFC FD412671 82E3AEE0 0F0FAAA7 8FA773C9 383A9E18 —— END LICENSE —— —– BEGIN LICENSE —– J2TeaM 2 User License EA7E-940282 45CB0D8F 09100037 7D1056EB A1DDC1A2 39C102C5 DF8D0BF0 FC3B1A94 4F2892B4 0AEE61BA 65758D3B 2EED551F A3E3478C C1C0E04E CA4E4541 1FC1A2C1 3F5FB6DB CFDA1551 51B05B5D 2D3C8CFE FA8B4285 051750E3 22D1422A 7AE3A8A1 3B4188AC 346372DA 37AA8ABA 6EB30E41 781BC81F B5CA66E3 A09DBD3A 3FE85BBD 69893DBD —— END LICENSE ——  注册码来源\n ","date":"2016-08-25","permalink":"https://chenquan.me/posts/sublime-text-installation-proxy-plugin/","tags":["ide","sublime","编辑器"],"title":"Sublime Text 安装配置，代理设置，插件管理"},{"categories":["影评"],"contents":" 史密斯夫妇影评\n 早早就看过一遍《史密斯夫妇》，对于布拉德·皮特与安吉丽娜·朱莉之间的绯闻也略有耳闻，今天同学在看《史密斯夫妇》的时候，我也凑过去重温了一遍这部豆瓣上也算高分（7.5）的电影,。\n\n注意：严重剧透，请慎重观看。\n说实话，这是一部非常有趣的电影，有帅哥美女也有枪战飙车，最重要的是，夫妇之间情感流露的表达，在我看来简直就是子弹中流露着温情，刀刃中暗藏着暧昧，真心是一部不可多得的电影。整部电影最让我觉得赞的点在于夫妻两人在相搏的时候，咬牙狠心，想着对着自己的老婆（老公）开枪，给人的感觉就是一种莫名的喜感，真心没有说哪一方会得胜，哪一方会被杀。搏斗期间还不忘问候自己的baby.搏斗到最后，是John 先放下枪，给人的感觉又是作为一个男人不舍得杀死自己的妻子，这其中可以看出男人是心爱着她的，于此同时，Jane又说“oh come,just kill me!”(我是凭着印象写的，未必是原话，但是意思是差不多的)。可以看出，Jane内心是宁愿被John 杀死，也不愿亲手开枪杀死眼前的这个男人。这个男人给的小甜蜜真心是可以侵染一个女人的心，而且这个男人还这么handsome 。 最后枪战变成了肉搏，变成了“肉搏”（在这里我不禁要说呵呵了，话说安吉丽娜·朱莉真心有个好身材）\n\n最后天亮的时候穿的白衬衫真心是迷人的（若隐若现的感觉是最为性感的）。到这里，开始有了不打不相识的感觉，他们开始相互袒露心声，虽然还是有所戒备，但是其实对于在这种地方结尾观众老爷也不会有什么不满意了，但是，很不幸，故事还没有结束…\n整部电影的打斗场面没有什么好说的，真心不是一部动作片，但是我喜欢的是它的情节，巧妙的剧本很吸引人，两个相互敌对的间谍组织成员结成的夫妇将会擦出怎么样的火花，导演并没有让我们失望，这部电影在我看来都可以算是温情爱情片了哈哈。在追杀情节中，男主说我其实以前结过婚，Jane猛地一踩刹车，给了John一耳光，真的是很搞笑也很温馨，安吉丽娜把小女生的心理表现得非常到位，但是恰巧她又是一个冷酷无情的杀手。\n影片的剧情本来没有什么跌宕起伏，但是导演并不甘心平淡的结局，最后派遣给他们的任务是组织企图抹杀他们夫妇两而设的圈套，虽然不是什么360度大转变的情节，但是也着实让我没有想到，因为只有把观众都引导向导演安排的心境，这个电影才能达到所需要的效果，这个情节的加入让本以为已经以圆满结尾结束的整部电影，现在又让观众提起了精神继续看下去，说实话，真的是被导演牵着鼻子走，所以看着真心是不错。\n在无情剧透影评之后，我还是想说，这部电影是一部“温情动作谍战爱情”片，还是推荐大家去看看的。\n","date":"2014-11-15","permalink":"https://chenquan.me/archives/22/","tags":["电影, 史密斯夫妇"],"title":"相杀相爱，人生就是要有激情不断的爱情"}]